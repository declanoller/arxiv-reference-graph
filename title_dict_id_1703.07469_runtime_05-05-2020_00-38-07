{
    "robustfill: neural program learning under noisy i/o": {
        "id": "1703.07469",
        "depth": 0,
        "children_titles": [
            "syntax guided synthesis",
            "neural machine translation by jointly learning to align and translate",
            "deepcoder learning to write programs",
            "terpret a probabilistic programming language for program induction",
            "neural turing machines",
            "hybrid computing using a neural network with dynamic external memory",
            "automating string processing in spreadsheets using input output examples",
            "spreadsheet data manipulation using examples",
            "attention based multimodal neural machine translation",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "neural gpus learn algorithms",
            "neural random access machines",
            "effective approaches to attention based neural machine translation",
            "knowledge and reasoning in program synthesis",
            "a deductive approach to program synthesis",
            "computation of normalized edit distance and applications",
            "a machine learning framework for programming by example",
            "neural programmer inducing latent programs with gradient descent",
            "neuro symbolic program synthesis",
            "flashmeta a framework for inductive program synthesis",
            "neural programmer interpreters",
            "programming with a differentiable forth interpreter",
            "sequence to sequence learning with neural networks",
            "improved semantic representations from tree structured long short term memory networks",
            "grammar as a foreign language",
            "prow a step toward automatic program writing",
            "show attend and tell neural image caption generation with visual attention",
            "learning to execute"
        ],
        "status": "root",
        "title_full": "RobustFill: Neural Program Learning under Noisy I/O",
        "link": "https://arxiv.org/abs/1703.07469",
        "n_parents": 0,
        "year": "2017",
        "children_full_dicts": [
            {
                "ref_title_clean": "syntax guided synthesis",
                "ref_title_full": "Syntax guided synthesis",
                "year": "2013",
                "full_block": "Syntax-guided synthesis.\n"
            },
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "2014",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "deepcoder learning to write programs",
                "ref_title_full": "Deepcoder: Learning to write programs",
                "year": "2016",
                "full_block": "Deepcoder: Learning to write programs.\n"
            },
            {
                "ref_title_clean": "terpret a probabilistic programming language for program induction",
                "ref_title_full": "Terpret: A probabilistic programming language for program induction",
                "year": "2016",
                "full_block": "Terpret: {A} probabilistic programming language for program\n  induction.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural turing machines",
                "year": "2014",
                "full_block": "Neural turing machines.\n"
            },
            {
                "ref_title_clean": "hybrid computing using a neural network with dynamic external memory",
                "ref_title_full": "Hybrid computing using a neural network with dynamic external memory",
                "year": "2016",
                "full_block": "Hybrid computing using a neural network with dynamic external memory.\n"
            },
            {
                "ref_title_clean": "automating string processing in spreadsheets using input output examples",
                "ref_title_full": "Automating string processing in spreadsheets using input output examples",
                "year": "2011",
                "full_block": "Automating string processing in spreadsheets using input-output\n  examples.\n"
            },
            {
                "ref_title_clean": "spreadsheet data manipulation using examples",
                "ref_title_full": "Spreadsheet data manipulation using examples",
                "year": "2012",
                "full_block": "Spreadsheet data manipulation using examples.\n"
            },
            {
                "ref_title_clean": "attention based multimodal neural machine translation",
                "ref_title_full": "Attention based multimodal neural machine translation",
                "year": "2016",
                "full_block": "Attention-based multimodal neural machine translation.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "neural gpus learn algorithms",
                "ref_title_full": "Neural gpus learn algorithms",
                "year": "2015",
                "full_block": "Neural gpus learn algorithms.\n"
            },
            {
                "ref_title_clean": "neural random access machines",
                "ref_title_full": "Neural random access machines",
                "year": "2016",
                "full_block": "Neural random-access machines.\n"
            },
            {
                "ref_title_clean": "effective approaches to attention based neural machine translation",
                "ref_title_full": "Effective approaches to attention based neural machine translation",
                "year": "2015",
                "full_block": "Effective approaches to attention-based neural machine translation.\n"
            },
            {
                "ref_title_clean": "knowledge and reasoning in program synthesis",
                "ref_title_full": "Knowledge and reasoning in program synthesis",
                "year": "1975",
                "full_block": "Knowledge and reasoning in program synthesis.\n"
            },
            {
                "ref_title_clean": "a deductive approach to program synthesis",
                "ref_title_full": "A deductive approach to program synthesis",
                "year": "1980",
                "full_block": "A deductive approach to program synthesis.\n"
            },
            {
                "ref_title_clean": "computation of normalized edit distance and applications",
                "ref_title_full": "Computation of normalized edit distance and applications",
                "year": "1993",
                "full_block": "Computation of normalized edit distance and applications.\n"
            },
            {
                "ref_title_clean": "a machine learning framework for programming by example",
                "ref_title_full": "A machine learning framework for programming by example",
                "year": "2013",
                "full_block": "A machine learning framework for programming by example.\n"
            },
            {
                "ref_title_clean": "neural programmer inducing latent programs with gradient descent",
                "ref_title_full": "Neural programmer: Inducing latent programs with gradient descent",
                "year": "2016",
                "full_block": "Neural programmer: Inducing latent programs with gradient descent.\n"
            },
            {
                "ref_title_clean": "neuro symbolic program synthesis",
                "ref_title_full": "Neuro symbolic program synthesis",
                "year": "2017",
                "full_block": "Neuro-symbolic program synthesis.\n"
            },
            {
                "ref_title_clean": "flashmeta a framework for inductive program synthesis",
                "ref_title_full": "Flashmeta: a framework for inductive program synthesis",
                "year": "2015",
                "full_block": "Flashmeta: a framework for inductive program synthesis.\n"
            },
            {
                "ref_title_clean": "neural programmer interpreters",
                "ref_title_full": "Neural programmer interpreters",
                "year": "2016",
                "full_block": "Neural programmer-interpreters.\n"
            },
            {
                "ref_title_clean": "programming with a differentiable forth interpreter",
                "ref_title_full": "Programming with a differentiable forth interpreter",
                "year": "2016",
                "full_block": "Programming with a differentiable forth interpreter.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "2014",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "improved semantic representations from tree structured long short term memory networks",
                "ref_title_full": "Improved semantic representations from tree structured long short term memory networks",
                "year": "2015",
                "full_block": "Improved semantic representations from tree-structured long\n  short-term memory networks.\n"
            },
            {
                "ref_title_clean": "grammar as a foreign language",
                "ref_title_full": "Grammar as a foreign language",
                "year": "2015",
                "full_block": "Grammar as a foreign language.\n"
            },
            {
                "ref_title_clean": "prow a step toward automatic program writing",
                "ref_title_full": "Prow: A step toward automatic program writing",
                "year": "1969",
                "full_block": "Prow: A step toward automatic program writing.\n"
            },
            {
                "ref_title_clean": "show attend and tell neural image caption generation with visual attention",
                "ref_title_full": "Show, attend and tell: Neural image caption generation with visual attention",
                "year": "2015",
                "full_block": "Show, attend and tell: Neural image caption generation with visual\n  attention.\n"
            },
            {
                "ref_title_clean": "learning to execute",
                "ref_title_full": "Learning to execute",
                "year": "2014",
                "full_block": "Learning to execute.\n"
            }
        ],
        "refs_source": "unpacked_sources/1703.07469/ms.bbl"
    },
    "syntax guided synthesis": {
        "id": "1405.5590",
        "depth": 1,
        "children_titles": [
            "syntax guided synthesis"
        ],
        "status": "expanded",
        "title_full": "Syntax guided synthesis",
        "link": "https://arxiv.org/abs/1405.5590",
        "n_parents": 4,
        "year": "2013",
        "children_full_dicts": [
            {
                "ref_title_clean": "syntax guided synthesis",
                "ref_title_full": "Syntax guided synthesis",
                "year": "2013",
                "full_block": "Syntax-guided synthesis.\n"
            }
        ],
        "refs_source": "unpacked_sources/1405.5590/sygus.bbl"
    },
    "neural machine translation by jointly learning to align and translate": {
        "id": "1409.0473",
        "depth": 1,
        "children_titles": [
            "domain adaptation via pseudo in domain data selection",
            "theano new features and speed improvements",
            "learning long term dependencies with gradient descent is difficult",
            "a neural probabilistic language model",
            "theano a cpu and gpu math expression compiler",
            "audio chord recognition with recurrent neural networks",
            "learning phrase representations using rnn encoder decoder for statistical machine translation",
            "on the properties of neural machine translation encoder decoder approaches",
            "fast and robust neural network joint models for statistical machine translation",
            "recursive hetero associative memories for translation",
            "maxout networks",
            "sequence transduction with recurrent neural networks",
            "generating sequences with recurrent neural networks",
            "hybrid speech recognition with deep bidirectional lstm",
            "multilingual distributed representations without word alignment",
            "long short term memory",
            "recurrent continuous translation models",
            "statistical machine translation",
            "statistical phrase based translation",
            "on the difficulty of training recurrent neural networks",
            "on the difficulty of training recurrent neural networks",
            "how to construct deep recurrent neural networks",
            "overcoming the curse of sentence length for neural machine translation using automatic segmentation",
            "bidirectional recurrent neural networks",
            "continuous space translation models for phrase based statistical machine translation",
            "continuous space language models for statistical machine translation",
            "sequence to sequence learning with neural networks",
            "adadelta an adaptive learning rate method"
        ],
        "status": "expanded",
        "title_full": "Neural machine translation by jointly learning to align and translate",
        "link": "https://arxiv.org/abs/1409.0473",
        "n_parents": 9,
        "year": "2014",
        "children_full_dicts": [
            {
                "ref_title_clean": "domain adaptation via pseudo in domain data selection",
                "ref_title_full": "Domain adaptation via pseudo in domain data selection",
                "year": "none",
                "full_block": "Domain adaptation via pseudo in-domain data selection.\n"
            },
            {
                "ref_title_clean": "theano new features and speed improvements",
                "ref_title_full": "Theano: new features and speed improvements",
                "year": "none",
                "full_block": "Theano: new features and speed improvements.\n"
            },
            {
                "ref_title_clean": "learning long term dependencies with gradient descent is difficult",
                "ref_title_full": "Learning long term dependencies with gradient descent is difficult",
                "year": "none",
                "full_block": "Learning long-term dependencies with gradient descent is difficult.\n"
            },
            {
                "ref_title_clean": "a neural probabilistic language model",
                "ref_title_full": "A neural probabilistic language model",
                "year": "none",
                "full_block": "A neural probabilistic language model.\n"
            },
            {
                "ref_title_clean": "theano a cpu and gpu math expression compiler",
                "ref_title_full": "Theano: a CPU and GPU math expression compiler",
                "year": "none",
                "full_block": "Theano: a {CPU} and {GPU} math expression compiler.\n"
            },
            {
                "ref_title_clean": "audio chord recognition with recurrent neural networks",
                "ref_title_full": "Audio chord recognition with recurrent neural networks",
                "year": "none",
                "full_block": "Audio chord recognition with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "learning phrase representations using rnn encoder decoder for statistical machine translation",
                "ref_title_full": "Learning phrase representations using RNN encoder decoder for statistical machine translation",
                "year": "none",
                "full_block": "Learning phrase representations using {RNN} encoder-decoder for\n  statistical machine translation.\n"
            },
            {
                "ref_title_clean": "on the properties of neural machine translation encoder decoder approaches",
                "ref_title_full": "On the properties of neural machine translation: Encoder Decoder approaches",
                "year": "none",
                "full_block": "On the properties of neural machine translation: {E}ncoder--{D}ecoder\n  approaches.\n"
            },
            {
                "ref_title_clean": "fast and robust neural network joint models for statistical machine translation",
                "ref_title_full": "Fast and robust neural network joint models for statistical machine translation",
                "year": "none",
                "full_block": "Fast and robust neural network joint models for statistical machine\n  translation.\n"
            },
            {
                "ref_title_clean": "recursive hetero associative memories for translation",
                "ref_title_full": "Recursive hetero associative memories for translation",
                "year": "none",
                "full_block": "Recursive hetero-associative memories for translation.\n"
            },
            {
                "ref_title_clean": "maxout networks",
                "ref_title_full": "Maxout networks",
                "year": "none",
                "full_block": "Maxout networks.\n"
            },
            {
                "ref_title_clean": "sequence transduction with recurrent neural networks",
                "ref_title_full": "Sequence transduction with recurrent neural networks",
                "year": "none",
                "full_block": "Sequence transduction with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "generating sequences with recurrent neural networks",
                "ref_title_full": "Generating sequences with recurrent neural networks",
                "year": "1308",
                "full_block": "Generating sequences with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "hybrid speech recognition with deep bidirectional lstm",
                "ref_title_full": "Hybrid speech recognition with deep bidirectional LSTM",
                "year": "none",
                "full_block": "Hybrid speech recognition with deep bidirectional {LSTM}.\n"
            },
            {
                "ref_title_clean": "multilingual distributed representations without word alignment",
                "ref_title_full": "Multilingual distributed representations without word alignment",
                "year": "none",
                "full_block": "Multilingual distributed representations without word alignment.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "none",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "recurrent continuous translation models",
                "ref_title_full": "Recurrent continuous translation models",
                "year": "none",
                "full_block": "Recurrent continuous translation models.\n"
            },
            {
                "ref_title_clean": "statistical machine translation",
                "ref_title_full": "Statistical Machine Translation\\/",
                "year": "none",
                "full_block": "{\\em Statistical Machine Translation\\/}.\n"
            },
            {
                "ref_title_clean": "statistical phrase based translation",
                "ref_title_full": "Statistical phrase based translation",
                "year": "none",
                "full_block": "Statistical phrase-based translation.\n"
            },
            {
                "ref_title_clean": "on the difficulty of training recurrent neural networks",
                "ref_title_full": "On the difficulty of training recurrent neural networks",
                "year": "none",
                "full_block": "On the difficulty of training recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "on the difficulty of training recurrent neural networks",
                "ref_title_full": "On the difficulty of training recurrent neural networks",
                "year": "none",
                "full_block": "On the difficulty of training recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "how to construct deep recurrent neural networks",
                "ref_title_full": "How to construct deep recurrent neural networks",
                "year": "none",
                "full_block": "How to construct deep recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "overcoming the curse of sentence length for neural machine translation using automatic segmentation",
                "ref_title_full": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation",
                "year": "none",
                "full_block": "Overcoming the curse of sentence length for neural machine\n  translation using automatic segmentation.\n"
            },
            {
                "ref_title_clean": "bidirectional recurrent neural networks",
                "ref_title_full": "Bidirectional recurrent neural networks",
                "year": "none",
                "full_block": "Bidirectional recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "continuous space translation models for phrase based statistical machine translation",
                "ref_title_full": "Continuous space translation models for phrase based statistical machine translation",
                "year": "none",
                "full_block": "Continuous space translation models for phrase-based statistical\n  machine translation.\n"
            },
            {
                "ref_title_clean": "continuous space language models for statistical machine translation",
                "ref_title_full": "Continuous space language models for statistical machine translation",
                "year": "none",
                "full_block": "Continuous space language models for statistical machine translation.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "none",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "adadelta an adaptive learning rate method",
                "ref_title_full": "ADADELTA: An adaptive learning rate method",
                "year": "1212",
                "full_block": "{ADADELTA}: An adaptive learning rate method.\n"
            }
        ],
        "refs_source": "unpacked_sources/1409.0473/search.bbl"
    },
    "deepcoder learning to write programs": {
        "id": "1611.01989",
        "depth": 1,
        "children_titles": [
            "deepmath deep sequence models for premise selection",
            "adaptive neural compilation",
            "the helmholtz machine",
            "on label dependence and loss minimization in multi label classification",
            "bayes optimal multilabel classification via probabilistic classifier chains",
            "synthesizing data structure transformations from input output examples",
            "terpret a probabilistic programming language for program induction",
            "neural turing machines",
            "hybrid computing using a neural network with dynamic external memory",
            "learning to transduce with unbounded memory",
            "programming by examples applications algorithms and ambiguity resolution",
            "synthesis of loop free programs",
            "learning to pass expectation propagation messages",
            "the informed sampler a discriminative approach to bayesian inference in generative computer vision models",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "neural gpus learn algorithms",
            "stochastic gradient vb and the variational auto encoder",
            "neural random access machines",
            "gated graph sequence neural networks",
            "latent predictor networks for code generation",
            "deep network guided proof search",
            "a machine learning framework for programming by example",
            "neural programmer inducing latent programs with gradient descent",
            "learning program embeddings to propagate feedback on student code",
            "flashmeta a framework for inductive program synthesis",
            "neural programmer interpreters",
            "programming with a differentiable forth interpreter",
            "stochastic program optimization",
            "real time human pose recognition in parts from single depth images",
            "predicting a correct program in programming by example",
            "program synthesis by sketching",
            "learning stochastic inverses",
            "end to end memory networks",
            "memory networks",
            "learning simple algorithms from examples"
        ],
        "status": "expanded",
        "title_full": "Deepcoder: Learning to write programs",
        "link": "https://arxiv.org/abs/1611.01989",
        "n_parents": 1,
        "year": "2016",
        "children_full_dicts": [
            {
                "ref_title_clean": "deepmath deep sequence models for premise selection",
                "ref_title_full": "DeepMath deep sequence models for premise selection",
                "year": "2016",
                "full_block": "{DeepMath} - deep sequence models for premise selection.\n"
            },
            {
                "ref_title_clean": "adaptive neural compilation",
                "ref_title_full": "Adaptive neural compilation",
                "year": "2016",
                "full_block": "Adaptive neural compilation.\n"
            },
            {
                "ref_title_clean": "the helmholtz machine",
                "ref_title_full": "The Helmholtz machine",
                "year": "1995",
                "full_block": "The {H}elmholtz machine.\n"
            },
            {
                "ref_title_clean": "on label dependence and loss minimization in multi label classification",
                "ref_title_full": "On label dependence and loss minimization in multi label classification",
                "year": "2012",
                "full_block": "On label dependence and loss minimization in multi-label\n  classification.\n"
            },
            {
                "ref_title_clean": "bayes optimal multilabel classification via probabilistic classifier chains",
                "ref_title_full": "Bayes optimal multilabel classification via probabilistic classifier chains",
                "year": "2010",
                "full_block": "Bayes optimal multilabel classification via probabilistic classifier\n  chains.\n"
            },
            {
                "ref_title_clean": "synthesizing data structure transformations from input output examples",
                "ref_title_full": "Synthesizing data structure transformations from input output examples",
                "year": "2015",
                "full_block": "Synthesizing data structure transformations from input-output\n  examples.\n"
            },
            {
                "ref_title_clean": "terpret a probabilistic programming language for program induction",
                "ref_title_full": "Terpret: A probabilistic programming language for program induction",
                "year": "2016",
                "full_block": "Terpret: A probabilistic programming language for program induction.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural Turing machines",
                "year": "2014",
                "full_block": "Neural {T}uring machines.\n"
            },
            {
                "ref_title_clean": "hybrid computing using a neural network with dynamic external memory",
                "ref_title_full": "Hybrid computing using a neural network with dynamic external memory",
                "year": "2016",
                "full_block": "Hybrid computing using a neural network with dynamic external memory.\n"
            },
            {
                "ref_title_clean": "learning to transduce with unbounded memory",
                "ref_title_full": "Learning to transduce with unbounded memory",
                "year": "2015",
                "full_block": "Learning to transduce with unbounded memory.\n"
            },
            {
                "ref_title_clean": "programming by examples applications algorithms and ambiguity resolution",
                "ref_title_full": "Programming by examples: Applications, algorithms, and ambiguity resolution",
                "year": "2016",
                "full_block": "Programming by examples: Applications, algorithms, and ambiguity\n  resolution.\n"
            },
            {
                "ref_title_clean": "synthesis of loop free programs",
                "ref_title_full": "Synthesis of loop free programs",
                "year": "2011",
                "full_block": "Synthesis of loop-free programs.\n"
            },
            {
                "ref_title_clean": "learning to pass expectation propagation messages",
                "ref_title_full": "Learning to pass expectation propagation messages",
                "year": "2013",
                "full_block": "Learning to pass expectation propagation messages.\n"
            },
            {
                "ref_title_clean": "the informed sampler a discriminative approach to bayesian inference in generative computer vision models",
                "ref_title_full": "The informed sampler: A discriminative approach to Bayesian inference in generative computer vision models",
                "year": "2015",
                "full_block": "The informed sampler: A discriminative approach to {B}ayesian\n  inference in generative computer vision models.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "neural gpus learn algorithms",
                "ref_title_full": "Neural GPUs learn algorithms",
                "year": "2016",
                "full_block": "Neural {GPU}s learn algorithms.\n"
            },
            {
                "ref_title_clean": "stochastic gradient vb and the variational auto encoder",
                "ref_title_full": "Stochastic gradient VB and the variational auto encoder",
                "year": "2014",
                "full_block": "Stochastic gradient {VB} and the variational auto-encoder.\n"
            },
            {
                "ref_title_clean": "neural random access machines",
                "ref_title_full": "Neural random access machines",
                "year": "2015",
                "full_block": "Neural random-access machines.\n"
            },
            {
                "ref_title_clean": "gated graph sequence neural networks",
                "ref_title_full": "Gated graph sequence neural networks",
                "year": "2016",
                "full_block": "Gated graph sequence neural networks.\n"
            },
            {
                "ref_title_clean": "latent predictor networks for code generation",
                "ref_title_full": "Latent predictor networks for code generation",
                "year": "2016",
                "full_block": "Latent predictor networks for code generation.\n"
            },
            {
                "ref_title_clean": "deep network guided proof search",
                "ref_title_full": "Deep network guided proof search",
                "year": "2017",
                "full_block": "Deep network guided proof search.\n"
            },
            {
                "ref_title_clean": "a machine learning framework for programming by example",
                "ref_title_full": "A machine learning framework for programming by example",
                "year": "2013",
                "full_block": "A machine learning framework for programming by example.\n"
            },
            {
                "ref_title_clean": "neural programmer inducing latent programs with gradient descent",
                "ref_title_full": "Neural programmer: Inducing latent programs with gradient descent",
                "year": "2016",
                "full_block": "Neural programmer: Inducing latent programs with gradient descent.\n"
            },
            {
                "ref_title_clean": "learning program embeddings to propagate feedback on student code",
                "ref_title_full": "Learning program embeddings to propagate feedback on student code",
                "year": "2015",
                "full_block": "Learning program embeddings to propagate feedback on student code.\n"
            },
            {
                "ref_title_clean": "flashmeta a framework for inductive program synthesis",
                "ref_title_full": "FlashMeta: a framework for inductive program synthesis",
                "year": "2015",
                "full_block": "{FlashMeta}: a framework for inductive program synthesis.\n"
            },
            {
                "ref_title_clean": "neural programmer interpreters",
                "ref_title_full": "Neural programmer interpreters",
                "year": "2016",
                "full_block": "Neural programmer-interpreters.\n"
            },
            {
                "ref_title_clean": "programming with a differentiable forth interpreter",
                "ref_title_full": "Programming with a differentiable forth interpreter",
                "year": "2016",
                "full_block": "Programming with a differentiable forth interpreter.\n"
            },
            {
                "ref_title_clean": "stochastic program optimization",
                "ref_title_full": "Stochastic program optimization",
                "year": "2016",
                "full_block": "Stochastic program optimization.\n"
            },
            {
                "ref_title_clean": "real time human pose recognition in parts from single depth images",
                "ref_title_full": "Real time human pose recognition in parts from single depth images",
                "year": "2013",
                "full_block": "Real-time human pose recognition in parts from single depth images.\n"
            },
            {
                "ref_title_clean": "predicting a correct program in programming by example",
                "ref_title_full": "Predicting a correct program in programming by example",
                "year": "2015",
                "full_block": "Predicting a correct program in programming by example.\n"
            },
            {
                "ref_title_clean": "program synthesis by sketching",
                "ref_title_full": "\\Program Synthesis By Sketching",
                "year": "2008",
                "full_block": "\\emph{Program Synthesis By Sketching}.\n"
            },
            {
                "ref_title_clean": "learning stochastic inverses",
                "ref_title_full": "Learning stochastic inverses",
                "year": "2013",
                "full_block": "Learning stochastic inverses.\n"
            },
            {
                "ref_title_clean": "end to end memory networks",
                "ref_title_full": "End to end memory networks",
                "year": "2015",
                "full_block": "End-to-end memory networks.\n"
            },
            {
                "ref_title_clean": "memory networks",
                "ref_title_full": "Memory networks",
                "year": "2015",
                "full_block": "Memory networks.\n"
            },
            {
                "ref_title_clean": "learning simple algorithms from examples",
                "ref_title_full": "Learning simple algorithms from examples",
                "year": "2016",
                "full_block": "Learning simple algorithms from examples.\n"
            }
        ],
        "refs_source": "unpacked_sources/1611.01989/main.bbl"
    },
    "terpret a probabilistic programming language for program induction": {
        "id": "1608.04428",
        "depth": 1,
        "children_titles": [
            "control flow analysis",
            "syntax guided synthesis",
            "the toolsmt lib standard version 25",
            "learning long term dependencies with gradient descent is difficult",
            "the inference of regular lisp programs from examples",
            "adaptive neural compilation",
            "stan a probabilistic programming language",
            "on the properties of neural machine translation encoder decoder approaches",
            "bounded model checking using satisfiability solving",
            "using prior knowledge in a nnpda to learn context free languages",
            "identifying and attacking the saddle point problem in high dimensional non convex optimization",
            "z3 an efficient smt solver",
            "unsupervised learning by program synthesis",
            "higher order recurrent networks and grammatical inference",
            "church a language for generative models",
            "generating sequences with recurrent neural networks",
            "neural turing machines",
            "learning to transduce with unbounded memory",
            "automating string processing in spreadsheets using input output examples",
            "program verification as probabilistic inference",
            "spreadsheet data manipulation using examples",
            "long short term memory",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "neural gpus learn algorithms",
            "a clockwork rnn",
            "complete functional synthesis",
            "neural random access machines",
            "human level concept learning through probabilistic program induction",
            "llvm a compilation framework for lifelong program analysis  transformation",
            "learning longer memory in recurrent neural networks",
            "microsoft research cambridge httpresearchmicrosoftcominfernet",
            "gates",
            "a connectionist symbol manipulator that discovers the structure of context free languages",
            "neural programmer inducing latent programs with gradient descent",
            "adding gradient noise improves learning for very deep networks",
            "automatic sampler discovery via probabilistic programming and approximate bayesian computation",
            "chlorophyll synthesis aided compiler for low power spatial architectures",
            "flashmeta a framework for inductive program synthesis",
            "learning programs from noisy data",
            "neural programmer interpreters",
            "counterexample guided quantifier instantiation for synthesis in smt",
            "programming with a differentiable forth interpreter",
            "stochastic superoptimization",
            "syntactic analysis of two dimensional visual signals in the presence of noise",
            "blinkfill semi supervised programming by example for syntactic string transformations",
            "automated feedback generation for introductory programming assignments",
            "program synthesis by sketching",
            "programming by sketching for bit streaming programs",
            "combinatorial sketching for finite programs",
            "tightening lp relaxations for map using message passing",
            "url urlhttpmc stanorg",
            "end to end memory networks",
            "a methodology for lisp program construction from examples",
            "lecture 65 rmsprop divide the gradient by a running average of its recent magnitude",
            "transit specifying protocols with concolic snippets",
            "graphical models exponential families and variational inference",
            "a linear programming approach to max sum problem a review",
            "memory networks",
            "learning simple algorithms from examples"
        ],
        "status": "expanded",
        "title_full": "Terpret: A probabilistic programming language for program induction",
        "link": "https://arxiv.org/abs/1608.04428",
        "n_parents": 4,
        "year": "2016",
        "children_full_dicts": [
            {
                "ref_title_clean": "control flow analysis",
                "ref_title_full": "Control flow analysis",
                "year": "1970",
                "full_block": "Control flow analysis.\n"
            },
            {
                "ref_title_clean": "syntax guided synthesis",
                "ref_title_full": "Syntax guided synthesis",
                "year": "2015",
                "full_block": "Syntax-guided synthesis.\n"
            },
            {
                "ref_title_clean": "the toolsmt lib standard version 25",
                "ref_title_full": "The \\toolSMT LIB standard: Version 2.5",
                "year": "2015",
                "full_block": "The \\tool{SMT-LIB} standard: Version 2.5.\n"
            },
            {
                "ref_title_clean": "learning long term dependencies with gradient descent is difficult",
                "ref_title_full": "Learning long term dependencies with gradient descent is difficult",
                "year": "1994",
                "full_block": "Learning long-term dependencies with gradient descent is difficult.\n"
            },
            {
                "ref_title_clean": "the inference of regular lisp programs from examples",
                "ref_title_full": "The inference of regular lisp programs from examples",
                "year": "1978",
                "full_block": "The inference of regular lisp programs from examples.\n"
            },
            {
                "ref_title_clean": "adaptive neural compilation",
                "ref_title_full": "Adaptive neural compilation",
                "year": "2016",
                "full_block": "Adaptive neural compilation.\n"
            },
            {
                "ref_title_clean": "stan a probabilistic programming language",
                "ref_title_full": "Stan: A probabilistic programming language",
                "year": "2015",
                "full_block": "Stan: A probabilistic programming language.\n"
            },
            {
                "ref_title_clean": "on the properties of neural machine translation encoder decoder approaches",
                "ref_title_full": "On the properties of neural machine translation: Encoder decoder approaches",
                "year": "2014",
                "full_block": "On the properties of neural machine translation: Encoder-decoder\n  approaches.\n"
            },
            {
                "ref_title_clean": "bounded model checking using satisfiability solving",
                "ref_title_full": "Bounded model checking using satisfiability solving",
                "year": "2001",
                "full_block": "Bounded model checking using satisfiability solving.\n"
            },
            {
                "ref_title_clean": "using prior knowledge in a nnpda to learn context free languages",
                "ref_title_full": "Using prior knowledge in a \\NNPDA\\ to learn context free languages",
                "year": "1992",
                "full_block": "Using prior knowledge in a \\{NNPDA\\} to learn context-free languages.\n"
            },
            {
                "ref_title_clean": "identifying and attacking the saddle point problem in high dimensional non convex optimization",
                "ref_title_full": "Identifying and attacking the saddle point problem in high dimensional non convex optimization",
                "year": "2014",
                "full_block": "Identifying and attacking the saddle point problem in\n  high-dimensional non-convex optimization.\n"
            },
            {
                "ref_title_clean": "z3 an efficient smt solver",
                "ref_title_full": "Z3: an efficient SMT solver",
                "year": "2008",
                "full_block": "{Z3:} an efficient {SMT} solver.\n"
            },
            {
                "ref_title_clean": "unsupervised learning by program synthesis",
                "ref_title_full": "Unsupervised learning by program synthesis",
                "year": "2015",
                "full_block": "Unsupervised learning by program synthesis.\n"
            },
            {
                "ref_title_clean": "higher order recurrent networks and grammatical inference",
                "ref_title_full": "Higher order recurrent networks and grammatical inference",
                "year": "1989",
                "full_block": "Higher order recurrent networks and grammatical inference.\n"
            },
            {
                "ref_title_clean": "church a language for generative models",
                "ref_title_full": "Church: a language for generative models",
                "year": "2008",
                "full_block": "Church: a language for generative models.\n"
            },
            {
                "ref_title_clean": "generating sequences with recurrent neural networks",
                "ref_title_full": "Generating sequences with recurrent neural networks",
                "year": "2013",
                "full_block": "Generating sequences with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural turing machines",
                "year": "2014",
                "full_block": "Neural turing machines.\n"
            },
            {
                "ref_title_clean": "learning to transduce with unbounded memory",
                "ref_title_full": "Learning to transduce with unbounded memory",
                "year": "2015",
                "full_block": "Learning to transduce with unbounded memory.\n"
            },
            {
                "ref_title_clean": "automating string processing in spreadsheets using input output examples",
                "ref_title_full": "Automating string processing in spreadsheets using input output examples",
                "year": "2011",
                "full_block": "Automating string processing in spreadsheets using input-output\n  examples.\n"
            },
            {
                "ref_title_clean": "program verification as probabilistic inference",
                "ref_title_full": "Program verification as probabilistic inference",
                "year": "2007",
                "full_block": "Program verification as probabilistic inference.\n"
            },
            {
                "ref_title_clean": "spreadsheet data manipulation using examples",
                "ref_title_full": "Spreadsheet data manipulation using examples",
                "year": "2012",
                "full_block": "Spreadsheet data manipulation using examples.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "neural gpus learn algorithms",
                "ref_title_full": "Neural gpus learn algorithms",
                "year": "2016",
                "full_block": "Neural gpus learn algorithms.\n"
            },
            {
                "ref_title_clean": "a clockwork rnn",
                "ref_title_full": "A clockwork RNN",
                "year": "2014",
                "full_block": "A clockwork {RNN}.\n"
            },
            {
                "ref_title_clean": "complete functional synthesis",
                "ref_title_full": "Complete functional synthesis",
                "year": "2010",
                "full_block": "Complete functional synthesis.\n"
            },
            {
                "ref_title_clean": "neural random access machines",
                "ref_title_full": "Neural random access machines",
                "year": "2015",
                "full_block": "Neural random-access machines.\n"
            },
            {
                "ref_title_clean": "human level concept learning through probabilistic program induction",
                "ref_title_full": "Human level concept learning through probabilistic program induction",
                "year": "2015",
                "full_block": "Human-level concept learning through probabilistic program induction.\n"
            },
            {
                "ref_title_clean": "llvm a compilation framework for lifelong program analysis  transformation",
                "ref_title_full": "Llvm: A compilation framework for lifelong program analysis \\& transformation",
                "year": "none",
                "full_block": "Llvm: A compilation framework for lifelong program analysis \\&\n  transformation.\n"
            },
            {
                "ref_title_clean": "learning longer memory in recurrent neural networks",
                "ref_title_full": "Learning longer memory in recurrent neural networks",
                "year": "2015",
                "full_block": "Learning longer memory in recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "microsoft research cambridge httpresearchmicrosoftcominfernet",
                "ref_title_full": "Microsoft Research Cambridge. http://research.microsoft.com/infernet",
                "year": "2014",
                "full_block": "Microsoft Research Cambridge. http://research.microsoft.com/infernet.\n\n"
            },
            {
                "ref_title_clean": "gates",
                "ref_title_full": "Gates",
                "year": "2009",
                "full_block": "Gates.\n"
            },
            {
                "ref_title_clean": "a connectionist symbol manipulator that discovers the structure of context free languages",
                "ref_title_full": "A connectionist symbol manipulator that discovers the structure of context free languages",
                "year": "1992",
                "full_block": "A connectionist symbol manipulator that discovers the structure of\n  context-free languages.\n"
            },
            {
                "ref_title_clean": "neural programmer inducing latent programs with gradient descent",
                "ref_title_full": "Neural programmer: Inducing latent programs with gradient descent",
                "year": "none",
                "full_block": "Neural programmer: Inducing latent programs with gradient descent.\n"
            },
            {
                "ref_title_clean": "adding gradient noise improves learning for very deep networks",
                "ref_title_full": "Adding gradient noise improves learning for very deep networks",
                "year": "none",
                "full_block": "Adding gradient noise improves learning for very deep networks.\n"
            },
            {
                "ref_title_clean": "automatic sampler discovery via probabilistic programming and approximate bayesian computation",
                "ref_title_full": "Automatic sampler discovery via probabilistic programming and approximate bayesian computation",
                "year": "2016",
                "full_block": "Automatic sampler discovery via probabilistic programming and\n  approximate bayesian computation.\n"
            },
            {
                "ref_title_clean": "chlorophyll synthesis aided compiler for low power spatial architectures",
                "ref_title_full": "Chlorophyll: synthesis aided compiler for low power spatial architectures",
                "year": "2014",
                "full_block": "Chlorophyll: synthesis-aided compiler for low-power spatial\n  architectures.\n"
            },
            {
                "ref_title_clean": "flashmeta a framework for inductive program synthesis",
                "ref_title_full": "Flashmeta: a framework for inductive program synthesis",
                "year": "2015",
                "full_block": "Flashmeta: a framework for inductive program synthesis.\n"
            },
            {
                "ref_title_clean": "learning programs from noisy data",
                "ref_title_full": "Learning programs from noisy data",
                "year": "2016",
                "full_block": "Learning programs from noisy data.\n"
            },
            {
                "ref_title_clean": "neural programmer interpreters",
                "ref_title_full": "Neural programmer interpreters",
                "year": "none",
                "full_block": "Neural programmer-interpreters.\n"
            },
            {
                "ref_title_clean": "counterexample guided quantifier instantiation for synthesis in smt",
                "ref_title_full": "Counterexample guided quantifier instantiation for synthesis in SMT",
                "year": "2015",
                "full_block": "Counterexample-guided quantifier instantiation for synthesis in\n  {SMT}.\n"
            },
            {
                "ref_title_clean": "programming with a differentiable forth interpreter",
                "ref_title_full": "Programming with a differentiable forth interpreter",
                "year": "2016",
                "full_block": "Programming with a differentiable forth interpreter.\n"
            },
            {
                "ref_title_clean": "stochastic superoptimization",
                "ref_title_full": "Stochastic superoptimization",
                "year": "2013",
                "full_block": "Stochastic superoptimization.\n"
            },
            {
                "ref_title_clean": "syntactic analysis of two dimensional visual signals in the presence of noise",
                "ref_title_full": "Syntactic analysis of two dimensional visual signals in the presence of noise",
                "year": "1976",
                "full_block": "Syntactic analysis of two-dimensional visual signals in the presence\n  of noise.\n"
            },
            {
                "ref_title_clean": "blinkfill semi supervised programming by example for syntactic string transformations",
                "ref_title_full": "Blinkfill: Semi supervised programming by example for syntactic string transformations",
                "year": "2016",
                "full_block": "Blinkfill: Semi-supervised programming by example for syntactic\n  string transformations.\n"
            },
            {
                "ref_title_clean": "automated feedback generation for introductory programming assignments",
                "ref_title_full": "Automated feedback generation for introductory programming assignments",
                "year": "2013",
                "full_block": "Automated feedback generation for introductory programming\n  assignments.\n"
            },
            {
                "ref_title_clean": "program synthesis by sketching",
                "ref_title_full": "\\Program Synthesis By Sketching",
                "year": "2008",
                "full_block": "\\emph{Program Synthesis By Sketching}.\n"
            },
            {
                "ref_title_clean": "programming by sketching for bit streaming programs",
                "ref_title_full": "Programming by sketching for bit streaming programs",
                "year": "2005",
                "full_block": "Programming by sketching for bit-streaming programs.\n"
            },
            {
                "ref_title_clean": "combinatorial sketching for finite programs",
                "ref_title_full": "Combinatorial sketching for finite programs",
                "year": "2006",
                "full_block": "Combinatorial sketching for finite programs.\n"
            },
            {
                "ref_title_clean": "tightening lp relaxations for map using message passing",
                "ref_title_full": "Tightening lp relaxations for map using message passing",
                "year": "2008",
                "full_block": "Tightening lp relaxations for map using message passing.\n"
            },
            {
                "ref_title_clean": "url urlhttpmc stanorg",
                "ref_title_full": "URL \\urlhttp://mc stan.org/",
                "year": "2015",
                "full_block": "URL \\url{http://mc-stan.org/}.\n\n"
            },
            {
                "ref_title_clean": "end to end memory networks",
                "ref_title_full": "End to end memory networks",
                "year": "2015",
                "full_block": "End-to-end memory networks.\n"
            },
            {
                "ref_title_clean": "a methodology for lisp program construction from examples",
                "ref_title_full": "A methodology for lisp program construction from examples",
                "year": "1977",
                "full_block": "A methodology for lisp program construction from examples.\n"
            },
            {
                "ref_title_clean": "lecture 65 rmsprop divide the gradient by a running average of its recent magnitude",
                "ref_title_full": "Lecture 6.5 RmsProp: Divide the gradient by a running average of its recent magnitude",
                "year": "2012",
                "full_block": "{Lecture 6.5---RmsProp: Divide the gradient by a running average of\n  its recent magnitude}.\n"
            },
            {
                "ref_title_clean": "transit specifying protocols with concolic snippets",
                "ref_title_full": "TRANSIT: specifying protocols with concolic snippets",
                "year": "2013",
                "full_block": "{TRANSIT:} specifying protocols with concolic snippets.\n"
            },
            {
                "ref_title_clean": "graphical models exponential families and variational inference",
                "ref_title_full": "Graphical models, exponential families, and variational inference",
                "year": "2008",
                "full_block": "Graphical models, exponential families, and variational inference.\n"
            },
            {
                "ref_title_clean": "a linear programming approach to max sum problem a review",
                "ref_title_full": "A linear programming approach to max sum problem: A review",
                "year": "2007",
                "full_block": "A linear programming approach to max-sum problem: A review.\n"
            },
            {
                "ref_title_clean": "memory networks",
                "ref_title_full": "Memory networks",
                "year": "2014",
                "full_block": "Memory networks.\n"
            },
            {
                "ref_title_clean": "learning simple algorithms from examples",
                "ref_title_full": "Learning simple algorithms from examples",
                "year": "2016",
                "full_block": "Learning simple algorithms from examples.\n"
            }
        ],
        "refs_source": "unpacked_sources/1608.04428/main.bbl"
    },
    "neural turing machines": {
        "id": "1410.5401",
        "depth": 1,
        "children_titles": [
            "memory",
            "neural machine translation by jointly learning to align and translate",
            "time constraints and resource sharing in adults working memory spans",
            "three models for the description of language",
            "learning context free grammars capabilities and limitations of a recurrent neural network with an external stack memory",
            "simple substrates for complex cognition",
            "how to build a brain a neural architecture for biological cognition",
            "the evolution of the language faculty clarifications and implications",
            "connectionism and cognitive architecture a critical analysis",
            "a general framework for adaptive processing of data structures",
            "memory and the computational brain why cognitive science will transform neuroscience volume3",
            "cellular basis of working memory",
            "generating sequences with recurrent neural networks",
            "towards end to end speech recognition with recurrent neural networks",
            "speech recognition with deep recurrent neural networks",
            "the problem of rapid variable creation",
            "banishing the homunculus making working memory work",
            "learning distributed representations of concepts",
            "gradient flow in recurrent nets the difficulty of learning long term dependencies",
            "long short term memory",
            "learning to learn using gradient descent",
            "neural networks and physical systems with emergent collective computational abilities",
            "the nature of the language faculty and its implications for evolution of language reply to fitch hauser and chomsky",
            "hyperdimensional computing an introduction to computing in distributed representation with high dimensional random vectors",
            "the algebraic mind integrating connectionism and cognitive science",
            "the magical number seven plus or minus two some limits on our capacity for processing information",
            "the cognitive revolution a historical perspective",
            "computation finite and infinite machines",
            "machine learning a probabilistic perspective",
            "holographic reduced representation distributed representation for cognitive structures",
            "recursive distributed representations",
            "the importance of mixed selectivity in complex cognitive tasks",
            "parallel distributed processing volume1",
            "continuous attractors and oculomotor control",
            "on the computational power of neural nets",
            "tensor product variable binding and the representation of symbolic structures in connectionist systems",
            "semantic compositionality through recursive matrix vector spaces",
            "generating text with recurrent neural networks",
            "sequence to sequence learning with neural networks",
            "boltzcons dynamic symbol structures in a connectionist network",
            "first draft of a report on the edvac",
            "synaptic basis of cortical persistent activity the importance of nmda receptors to working memory"
        ],
        "status": "expanded",
        "title_full": "Neural turing machines",
        "link": "https://arxiv.org/abs/1410.5401",
        "n_parents": 9,
        "year": "2014",
        "children_full_dicts": [
            {
                "ref_title_clean": "memory",
                "ref_title_full": "Memory",
                "year": "none",
                "full_block": "{\\em Memory}.\n"
            },
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "none",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "time constraints and resource sharing in adults working memory spans",
                "ref_title_full": "Time constraints and resource sharing in adults' working memory spans",
                "year": "none",
                "full_block": "Time constraints and resource sharing in adults' working memory\n  spans.\n"
            },
            {
                "ref_title_clean": "three models for the description of language",
                "ref_title_full": "Three models for the description of language",
                "year": "none",
                "full_block": "Three models for the description of language.\n"
            },
            {
                "ref_title_clean": "learning context free grammars capabilities and limitations of a recurrent neural network with an external stack memory",
                "ref_title_full": "Learning context free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory",
                "year": "none",
                "full_block": "Learning context-free grammars: Capabilities and limitations of a\n  recurrent neural network with an external stack memory.\n"
            },
            {
                "ref_title_clean": "simple substrates for complex cognition",
                "ref_title_full": "Simple substrates for complex cognition",
                "year": "none",
                "full_block": "Simple substrates for complex cognition.\n"
            },
            {
                "ref_title_clean": "how to build a brain a neural architecture for biological cognition",
                "ref_title_full": "How to build a brain: A neural architecture for biological cognition",
                "year": "none",
                "full_block": "{\\em How to build a brain: A neural architecture for biological\n  cognition}.\n"
            },
            {
                "ref_title_clean": "the evolution of the language faculty clarifications and implications",
                "ref_title_full": "The evolution of the language faculty: clarifications and implications",
                "year": "none",
                "full_block": "The evolution of the language faculty: clarifications and\n  implications.\n"
            },
            {
                "ref_title_clean": "connectionism and cognitive architecture a critical analysis",
                "ref_title_full": "Connectionism and cognitive architecture: A critical analysis",
                "year": "none",
                "full_block": "Connectionism and cognitive architecture: A critical analysis.\n"
            },
            {
                "ref_title_clean": "a general framework for adaptive processing of data structures",
                "ref_title_full": "A general framework for adaptive processing of data structures",
                "year": "none",
                "full_block": "A general framework for adaptive processing of data structures.\n"
            },
            {
                "ref_title_clean": "memory and the computational brain why cognitive science will transform neuroscience volume3",
                "ref_title_full": "Memory and the computational brain: Why cognitive science will transform neuroscience, volume~3",
                "year": "none",
                "full_block": "{\\em Memory and the computational brain: Why cognitive science will\n  transform neuroscience}, volume~3.\n"
            },
            {
                "ref_title_clean": "cellular basis of working memory",
                "ref_title_full": "Cellular basis of working memory",
                "year": "none",
                "full_block": "Cellular basis of working memory.\n"
            },
            {
                "ref_title_clean": "generating sequences with recurrent neural networks",
                "ref_title_full": "Generating sequences with recurrent neural networks",
                "year": "none",
                "full_block": "Generating sequences with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "towards end to end speech recognition with recurrent neural networks",
                "ref_title_full": "Towards end to end speech recognition with recurrent neural networks",
                "year": "none",
                "full_block": "Towards end-to-end speech recognition with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "speech recognition with deep recurrent neural networks",
                "ref_title_full": "Speech recognition with deep recurrent neural networks",
                "year": "none",
                "full_block": "Speech recognition with deep recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "the problem of rapid variable creation",
                "ref_title_full": "The problem of rapid variable creation",
                "year": "none",
                "full_block": "The problem of rapid variable creation.\n"
            },
            {
                "ref_title_clean": "banishing the homunculus making working memory work",
                "ref_title_full": "Banishing the homunculus: making working memory work",
                "year": "none",
                "full_block": "Banishing the homunculus: making working memory work.\n"
            },
            {
                "ref_title_clean": "learning distributed representations of concepts",
                "ref_title_full": "Learning distributed representations of concepts",
                "year": "none",
                "full_block": "Learning distributed representations of concepts.\n"
            },
            {
                "ref_title_clean": "gradient flow in recurrent nets the difficulty of learning long term dependencies",
                "ref_title_full": "Gradient flow in recurrent nets: the difficulty of learning long term dependencies",
                "year": "none",
                "full_block": "Gradient flow in recurrent nets: the difficulty of learning long-term\n  dependencies.\n\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "none",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "learning to learn using gradient descent",
                "ref_title_full": "Learning to learn using gradient descent",
                "year": "none",
                "full_block": "Learning to learn using gradient descent.\n"
            },
            {
                "ref_title_clean": "neural networks and physical systems with emergent collective computational abilities",
                "ref_title_full": "Neural networks and physical systems with emergent collective computational abilities",
                "year": "none",
                "full_block": "Neural networks and physical systems with emergent collective\n  computational abilities.\n"
            },
            {
                "ref_title_clean": "the nature of the language faculty and its implications for evolution of language reply to fitch hauser and chomsky",
                "ref_title_full": "The nature of the language faculty and its implications for evolution of language (reply to fitch, hauser, and chomsky)",
                "year": "none",
                "full_block": "The nature of the language faculty and its implications for evolution\n  of language (reply to fitch, hauser, and chomsky).\n"
            },
            {
                "ref_title_clean": "hyperdimensional computing an introduction to computing in distributed representation with high dimensional random vectors",
                "ref_title_full": "Hyperdimensional computing: An introduction to computing in distributed representation with high dimensional random vectors",
                "year": "none",
                "full_block": "Hyperdimensional computing: An introduction to computing in\n  distributed representation with high-dimensional random vectors.\n"
            },
            {
                "ref_title_clean": "the algebraic mind integrating connectionism and cognitive science",
                "ref_title_full": "The algebraic mind: Integrating connectionism and cognitive science",
                "year": "none",
                "full_block": "{\\em The algebraic mind: Integrating connectionism and cognitive\n  science}.\n"
            },
            {
                "ref_title_clean": "the magical number seven plus or minus two some limits on our capacity for processing information",
                "ref_title_full": "The magical number seven, plus or minus two: some limits on our capacity for processing information",
                "year": "none",
                "full_block": "The magical number seven, plus or minus two: some limits on our\n  capacity for processing information.\n"
            },
            {
                "ref_title_clean": "the cognitive revolution a historical perspective",
                "ref_title_full": "The cognitive revolution: a historical perspective",
                "year": "none",
                "full_block": "The cognitive revolution: a historical perspective.\n"
            },
            {
                "ref_title_clean": "computation finite and infinite machines",
                "ref_title_full": "Computation: finite and infinite machines",
                "year": "none",
                "full_block": "{\\em Computation: finite and infinite machines}.\n"
            },
            {
                "ref_title_clean": "machine learning a probabilistic perspective",
                "ref_title_full": "Machine learning: a probabilistic perspective",
                "year": "none",
                "full_block": "{\\em Machine learning: a probabilistic perspective}.\n"
            },
            {
                "ref_title_clean": "holographic reduced representation distributed representation for cognitive structures",
                "ref_title_full": "Holographic Reduced Representation: Distributed representation for cognitive structures",
                "year": "none",
                "full_block": "{\\em Holographic Reduced Representation: Distributed representation\n  for cognitive structures}.\n"
            },
            {
                "ref_title_clean": "recursive distributed representations",
                "ref_title_full": "Recursive distributed representations",
                "year": "none",
                "full_block": "Recursive distributed representations.\n"
            },
            {
                "ref_title_clean": "the importance of mixed selectivity in complex cognitive tasks",
                "ref_title_full": "The importance of mixed selectivity in complex cognitive tasks",
                "year": "none",
                "full_block": "The importance of mixed selectivity in complex cognitive tasks.\n"
            },
            {
                "ref_title_clean": "parallel distributed processing volume1",
                "ref_title_full": "Parallel distributed processing, volume~1",
                "year": "none",
                "full_block": "{\\em Parallel distributed processing}, volume~1.\n"
            },
            {
                "ref_title_clean": "continuous attractors and oculomotor control",
                "ref_title_full": "Continuous attractors and oculomotor control",
                "year": "none",
                "full_block": "Continuous attractors and oculomotor control.\n"
            },
            {
                "ref_title_clean": "on the computational power of neural nets",
                "ref_title_full": "On the computational power of neural nets",
                "year": "none",
                "full_block": "On the computational power of neural nets.\n"
            },
            {
                "ref_title_clean": "tensor product variable binding and the representation of symbolic structures in connectionist systems",
                "ref_title_full": "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
                "year": "none",
                "full_block": "Tensor product variable binding and the representation of symbolic\n  structures in connectionist systems.\n"
            },
            {
                "ref_title_clean": "semantic compositionality through recursive matrix vector spaces",
                "ref_title_full": "Semantic compositionality through recursive matrix vector spaces",
                "year": "none",
                "full_block": "Semantic compositionality through recursive matrix-vector spaces.\n"
            },
            {
                "ref_title_clean": "generating text with recurrent neural networks",
                "ref_title_full": "Generating text with recurrent neural networks",
                "year": "none",
                "full_block": "Generating text with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "none",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "boltzcons dynamic symbol structures in a connectionist network",
                "ref_title_full": "Boltzcons: Dynamic symbol structures in a connectionist network",
                "year": "none",
                "full_block": "Boltzcons: Dynamic symbol structures in a connectionist network.\n"
            },
            {
                "ref_title_clean": "first draft of a report on the edvac",
                "ref_title_full": "First draft of a report on the edvac",
                "year": "none",
                "full_block": "First draft of a report on the edvac.\n\n"
            },
            {
                "ref_title_clean": "synaptic basis of cortical persistent activity the importance of nmda receptors to working memory",
                "ref_title_full": "Synaptic basis of cortical persistent activity: the importance of nmda receptors to working memory",
                "year": "none",
                "full_block": "Synaptic basis of cortical persistent activity: the importance of\n  nmda receptors to working memory.\n"
            }
        ],
        "refs_source": "unpacked_sources/1410.5401/ntm_arxiv.bbl"
    },
    "hybrid computing using a neural network with dynamic external memory": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Hybrid computing using a neural network with dynamic external memory",
        "link": "none",
        "n_parents": 3,
        "year": "2016"
    },
    "automating string processing in spreadsheets using input output examples": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Automating string processing in spreadsheets using input output examples",
        "link": "none",
        "n_parents": 3,
        "year": "2011"
    },
    "spreadsheet data manipulation using examples": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Spreadsheet data manipulation using examples",
        "link": "none",
        "n_parents": 3,
        "year": "2012"
    },
    "attention based multimodal neural machine translation": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Attention based multimodal neural machine translation",
        "link": "none",
        "n_parents": 1,
        "year": "2016"
    },
    "inferring algorithmic patterns with stack augmented recurrent nets": {
        "id": "1503.01007",
        "depth": 1,
        "children_titles": [
            "scaling learning algorithms towards ai",
            "pattern recognition and machine learning",
            "context free and context sensitive dynamics in recurrent neural networks",
            "large scale machine learning with stochastic gradient descent",
            "random forests",
            "probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition",
            "learning phrase representations using rnn encoder decoder for statistical machine translation",
            "toward a connectionist model of recursion in human linguistic performance",
            "gated feedback recurrent neural networks",
            "high performance neural networks for visual object classification",
            "mechanisms for sentence processing",
            "context dependent pre trained deep neural networks for large vocabulary speech recognition",
            "learning context free grammars capabilities and limitations of a recurrent neural network with an external stack memory",
            "using prior knowledge in a nnpda to learn context free languages",
            "finding structure in time",
            "context free parsing in connectionist networks",
            "lstm recurrent networks learn simple context free and context sensitive languages",
            "neural turing machines",
            "a recurrent network that performs a context sensitive prediction task",
            "long short term memory",
            "designing a counter another case study of dynamics and activation landscapes in recurrent networks",
            "imagenet classification with deep convolutional neural networks",
            "gradient based learning applied to document recognition",
            "statistical language models based on neural networks",
            "learning longer memory in recurrent neural networks",
            "perceptrons",
            "a connectionist symbol manipulator that discovers the structure of context free languages",
            "the induction of dynamical recognizers",
            "hogwild a lock free approach to parallelizing stochastic gradient descent",
            "a recurrent neural network that learns to count",
            "learning internal representations by error propagation",
            "fractal encoding of context free grammars in connectionist networks",
            "generalization of backpropagation with application to a recurrent gas market model",
            "memory networks",
            "learning to count without a counter a case study of dynamics and activation landscapes in recurrent networks",
            "gradient based learning algorithms for recurrent networks and their computational complexity",
            "learning to execute",
            "discrete recurrent neural networks for grammatical inference"
        ],
        "status": "expanded",
        "title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
        "link": "https://arxiv.org/abs/1503.01007",
        "n_parents": 8,
        "year": "2015",
        "children_full_dicts": [
            {
                "ref_title_clean": "scaling learning algorithms towards ai",
                "ref_title_full": "Scaling learning algorithms towards ai",
                "year": "2007",
                "full_block": "Scaling learning algorithms towards ai.\n"
            },
            {
                "ref_title_clean": "pattern recognition and machine learning",
                "ref_title_full": "Pattern recognition and machine learning",
                "year": "2006",
                "full_block": "{\\em Pattern recognition and machine learning}.\n"
            },
            {
                "ref_title_clean": "context free and context sensitive dynamics in recurrent neural networks",
                "ref_title_full": "Context free and context sensitive dynamics in recurrent neural networks",
                "year": "2000",
                "full_block": "Context-free and context-sensitive dynamics in recurrent neural\n  networks.\n"
            },
            {
                "ref_title_clean": "large scale machine learning with stochastic gradient descent",
                "ref_title_full": "Large scale machine learning with stochastic gradient descent",
                "year": "2010",
                "full_block": "Large-scale machine learning with stochastic gradient descent.\n"
            },
            {
                "ref_title_clean": "random forests",
                "ref_title_full": "Random forests",
                "year": "2001",
                "full_block": "Random forests.\n"
            },
            {
                "ref_title_clean": "probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition",
                "ref_title_full": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition",
                "year": "1990",
                "full_block": "Probabilistic interpretation of feedforward classification network\n  outputs, with relationships to statistical pattern recognition.\n"
            },
            {
                "ref_title_clean": "learning phrase representations using rnn encoder decoder for statistical machine translation",
                "ref_title_full": "Learning phrase representations using rnn encoder decoder for statistical machine translation",
                "year": "2014",
                "full_block": "Learning phrase representations using rnn encoder-decoder for\n  statistical machine translation.\n"
            },
            {
                "ref_title_clean": "toward a connectionist model of recursion in human linguistic performance",
                "ref_title_full": "Toward a connectionist model of recursion in human linguistic performance",
                "year": "1999",
                "full_block": "Toward a connectionist model of recursion in human linguistic\n  performance.\n"
            },
            {
                "ref_title_clean": "gated feedback recurrent neural networks",
                "ref_title_full": "Gated feedback recurrent neural networks",
                "year": "2015",
                "full_block": "Gated feedback recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "high performance neural networks for visual object classification",
                "ref_title_full": "High performance neural networks for visual object classification",
                "year": "2011",
                "full_block": "High-performance neural networks for visual object classification.\n"
            },
            {
                "ref_title_clean": "mechanisms for sentence processing",
                "ref_title_full": "Mechanisms for sentence processing",
                "year": "1996",
                "full_block": "{\\em Mechanisms for sentence processing}.\n"
            },
            {
                "ref_title_clean": "context dependent pre trained deep neural networks for large vocabulary speech recognition",
                "ref_title_full": "Context dependent pre trained deep neural networks for large vocabulary speech recognition",
                "year": "2012",
                "full_block": "Context-dependent pre-trained deep neural networks for\n  large-vocabulary speech recognition.\n"
            },
            {
                "ref_title_clean": "learning context free grammars capabilities and limitations of a recurrent neural network with an external stack memory",
                "ref_title_full": "Learning context free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory",
                "year": "1992",
                "full_block": "Learning context-free grammars: Capabilities and limitations of a\n  recurrent neural network with an external stack memory.\n"
            },
            {
                "ref_title_clean": "using prior knowledge in a nnpda to learn context free languages",
                "ref_title_full": "Using prior knowledge in a nnpda to learn context free languages",
                "year": "1993",
                "full_block": "Using prior knowledge in a nnpda to learn context-free languages.\n"
            },
            {
                "ref_title_clean": "finding structure in time",
                "ref_title_full": "Finding structure in time",
                "year": "1990",
                "full_block": "Finding structure in time.\n"
            },
            {
                "ref_title_clean": "context free parsing in connectionist networks",
                "ref_title_full": "Context free parsing in connectionist networks",
                "year": "1994",
                "full_block": "Context-free parsing in connectionist networks.\n"
            },
            {
                "ref_title_clean": "lstm recurrent networks learn simple context free and context sensitive languages",
                "ref_title_full": "Lstm recurrent networks learn simple context free and context sensitive languages",
                "year": "2001",
                "full_block": "Lstm recurrent networks learn simple context-free and\n  context-sensitive languages.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural turing machines",
                "year": "2014",
                "full_block": "Neural turing machines.\n"
            },
            {
                "ref_title_clean": "a recurrent network that performs a context sensitive prediction task",
                "ref_title_full": "A recurrent network that performs a context sensitive prediction task",
                "year": "1996",
                "full_block": "A recurrent network that performs a context-sensitive prediction\n  task.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "designing a counter another case study of dynamics and activation landscapes in recurrent networks",
                "ref_title_full": "Designing a counter: Another case study of dynamics and activation landscapes in recurrent networks",
                "year": "1997",
                "full_block": "Designing a counter: Another case study of dynamics and activation\n  landscapes in recurrent networks.\n"
            },
            {
                "ref_title_clean": "imagenet classification with deep convolutional neural networks",
                "ref_title_full": "Imagenet classification with deep convolutional neural networks",
                "year": "2012",
                "full_block": "Imagenet classification with deep convolutional neural networks.\n"
            },
            {
                "ref_title_clean": "gradient based learning applied to document recognition",
                "ref_title_full": "Gradient based learning applied to document recognition",
                "year": "none",
                "full_block": "Gradient-based learning applied to document recognition.\n"
            },
            {
                "ref_title_clean": "statistical language models based on neural networks",
                "ref_title_full": "Statistical language models based on neural networks",
                "year": "2012",
                "full_block": "{\\em Statistical language models based on neural networks}.\n"
            },
            {
                "ref_title_clean": "learning longer memory in recurrent neural networks",
                "ref_title_full": "Learning longer memory in recurrent neural networks",
                "year": "2014",
                "full_block": "Learning longer memory in recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "perceptrons",
                "ref_title_full": "Perceptrons",
                "year": "1969",
                "full_block": "{\\em Perceptrons}.\n"
            },
            {
                "ref_title_clean": "a connectionist symbol manipulator that discovers the structure of context free languages",
                "ref_title_full": "A connectionist symbol manipulator that discovers the structure of context free languages",
                "year": "1993",
                "full_block": "A connectionist symbol manipulator that discovers the structure of\n  context-free languages.\n"
            },
            {
                "ref_title_clean": "the induction of dynamical recognizers",
                "ref_title_full": "The induction of dynamical recognizers",
                "year": "1991",
                "full_block": "The induction of dynamical recognizers.\n"
            },
            {
                "ref_title_clean": "hogwild a lock free approach to parallelizing stochastic gradient descent",
                "ref_title_full": "Hogwild: A lock free approach to parallelizing stochastic gradient descent",
                "year": "2011",
                "full_block": "Hogwild: A lock-free approach to parallelizing stochastic gradient\n  descent.\n"
            },
            {
                "ref_title_clean": "a recurrent neural network that learns to count",
                "ref_title_full": "A recurrent neural network that learns to count",
                "year": "1999",
                "full_block": "A recurrent neural network that learns to count.\n"
            },
            {
                "ref_title_clean": "learning internal representations by error propagation",
                "ref_title_full": "Learning internal representations by error propagation",
                "year": "1985",
                "full_block": "Learning internal representations by error propagation.\n"
            },
            {
                "ref_title_clean": "fractal encoding of context free grammars in connectionist networks",
                "ref_title_full": "Fractal encoding of context free grammars in connectionist networks",
                "year": "2000",
                "full_block": "Fractal encoding of context-free grammars in connectionist networks.\n"
            },
            {
                "ref_title_clean": "generalization of backpropagation with application to a recurrent gas market model",
                "ref_title_full": "Generalization of backpropagation with application to a recurrent gas market model",
                "year": "1988",
                "full_block": "Generalization of backpropagation with application to a recurrent gas\n  market model.\n"
            },
            {
                "ref_title_clean": "memory networks",
                "ref_title_full": "Memory networks",
                "year": "2015",
                "full_block": "Memory networks.\n"
            },
            {
                "ref_title_clean": "learning to count without a counter a case study of dynamics and activation landscapes in recurrent networks",
                "ref_title_full": "Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks",
                "year": "1995",
                "full_block": "Learning to count without a counter: A case study of dynamics and\n  activation landscapes in recurrent networks.\n"
            },
            {
                "ref_title_clean": "gradient based learning algorithms for recurrent networks and their computational complexity",
                "ref_title_full": "Gradient based learning algorithms for recurrent networks and their computational complexity",
                "year": "1995",
                "full_block": "Gradient-based learning algorithms for recurrent networks and their\n  computational complexity.\n"
            },
            {
                "ref_title_clean": "learning to execute",
                "ref_title_full": "Learning to execute",
                "year": "2014",
                "full_block": "Learning to execute.\n"
            },
            {
                "ref_title_clean": "discrete recurrent neural networks for grammatical inference",
                "ref_title_full": "Discrete recurrent neural networks for grammatical inference",
                "year": "1994",
                "full_block": "Discrete recurrent neural networks for grammatical inference.\n"
            }
        ],
        "refs_source": "unpacked_sources/1503.01007/paper.bbl"
    },
    "neural gpus learn algorithms": {
        "id": "1511.08228",
        "depth": 1,
        "children_titles": [
            "r eferences angluin dana",
            "url httparxiv",
            "automatic structures",
            "listen attend and spell",
            "url httparxiv",
            "empirical evaluation of gated recurrent neural networks on sequence modeling",
            "url httparxiv",
            "context dependent pre trained deep neural networks for large vocabulary speech recognition",
            "url httparxiv",
            "8 \fpublished as a conference paper at iclr 2016 grefenstette edward hermann karl moritz suleyman mustafa and blunsom learning to transduce with unbounded memory",
            "httparxiv",
            "url httparxiv",
            "dimensions in program synthesis",
            "long short term memory",
            "url httparxiv",
            "url httparxiv",
            "url httparxiv",
            "inductive programming a survey of program synthesis techniques",
            "imagenet classification with deep convolutional neural network",
            "url httparxiv",
            "url httparxiv",
            "url httparxiv",
            "sequence to sequence learning with neural networks",
            "url httparxiv",
            "variable rate image compression with recurrent neural networks",
            "url httparxiv",
            "url httparxiv",
            "url httparxiv",
            "an introduction to cellular automata",
            "bayesian learning via stochastic gradient langevin dynamics",
            "httparxiv",
            "url httparxiv"
        ],
        "status": "expanded",
        "title_full": "Neural gpus learn algorithms",
        "link": "https://arxiv.org/abs/1511.08228",
        "n_parents": 5,
        "year": "2015",
        "children_full_dicts": [
            {
                "ref_title_clean": "r eferences angluin dana",
                "ref_title_full": "R EFERENCES Angluin, Dana",
                "year": "none",
                "full_block": "Published as a conference paper at ICLR 2016\n\nN EURAL GPU S L EARN A LGORITHMS\n\u0141ukasz Kaiser & Ilya Sutskever\nGoogle Brain {lukaszkaiser,ilyasu}@google.com\n\narXiv:1511.08228v3 [cs.LG] 15 Mar 2016\n\nA BSTRACT\nLearning an algorithm from examples is a fundamental problem that has been\nwidely studied. It has been addressed using neural networks too, in particular by\nNeural Turing Machines (NTMs). These are fully differentiable computers that\nuse backpropagation to learn their own programming. Despite their appeal NTMs\nhave a weakness that is caused by their sequential nature: they are not parallel and\nare are hard to train due to their large depth when unfolded.\nWe present a neural network architecture to address this problem: the Neural\nGPU. It is based on a type of convolutional gated recurrent unit and, like the\nNTM, is computationally universal. Unlike the NTM, the Neural GPU is highly\nparallel which makes it easier to train and efficient to run.\nAn essential property of algorithms is their ability to handle inputs of arbitrary\nsize. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a\nnumber of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with up-to 20 bits and\nobserve no errors whatsoever while testing it, even on much longer numbers.\nTo achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout\nand gradient noise to have a large positive effect on learning and generalization.\n\n1\n\nI NTRODUCTION\n\nDeep neural networks have recently proven successful at various tasks, such as computer vision\n(Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012), and in other domains. Recurrent\nneural networks based on long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997)\nhave been successfully applied to a number of natural language processing tasks. Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end\nmanner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing\n(Vinyals & Kaiser et al., 2015), speech recognition (Chan et al., 2016) or image caption generation\n(Vinyals et al., 2014). Since so many tasks can be solved with essentially one model, a natural\nquestion arises: is this model the best we can hope for in supervised learning?\nDespite its recent success, the sequence-to-sequence model has limitations. In its basic form, the\nentire input is encoded into a single fixed-size vector, so the model cannot generalize to inputs much\nlonger than this fixed capacity. One way to resolve this problem is by using an attention mechanism\n(Bahdanau et al., 2014). This allows the network to inspect arbitrary parts of the input in every decoding step, so the basic limitation is removed. But other problems remain, and Joulin & Mikolov\n(2015) show a number of basic algorithmic tasks on which sequence-to-sequence LSTM networks\nfail to generalize. They propose a stack-augmented recurrent network, and it works on some problems, but is limited in other ways.\nIn the best case one would desire a neural network model able to learn arbitrarily complex algorithms\ngiven enough resources. Neural Turing Machines (Graves et al., 2014) have this theoretical property.\nHowever, they are not computationally efficient because they use soft attention and because they tend\nto be of considerable depth. Their depth makes the training objective difficult to optimize and impossible to parallelize because they are learning a sequential program. Their use of soft attention\nrequires accessing the entire memory in order to simulate 1 step of computation, which introduces\nsubstantial overhead. These two factors make learning complex algorithms using Neural Turing Ma1\n\n\fPublished as a conference paper at ICLR 2016\n\nchines difficult. These issues are not limited to Neural Turing Machines, they apply to other architectures too, such as stack-RNNs (Joulin & Mikolov, 2015) or (De)Queue-RNNs (Grefenstette et al.,\n2015). One can try to alleviate these problems using hard attention and reinforcement learning, but\nsuch non-differentiable models do not learn well at present (Zaremba & Sutskever, 2015b).\nIn this work we present a neural network model, the Neural GPU, that addresses the above issues.\nIt is a Turing-complete model capable of learning arbitrary algorithms in principle, like a Neural\nTuring Machine. But, in contrast to Neural Turing Machines, it is designed to be as parallel and as\nshallow as possible. It is more similar to a GPU than to a Turing machine since it uses a smaller number of parallel computational steps. We show that the Neural GPU works in multiple experiments:\n\u2022 A Neural GPU can learn long binary multiplication from examples. It is the first neural\nnetwork able to learn an algorithm whose run-time is superlinear in the size of its input.\nTrained on up-to 20-bit numbers, we see no single error on any inputs we tested, and we\ntested on numbers up-to 2000 bits long.\n\u2022 The same architecture can also learn long binary addition and a number of other algorithmic tasks, such as counting, copying sequences, reversing them, or duplicating them.\n1.1\n\nR ELATED W ORK\n\nThe learning of algorithms with neural networks has seen a lot of interest after the success\nof sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014;\nBahdanau et al., 2014; Cho et al., 2014). An attempt has even been made to learn to evaluate simple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but\nmore success was seen with more complex models. Neural Turing Machines (Graves et al., 2014)\nwere shown to learn a number of basic sequence transformations and memory access patterns, and\ntheir reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable performance on\na number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also\nshown to learn basic sequence transformations such as bigram flipping or sequence reversal.\nThe Grid LSTM (Kalchbrenner et al., 2016) is another powerful architecture that can learn to multiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar\nto the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent and is explicitly\nconstructed from the highly parallel convolution operator.\nIn image processing, convolutional LSTMs, an architecture similar to the Neural GPU, have recently\nbeen used for weather prediction (Shi et al., 2015) and image compression (Toderici et al., 2016).\nWe find it encouraging as it hints that the Neural GPU might perform well in other contexts.\nMost comparable to this work are the prior experiments with the stack-augmented RNNs\n(Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on\na number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to\nadd numbers up-to 20-bit long generalize only to \u223c 100-bit numbers, never to 200-bit ones, and\nnever without error. Still, their generalization is the best we were able to obtain without using the\nNeural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention.\nThe quest for learning algorithms has been pursued much more widely with tools other than neural networks. It is known under names such as program synthesis, program induction, automatic\nprogramming, or inductive synthesis, and has a long history with many works that we do not cover\nhere; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.\nSince one of our results is the synthesis of an algorithm for long binary addition, let us recall how\nthis problem has been addressed without neural networks. Importantly, there are two cases of this\nproblem with different complexity. The easier case is when the two numbers that are to be added\nare aligned at input, i.e., if the first (lower-endian) bit of the first number is presented at the same\ntime as the first bit of the second number, then come the second bits, and so on, as depicted below\nfor x = 9 = 8 + 1 and y = 5 = 4 + 1 written in binary with least-significant bit left.\nInput\n(x and y aligned)\nDesired Output (x + y)\n2\n\n1\n1\n0\n\n0\n0\n1\n\n0\n1\n1\n\n1\n0\n1\n\n\fPublished as a conference paper at ICLR 2016\n\nIn this representation the triples of bits from (x, y, x + y), e.g., (1, 1, 0) (0, 0, 1) (0, 1, 1) (1, 0, 1)\nas in the figure above, form a regular language. To learn binary addition in this representation it\ntherefore suffices to find a regular expression or an automaton that accepts this language, which can\nbe done with a variant of Anguin\u2019s algorithm (Angluin, 1987). But only few interesting functions\nhave regular representations, as for example long multiplication does not (Blumensath & Gra\u0308del,\n2000). It is therefore desirable to learn long binary addition without alignment, for example when x\nand y are provided one after another. This is the representation we use in the present paper.\nInput (x, y)\nDesired Output (x + y)\n\n2\n\n1\n0\n\n0\n1\n\n0\n1\n\n1\n1\n\n+\n\n1\n\n0\n\n1\n\n0\n\nT HE N EURAL GPU\n\nBefore we introduce the Neural GPU, let us recall the architecture of a Gated Recurrent Unit\n(GRU) (Cho et al., 2014). A GRU is similar to an LSTM, but its input and state are the same\nsize, which makes it easier for us to generalize it later; a highway network could have also been\nused (Srivastava et al., 2015), but it lacks the reset gate. GRUs have shown performance similar to\nLSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015). A GRU takes an input vector\nx and a current state vector s, and outputs:\nGRU(x, s) = u \u2299 s + (1 \u2212 u) \u2299 tanh(W x + U (r \u2299 s) + B), where\nu = \u03c3(W \u2032 x + U \u2032 s + B \u2032 )\n\nand r = \u03c3(W \u2032\u2032 x + U \u2032\u2032 s + B \u2032\u2032 ).\n\nIn the equations above, W, W \u2032 , W \u2032\u2032 , U, U \u2032 , U \u2032\u2032 are matrices and B, B \u2032 , B \u2032\u2032 are bias vectors; these\nare the parameters that will be learned. We write W x for a matrix-vector multiplication and r \u2299 s\nfor elementwise vector multiplication. The vectors u and r are called gates since their elements are\nin [0, 1] \u2014 u is the update gate and r is the reset gate.\nIn recurrent neural networks a unit like GRU is applied at every step and the result is both passed as\nnew state and used to compute the output. In a Neural GPU we do not process a new input in every\nstep. Instead, all inputs are written into the starting state s0 . This state has 2-dimensional structure:\nit consists of w \u00d7 h vectors of m numbers, i.e., it is a 3-dimensional tensor of shape [w, h, m]. This\nmental image evolves in time in a way defined by a convolutional gated recurrent unit:\nCGRU(s) = u \u2299 s + (1 \u2212 u) \u2299 tanh(U \u2217 (r \u2299 s) + B), where\nu = \u03c3(U \u2032 \u2217 s + B \u2032 ) and r = \u03c3(U \u2032\u2032 \u2217 s + B \u2032\u2032 ).\nU \u2217 s above denotes the convolution of a kernel bank U with the mental image s. A kernel bank is a\n4-dimensional tensor of shape [kw , kh , m, m], i.e., it contains kw \u00b7 kh \u00b7 m2 parameters, where kw and\nkh are kernel width and height. It is applied to a mental image s of shape [w, h, m] which results in\nanother mental image U \u2217 s of the same shape defined by:\nX\n\n\u230akw /2\u230b\n\nU \u2217 s[x, y, i] =\n\nX\n\n\u230akh /2\u230b\n\nm\nX\n\ns[x + u, y + v, c] \u00b7 U [u, v, c, i].\n\nu=\u230a\u2212kw /2\u230b v=\u230a\u2212kh /2\u230b c=1\n\nIn the equation above the index x + u might sometimes be negative or larger than the size of s, and\nin such cases we assume the value is 0. This corresponds to the standard convolution operator used\nin convolutional neural networks with zero padding on both sides and stride 1. Using the standard\noperator has the advantage that it is heavily optimized (see Section 4 for Neural GPU performance).\nNew work on faster convolutions, e.g., Lavin & Gray (2015), can be directly used in a Neural GPU.\nKnowing how a CGRU gate works, the definition of a l-layer Neural GPU is simple, as depicted in\nFigure 1. The given sequence i = (i1 , . . . , in ) of n discrete symbols from {0, . . . , I} is first embedded into the mental image s0 by concatenating the vectors obtained from an embedding lookup\nof the input symbols into its first column. More precisely, we create the starting mental image s0 of\nshape [w, n, m] by using an embedding matrix E of shape [I, m] and setting s0 [0, k, :] = E[ik ] (in\npython notation) for all k = 1 . . . n (here i1 , . . . , in is the input). All other elements of s0 are set to\n0. Then, we apply l different CGRU gates in turn for n steps to produce the final mental image sfin :\nst+1 = CGRUl (CGRUl\u22121 . . . CGRU1 (st ) . . .) and sfin = sn .\n3\n\n\fPublished as a conference paper at ICLR 2016\n\no1\n\ni1\n\n..\n.\n\nCGRU1\n\n...\n\nCGRU2\n\nCGRU1\n\n..\n.\n\nCGRU2\n\non\n\nin\nsn\u22121\n\ns1\n\ns0\n\nsn\n\nFigure 1: Neural GPU with 2 layers and width w = 3 unfolded in time.\nThe result of a Neural GPU is produced by multiplying each item in the first column of sfin by\nan output matrix O to obtain the logits lk = Osfin [0, k, :] and then selecting the maximal one:\nok = argmax(lk ). During training we use the standard loss function, i.e., we compute a softmax\nover the logits lk and use the negative log probability of the target as the loss.\nSince all components of a Neural GPU are clearly differentiable, we can train using any stochastic\ngradient descent optimizer. For the results presented in this paper we used the Adam optimizer\n(Kingma & Ba, 2014) with \u03b5 = 10\u22124 and gradients norm clipped to 1. The number of layers was\nset to l = 2, the width of mental images was constant at w = 4, the number of maps in each mental\nimage point was m = 24, and the convolution kernels width and height was always kw = kh = 3.\nComputational power of Neural GPUs. While the above definition is simple, it might not be\nimmediately obvious what kind of functions a Neural GPU can compute. Why can we expect it to\nbe able to perform long multiplication? To answer such questions it is useful to draw an analogy\nbetween a Neural GPU and a discrete 2-dimensional cellular automaton. Except for being discrete\nand the lack of a gating mechanism, such automata are quite similar to Neural GPUs. Of course,\nthese are large exceptions. Dense representations have often more capacity than purely discrete\nstates and the gating mechanism is crucial to avoid vanishing gradients during training. But the\ncomputational power of cellular automata is much better understood. In particular, it is well known\nthat a cellular automaton can exploit its parallelism to multiply two n-bit numbers in O(n) steps\nusing Atrubin\u2019s algorithm. We recommend the online book (Vivien, 2003) to get an understanding\nof this algorithm and the computational power of cellular automata.\n\n3\n\nE XPERIMENTS\n\nIn this section, we present experiments showing that a Neural GPU can successfully learn a number\nof algorithmic tasks and generalize well beyond the lengths that it was trained on. We start with the\ntwo tasks we focused on, long binary addition and long binary multiplication. Then, to demonstrate\nthe generality of the model, we show that Neural GPUs perform well on several other tasks as well.\n3.1\n\nA DDITION\n\nAND\n\nM ULTIPLICATION\n\nThe two core tasks on which we study the performance of Neural GPUs are long binary addition\nand long binary multiplication. We chose them because they are fundamental tasks and because\nthere is no known linear-time algorithm for long multiplication. As described in Section 2, we\ninput a sequence of discrete symbols into the network and we read out a sequence of symbols\nagain. For binary addition, we use a set of 4 symbols: {0, 1, +, PAD} and for multiplication we use\n{0, 1, \u00b7, PAD}. The PAD symbol is only used for padding so we depict it as empty space below.\nLong binary addition (badd) is the task of adding two numbers represented lower-endian in\nbinary notation. We always add numbers of the same length, but we allow them to have 0s at start,\nso numbers of differing lengths can be padded to equal size. Given two d-bit numbers the full\nsequence length is n = 2d + 1, as seen in the example below, representing (1 + 4) + (2 + 4 + 8) =\n5 + 14 = 19 = (16 + 2 + 1).\n4\n\n\fPublished as a conference paper at ICLR 2016\n\nTask@Bits\nbadd@20\nbadd@25\nbadd@100\nbadd@200\nbadd@2000\nbmul@20\nbmul@25\nbmul@200\nbmul@2000\n\nNeural GPU\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n\nstackRNN\n100%\n100%\n88%\n0%\n0%\nN/A\nN/A\nN/A\nN/A\n\nLSTM+A\n100%\n73%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n\nTable 1: Neural GPU, stackRNN, and LSTM+A results on addition and multiplication. The table\nshows the fraction of test cases for which every single bit of the model\u2019s output is correct.\nInput\nOutput\n\n1\n1\n\n0\n1\n\n1\n0\n\n0\n0\n\n+\n1\n\n0\n\n1\n\n1\n\n1\n\nLong binary multiplication (bmul) is the task of multiplying two binary numbers, represented\nlower-endian. Again, we always multiply numbers of the same length, but we allow them to have 0s\nat start, so numbers of differing lengths can be padded to equal size. Given two d-bit numbers, the\nfull sequence length is again n = 2d+1, as seen in the example below, representing (2+4)\u00b7(2+8) =\n6 \u00b7 10 = 60 = 32 + 16 + 8 + 4.\nInput\nOutput\n\n0\n0\n\n1\n0\n\n1\n1\n\n0\n1\n\n\u00b7\n1\n\n0\n\n1\n\n0\n\n1\n\nModels. We compare three different models on the above tasks. In addition to the Neural GPU\nwe include a baseline LSTM recurrent neural network with an attention mechanism. We call this\nmodel LSTM+A as it is exactly the same as described in (Vinyals & Kaiser et al., 2015). It is a\n3-layer model with 64 units in each LSTM cell in each layer, which results in about 200k parameters (the Neural GPU uses m = 24 and has about 30k paramters). Both the Neural GPU and\nthe LSTM+A baseline were trained using all the techniques described below, including curriculum\ntraining and gradient noise. Finally, on binary addition, we also include the stack-RNN model from\n(Joulin & Mikolov, 2015). This model was not trained using our training regime, but in exactly the\nway as provided in its source code, only with nmax = 41. To match our training procedure, we ran\nit 729 times (cf. Section 3.3) with different random seeds and we report the best obtained result.\nResults. We measure also the rate of fully correct output sequences and report the results in Table 1. For both tasks, we show first the error at the maximum length seen during training, i.e., for\n20-bit numbers. Note that LSTM+A is not able to learn long binary multiplication at this length, it\ndoes not even fit the training data. Then we report numbers for sizes not seen during training.\nAs you can see, a Neural GPU can learn a multiplication algorithm that generalizes perfectly, at least\nas far as we were able to test (technical limits of our implementation prevented us from testing much\nabove 2000 bits). Even for the simpler task of binary addition, stack-RNNs work only up-to length\n100. This is still much better than the LSTM+A baseline which only generalizes to length 25.\n3.2\n\nOTHER A LGORITHMIC TASKS\n\nIn addition to the two main tasks above, we tested Neural GPUs on the following simpler algorithmic\ntasks. The same architecture as used above was able to solve all of the tasks described below, i.e.,\nafter being trained on sequences of length up-to 41 we were not able to find any error on sequences\non any length we tested (up-to 4001).\nCopying sequences is the simple task of producing on output the same sequence as on input. It is\nvery easy for a Neural GPU, in fact all models converge quickly and generalize perfectly.\nReversing sequences is the task of reversing a sequence of bits, n is the length of the sequence.\n5\n\n\fPublished as a conference paper at ICLR 2016\n\nDuplicating sequences is the task of duplicating the input bit sequence on output twice, as in the\nexample below. We use the padding symbol on input to make it match the output length. We trained\non sequences of inputs up-to 20 bits, so outputs were up-to 40-bits long, and tested on inputs up-to\n2000 bits long.\nInput\nOutput\n\n0\n0\n\n0\n0\n\n1\n1\n\n1\n1\n\n0\n\n0\n\n1\n\n1\n\nCounting by sorting bits is the task of sorting the input bit sequence on output. Since there are\nonly 2 symbols to sort, this is a counting tasks \u2013 the network must count how many 0s are in the\ninput and produce the output accordingly, as in the example below.\nInput\nOutput\n3.3\n\n1\n0\n\n0\n0\n\n1\n0\n\n1\n0\n\n0\n1\n\n0\n1\n\n1\n1\n\n0\n1\n\nT RAINING T ECHNIQUES\n\nHere we describe the training methods that we used to improve our results. Note that we applied\nthese methods to the LSTM+A baseline as well, to keep the above comparison fair. We focus on\nthe most important elements of our training regime, all less relevant details can be found in the code\nwhich is released as open-source.1\nGrid search. Each result we report is obtained by running a grid search over 36 = 729 instances.\nWe consider 3 settings of the learning rate, initial parameters scale, and 4 other hyperparameters\ndiscussed below: the relaxation pull factor, curriculum progress threshold, gradient noise scale, and\ndropout. An important effect of running this grid search is also that we train 729 models with different random seeds every time. Usually only a few of these models generalize to 2000-bit numbers,\nbut a significant fraction works well on 200-bit numbers, as discussed below.\nCurriculum learning. We use a curriculum learning approach inspired by Zaremba & Sutskever\n(2015a). This means that we train, e.g., on 7-digit numbers only after crossing a curriculum progress\nthreshold (e.g., over 90% fully correct outputs) on 6-digit numbers. However, with 20% probability\nwe pick a minibatch of d-digit numbers with d chosen uniformly at random between 1 and 20.\nGradients noise. To improve training speed and stability we add noise to gradients in each training\nstep. Inspired by the schedule from Welling & Teh (2011), we add to gradients a noise drawn from\nthe normal distribution with mean 0 and variance inversely proportional to the square root of stepnumber (i.e., with standard deviation proportional to the 4-th root of step-number). We multiply this\nnoise by the gradient noise scale and, to avoid noise in converged models, we also multiply it by the\nfraction of non-fully-correct outputs (which is 0 for a perfect model).\nGate cutoff. In Section 2 we defined the gates in a CGRU using the sigmoid function, e.g., we\nwrote u = \u03c3(U \u2032 \u2217 s + B \u2032 ). Usually the standard sigmoid function is used, \u03c3(x) = 1+e1\u2212x . We\nfound that adding a hard threshold on the top and bottom helps slightly in our setting, so we use\n1.2\u03c3(x) \u2212 0.1 cut to the interval [0, 1], i.e., \u03c3 \u2032 (x) = max(0, min(1, 1.2\u03c3(x) \u2212 0.1)).\n3.3.1\n\nD ROPOUT\n\nON RECURRENT CONNECTIONS\n\nDropout is a widely applied technique for regularizing neural networks. But when applying it to\nrecurrent networks, it has been counter-productive to apply it on recurrent connections \u2013 it only\nworked when applied to the non-recurrent ones, as reported by Pham et al. (2014).\nSince a Neural GPU does not have non-recurrent connections it might seem that dropout will not\nbe useful for this architecture. Surprisingly, we found the contrary \u2013 it is useful and improves\ngeneralization. The key to using dropout effectively in this setting is to set a small dropout rate.\nWhen we run a grid search for dropout rates we vary them between 6%, 9%, and 13.5%, meaning\nthat over 85% of the values are always preserved. It turns out that even this small dropout has large\n1\n\nThe code is at https://github.com/tensorflow/models/tree/master/neural_gpu.\n\n6\n\n\fPublished as a conference paper at ICLR 2016\n\neffect since we apply it to the whole mental image si in each step i. Presumably the network now\nlearns to include some redundancy in its internal representation and generalization benefits from it.\nWithout dropout we usually see only a few models from a 729 grid search generalize reasonably,\nwhile with dropout it is a much larger fraction and they generalize to higher lengths. In particular,\ndropout was necessary to train models for multiplication that generalize to 2000 bits.\n3.3.2\n\nPARAMETER SHARING\n\nRELAXATION .\n\nTo improve optimization of our deep network we use a relaxation technique for shared parameters\nwhich works as follows. Instead of training with parameters shared across time-steps we use r\nidentical sets of non-shared parameters (we often use r = 6, larger numbers work better but use\nmore memory). At time-step t of the Neural GPU we use the i-th set if t mod r = i.\nThe procedure described above relaxes the network, as it can now perform different operations in\ndifferent time-steps. Training becomes easier, but we now have r parameters instead of the single\nshared set we want. To unify them we add a term to the cost function representing the distance\nof each parameter from the average of this parameter in all the r sets. This term in the final cost\nfunction is multiplied by a scalar which we call the relaxation pull. If the relaxation pull is 0, the\nnetwork behaves as if the r parameter sets were separate, but when it is large, the cost forces the\nnetwork to unify the parameters across different set.\nDuring training, we gradually increase the relaxation pull. We start with a small value and every time\nthe curriculum makes progress, e.g., when the model performs well on 6-digit numbers, we multiply\nthe relaxation pull by a relaxation pull factor. When the curriculum reaches the maximal length we\naverage the parameters from all sets and continue to train with a single shared parameter set.\nThis method is crucial for learning multiplication. Without it, a Neural GPU with m = 24 has\ntrouble to even fit the training set, and the few models that manage to do it do not generalize. With\nrelaxation almost all models in our 729 runs manage to fit the training data.\n\n4\n\nD ISCUSSION\n\nWe prepared a video of the Neural GPU trained to solve the tasks mentioned above.2. It shows\nthe state in each step with values of \u22121 drawn in white, 1 in black, and other in gray. This gives\nan intuition how the Neural GPU solves the discussed problems, e.g., it is quite clear that for the\nduplication task the Neural GPU learned to move a part of the embedding downwards in each step.\nWhat did not work well? For one, using decimal inputs degrades performance. All tasks above can\neasily be formulated with decimal inputs instead of binary ones. One could hope that a Neural GPU\nwill work well in this case too, maybe with a larger m. We experimented with this formulation and\nour results were worse than when the representation was binary: we did not manage to learn long\ndecimal multiplication. Increasing m to 128 allows to learn all other tasks in the decimal setting.\nAnother problem is that often only a few models in a 729 grid search generalize to very long unseen\ninstances. Among those 729 models, there usually are many models that generalize to 40 or even 200\nbits, but only a few working without error for 2000-bit numbers. Using dropout and gradient noise\nimproves the reliability of training and generalization, but maybe another technique could help even\nmore. How could we make more models achieve good generalization? One idea that looks natural\nis to try to reduce the number of parameters by decreasing m. Surprisingly, this does not seem to\nhave any influence. In addition to the m = 24 results presented above we ran experiments with\nm = 32, 64, 128 and the results were similar. In fact using m = 128 we got the most models to\ngeneralize. Additionally, we observed that ensembling a few models, just by averaging their outputs,\nhelps to generalize: ensembles of 5 models almost always generalize perfectly on binary tasks.\nWhy use width? The Neural GPU is defined using two-dimensional convolutions and in our experiments one of the dimensions is always set to 4. Doing so is not necessary since a one-dimensional\nNeural GPU that uses four times larger m can represent every function representable by the original\none. In fact we trained a model for long binary multiplication that generalized to 2000-bit numbers\nusing a Neural GPU with width 1 and m = 64. However, the width of the Neural GPU increases the\n2\n\nThe video is available at https://www.youtube.com/watch?v=LzC8NkTZAF4\n\n7\n\n\fPublished as a conference paper at ICLR 2016\n\namount of information carried in its hidden state without increasing the number of its parameters.\nThus it can be thought of as a factorization and might be useful for other tasks.\nSpeed and data efficiency. Neural GPUs use the standard, heavily optimized convolution operation\nand are fast. We experimented with a 2-layer Neural GPU for n = 32 and m = 64. After unfolding\nin time it has 128 layers of CGRUs, each operating on 32 mental images, each 4 \u00d7 64 \u00d7 64 . The\njoint forward-backward step time for this network was about 0.6s on an NVIDIA GTX 970 GPU.\nWe were also surprised by how data-efficient a Neural GPU can be. The experiments presented\nabove were all performed using 10k random training data examples for each training length. Since\nwe train on up-to 20-bit numbers this adds to about 200k training examples. We tried to train using\nonly 100 examples per length, so about 2000 total training instances. We were surprised to see\nthat it actually worked well for binary addition: there were models that generalized well to 200-bit\nnumbers and to all lengths below despite such small training set. But we never managed to train a\ngood model for binary multiplication with that little training data.\n\n5\n\nC ONCLUSIONS\n\nAND\n\nF UTURE WORK\n\nThe results presented in Table 1 show clearly that there is a qualitative difference between what can\nbe achieved with a Neural GPU and what was possible with previous architectures. In particular, for\nthe first time, we show a neural network that learns a non-trivial superlinear-time algorithm in a way\nthat generalized to much higher lengths without errors.\nThis opens the way to use neural networks in domains that were previously only addressed by\ndiscrete methods, such as program synthesis. With the surprising data efficiency of Neural GPUs it\ncould even be possible to replicate previous program synthesis results, e.g., Kaiser (2012), but in a\nmore scalable way. It is also interesting that a Neural GPU can learn symbolic algorithms without\nusing any discrete state at all, and adding dropout and noise only improves its performance.\nAnother promising future work is to apply Neural GPUs to language processing tasks. Good\nresults have already been obtained on translation with a convolutional architecture over words\n(Kalchbrenner & Blunsom, 2013) and adding gating and recursion, like in a Neural GPU, should\nallow to train much deeper models without overfitting. Finally, the parameter sharing relaxation\ntechnique can be applied to any deep recurrent network and has the potential to improve RNN training in general.\n\nR EFERENCES\nAngluin, Dana. Learning regaular sets from queries and counterexamples. Information and Computation, 75:\n87\u2013106, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to\nalign and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/1409."
            },
            {
                "ref_title_clean": "automatic structures",
                "ref_title_full": "Automatic Structures",
                "year": "none",
                "full_block": "Blumensath, Achim and Gra\u0308del, Erich. Automatic Structures. In Proceedings of LICS 2000, pp. 51\u201362, "
            },
            {
                "ref_title_clean": "listen attend and spell",
                "ref_title_full": "Listen, attend and spell",
                "year": "none",
                "full_block": "URL http://www.logic.rwth-aachen.de/pub/graedel/BlGr-lics00.ps.\nChan, William, Jaitly, Navdeep, Le, Quoc V., and Vinyals, Oriol. Listen, attend and spell. In International\nConference on Acoustics, Speech and Signal Processing, ICASSP\u201916, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio,\nYoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation.\nCoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406."
            },
            {
                "ref_title_clean": "empirical evaluation of gated recurrent neural networks on sequence modeling",
                "ref_title_full": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                "year": "none",
                "full_block": "Chung, Junyoung, Gu\u0308lc\u0327ehre, C\u0327aglar, Cho, Kyunghyun, and Bengio, Yoshua.\nEmpirical evaluation\nof gated recurrent neural networks on sequence modeling.\nCoRR, abs/1412.3555, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "URL\nhttp://arxiv.org/abs/1412."
            },
            {
                "ref_title_clean": "context dependent pre trained deep neural networks for large vocabulary speech recognition",
                "ref_title_full": "Context dependent pre trained deep neural networks for large vocabulary speech recognition",
                "year": "none",
                "full_block": "Dahl, George E., Yu, Dong, Deng, Li, and Acero, Alex. Context-dependent pre-trained deep neural networks\nfor large-vocabulary speech recognition. IEEE Transactions on Audio, Speech & Language Processing, 20\n(1):30\u201342, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines. CoRR, abs/1410.5401, 2014. URL\nhttp://arxiv.org/abs/1410."
            },
            {
                "ref_title_clean": "8 \fpublished as a conference paper at iclr 2016 grefenstette edward hermann karl moritz suleyman mustafa and blunsom learning to transduce with unbounded memory",
                "ref_title_full": "8 \fPublished as a conference paper at ICLR 2016 Grefenstette, Edward, Hermann, Karl Moritz, Suleyman, Mustafa, and Blunsom, Learning to transduce with unbounded memory",
                "year": "none",
                "full_block": "\n8\n\n\fPublished as a conference paper at ICLR 2016\n\nGrefenstette, Edward, Hermann, Karl Moritz, Suleyman, Mustafa, and Blunsom,\nLearning to transduce with unbounded memory.\nCoRR, abs/1506.02516, "
            },
            {
                "ref_title_clean": "httparxiv",
                "ref_title_full": "http://arxiv",
                "year": "none",
                "full_block": "http://arxiv.org/abs/1506.0"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "\nPhil.\nURL\n\nGreff, Klaus, Srivastava, Rupesh Kumar, Koutn\u0131\u0301k, Jan, Steunebrink, Bas R., and Schmidhuber, Ju\u0308rgen. LSTM:\nA search space odyssey. CoRR, abs/1503.04069, 2015. URL http://arxiv.org/abs/1503.0"
            },
            {
                "ref_title_clean": "dimensions in program synthesis",
                "ref_title_full": "Dimensions in program synthesis",
                "year": "none",
                "full_block": "Gulwani, Sumit. Dimensions in program synthesis. In Proceedings of PPDP 2010, PPDP \u201910, pp. 13\u201324, "
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "none",
                "full_block": "Hochreiter, Sepp and Schmidhuber, Ju\u0308rgen. Long short-term memory. Neural computation, 9(8):1735\u20131780,\n"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent nets.\nCoRR, abs/1503.01007, 2015. URL http://arxiv.org/abs/1503.0"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Kaiser, \u0141ukasz. Learning games from videos guided by descriptive complexity. In Proceedings of the AAAI-12,\npp. 963\u2013970. AAAI Press, 2012. URL http://goo.gl/mRbfV5.\nKalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In Proceedings EMNLP 2013,\npp. 1700\u20131709, 2013. URL http://nal.co/papers/KalchbrennerBlunsom_EMNLP13.\nKalchbrenner, Nal, Danihelka, Ivo, and Graves, Alex. Grid long short-term memory. In International Conference on Learning Representations, 2016. URL http://arxiv.org/abs/1507.0"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,\n2014. URL http://arxiv.org/abs/1412."
            },
            {
                "ref_title_clean": "inductive programming a survey of program synthesis techniques",
                "ref_title_full": "Inductive programming: A survey of program synthesis techniques",
                "year": "none",
                "full_block": "Kitzelmann, Emanuel. Inductive programming: A survey of program synthesis techniques. In Approaches and\nApplications of Inductive Programming, AAIP 2009, volume 5812 of LNCS, pp. 50\u201373, "
            },
            {
                "ref_title_clean": "imagenet classification with deep convolutional neural network",
                "ref_title_full": "Imagenet classification with deep convolutional neural network",
                "year": "none",
                "full_block": "Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classification with deep convolutional neural\nnetwork. In Advances in Neural Information Processing Systems, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Lavin, Andrew and Gray, Scott. Fast algorithms for convolutional neural networks. CoRR, abs/1509.09308,\n2015. URL http://arxiv.org/abs/1509.0"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Pham, Vu, Bluche, The\u0301odore, Kermorvant, Christopher, and Louradour, Je\u0301ro\u0302me. Dropout improves recurrent neural networks for handwriting recognition. In International Conference on Frontiers in Handwriting\nRecognition (ICFHR), pp. 285\u2013290. IEEE, 2014. URL http://arxiv.org/pdf/1312.4569.pdf.\nShi, Xingjian, Chen, Zhourong, Wang, Hao, Yeung, Dit-Yan, kin Wong, Wai, and chun Woo, Wang. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in Neural\nInformation Processing Systems, 2015. URL http://arxiv.org/abs/1506.0"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, Ju\u0308rgen.\nHighway networks.\nabs/1505.00387, 2015. URL http://arxiv.org/abs/1505.0"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "none",
                "full_block": "\nCoRR,\n\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Systems, pp. 3104\u20133112, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "URL\nhttp://arxiv.org/abs/1409."
            },
            {
                "ref_title_clean": "variable rate image compression with recurrent neural networks",
                "ref_title_full": "Variable rate image compression with recurrent neural networks",
                "year": "none",
                "full_block": "Toderici, George, O\u2019Malley, Sean M., Hwang, Sung Jin, Vincent, Damien, Minnen, David, Baluja,\nShumeet, Covell, Michele, and Sukthankar, Rahul. Variable rate image compression with recurrent neural networks.\nIn International Conference on Learning Representations, "
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "URL\nhttp://arxiv.org/abs/1511.0"
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural\nInformation Processing Systems, 2015. URL http://arxiv.org/abs/1412."
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption\ngenerator. CoRR, abs/1411.4555, 2014. URL http://arxiv.org/abs/1411."
            },
            {
                "ref_title_clean": "an introduction to cellular automata",
                "ref_title_full": "An Introduction to cellular automata",
                "year": "none",
                "full_block": "Vivien,\nHelene.\nAn Introduction to cellular automata.\n"
            },
            {
                "ref_title_clean": "bayesian learning via stochastic gradient langevin dynamics",
                "ref_title_full": "Bayesian learning via stochastic gradient Langevin dynamics",
                "year": "none",
                "full_block": "URL\nhttp://www.liafa.univ-paris-diderot.fr/\u02dcyunes/ca/archives/bookvivien.pdf.\nWelling, Max and Teh, Yee Whye. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of ICML 2011, pp. 681\u2013688, "
            },
            {
                "ref_title_clean": "httparxiv",
                "ref_title_full": "http://arxiv",
                "year": "none",
                "full_block": "Zaremba, Wojciech and Sutskever, Ilya. Learning to execute.\nhttp://arxiv.org/abs/1410."
            },
            {
                "ref_title_clean": "url httparxiv",
                "ref_title_full": "URL http://arxiv",
                "year": "none",
                "full_block": "\nCoRR, abs/1410.4615, 2015a.\n\nZaremba, Wojciech and Sutskever, Ilya.\nReinforcement learning neural turing machines.\nabs/1505.00521, 2015b. URL http://arxiv.org/abs/1505.0"
            }
        ],
        "refs_source": "from_pdf"
    },
    "neural random access machines": {
        "id": "1511.06392",
        "depth": 1,
        "children_titles": [
            "neural machine translation by jointly learning to align and translate",
            "learning long term dependencies with gradient descent is difficult",
            "curriculum learning",
            "listen attend and spell",
            "neural turing machines",
            "learning to transduce with unbounded memory",
            "long short term memory",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "grid long short term memory",
            "adam a method for stochastic optimization",
            "effective approaches to attention based neural machine translation",
            "rectified linear units improve restricted boltzmann machines",
            "adding gradient noise improves learning for very deep networks",
            "a formal theory of inductive inference part i",
            "end to end memory networks",
            "grammar as a foreign language",
            "pointer networks",
            "memory networks",
            "learning to execute",
            "reinforcement learning neural turing machines"
        ],
        "status": "expanded",
        "title_full": "Neural random access machines",
        "link": "https://arxiv.org/abs/1511.06392",
        "n_parents": 6,
        "year": "2016",
        "children_full_dicts": [
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "2014",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "learning long term dependencies with gradient descent is difficult",
                "ref_title_full": "Learning long term dependencies with gradient descent is difficult",
                "year": "1994",
                "full_block": "Learning long-term dependencies with gradient descent is difficult.\n"
            },
            {
                "ref_title_clean": "curriculum learning",
                "ref_title_full": "Curriculum learning",
                "year": "2009",
                "full_block": "Curriculum learning.\n"
            },
            {
                "ref_title_clean": "listen attend and spell",
                "ref_title_full": "Listen, attend and spell",
                "year": "2015",
                "full_block": "Listen, attend and spell.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural turing machines",
                "year": "2014",
                "full_block": "Neural turing machines.\n"
            },
            {
                "ref_title_clean": "learning to transduce with unbounded memory",
                "ref_title_full": "Learning to transduce with unbounded memory",
                "year": "2015",
                "full_block": "Learning to transduce with unbounded memory.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "grid long short term memory",
                "ref_title_full": "Grid long short term memory",
                "year": "2015",
                "full_block": "Grid long short-term memory.\n"
            },
            {
                "ref_title_clean": "adam a method for stochastic optimization",
                "ref_title_full": "Adam: A method for stochastic optimization",
                "year": "2014",
                "full_block": "Adam: A method for stochastic optimization.\n"
            },
            {
                "ref_title_clean": "effective approaches to attention based neural machine translation",
                "ref_title_full": "Effective approaches to attention based neural machine translation",
                "year": "2015",
                "full_block": "Effective approaches to attention-based neural machine translation.\n"
            },
            {
                "ref_title_clean": "rectified linear units improve restricted boltzmann machines",
                "ref_title_full": "Rectified linear units improve restricted boltzmann machines",
                "year": "2010",
                "full_block": "Rectified linear units improve restricted boltzmann machines.\n"
            },
            {
                "ref_title_clean": "adding gradient noise improves learning for very deep networks",
                "ref_title_full": "Adding gradient noise improves learning for very deep networks",
                "year": "2015",
                "full_block": "Adding gradient noise improves learning for very deep networks.\n"
            },
            {
                "ref_title_clean": "a formal theory of inductive inference part i",
                "ref_title_full": "A formal theory of inductive inference. part i",
                "year": "1964",
                "full_block": "A formal theory of inductive inference. part i.\n"
            },
            {
                "ref_title_clean": "end to end memory networks",
                "ref_title_full": "End to end memory networks",
                "year": "2015",
                "full_block": "End-to-end memory networks.\n"
            },
            {
                "ref_title_clean": "grammar as a foreign language",
                "ref_title_full": "Grammar as a foreign language",
                "year": "2014",
                "full_block": "Grammar as a foreign language.\n"
            },
            {
                "ref_title_clean": "pointer networks",
                "ref_title_full": "Pointer networks",
                "year": "2015",
                "full_block": "Pointer networks.\n"
            },
            {
                "ref_title_clean": "memory networks",
                "ref_title_full": "Memory networks",
                "year": "2014",
                "full_block": "Memory networks.\n"
            },
            {
                "ref_title_clean": "learning to execute",
                "ref_title_full": "Learning to execute",
                "year": "2014",
                "full_block": "Learning to execute.\n"
            },
            {
                "ref_title_clean": "reinforcement learning neural turing machines",
                "ref_title_full": "Reinforcement learning neural turing machines",
                "year": "2015",
                "full_block": "Reinforcement learning neural turing machines.\n"
            }
        ],
        "refs_source": "unpacked_sources/1511.06392/paper.bbl"
    },
    "effective approaches to attention based neural machine translation": {
        "id": "1508.04025",
        "depth": 1,
        "children_titles": [
            "neural machine translation by jointly learning to align and translate",
            "n gram counts and language models from the common crawl",
            "learning phrase representations using rnn encoder decoder for statistical machine translation",
            "measuring word alignment quality for statistical machine translation",
            "draw a recurrent neural network for image generation",
            "on using very large target vocabulary for neural machine translation",
            "recurrent continuous translation models",
            "statistical phrase based translation",
            "alignment by agreement",
            "addressing the rare word problem in neural machine translation",
            "recurrent models of visual attention",
            "bleu a method for automatic evaluation of machine translation",
            "sequence to sequence learning with neural networks",
            "show attend and tell neural image caption generation with visual attention",
            "recurrent neural network regularization"
        ],
        "status": "expanded",
        "title_full": "Effective approaches to attention based neural machine translation",
        "link": "https://arxiv.org/abs/1508.04025",
        "n_parents": 2,
        "year": "2015",
        "children_full_dicts": [
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "none",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "n gram counts and language models from the common crawl",
                "ref_title_full": "N gram counts and language models from the common crawl",
                "year": "none",
                "full_block": "N-gram counts and language models from the common crawl.\n"
            },
            {
                "ref_title_clean": "learning phrase representations using rnn encoder decoder for statistical machine translation",
                "ref_title_full": "Learning phrase representations using RNN encoder decoder for statistical machine translation",
                "year": "none",
                "full_block": "Learning phrase representations using {RNN} encoder-decoder for\n  statistical machine translation.\n"
            },
            {
                "ref_title_clean": "measuring word alignment quality for statistical machine translation",
                "ref_title_full": "Measuring word alignment quality for statistical machine translation",
                "year": "none",
                "full_block": "Measuring word alignment quality for statistical machine translation.\n"
            },
            {
                "ref_title_clean": "draw a recurrent neural network for image generation",
                "ref_title_full": "DRAW: A recurrent neural network for image generation",
                "year": "none",
                "full_block": "{DRAW:} {A} recurrent neural network for image generation.\n"
            },
            {
                "ref_title_clean": "on using very large target vocabulary for neural machine translation",
                "ref_title_full": "On using very large target vocabulary for neural machine translation",
                "year": "none",
                "full_block": "On using very large target vocabulary for neural machine translation.\n"
            },
            {
                "ref_title_clean": "recurrent continuous translation models",
                "ref_title_full": "Recurrent continuous translation models",
                "year": "none",
                "full_block": "Recurrent continuous translation models.\n"
            },
            {
                "ref_title_clean": "statistical phrase based translation",
                "ref_title_full": "Statistical phrase based translation",
                "year": "none",
                "full_block": "Statistical phrase-based translation.\n"
            },
            {
                "ref_title_clean": "alignment by agreement",
                "ref_title_full": "Alignment by agreement",
                "year": "none",
                "full_block": "Alignment by agreement.\n"
            },
            {
                "ref_title_clean": "addressing the rare word problem in neural machine translation",
                "ref_title_full": "Addressing the rare word problem in neural machine translation",
                "year": "none",
                "full_block": "Addressing the rare word problem in neural machine translation.\n"
            },
            {
                "ref_title_clean": "recurrent models of visual attention",
                "ref_title_full": "Recurrent models of visual attention",
                "year": "none",
                "full_block": "Recurrent models of visual attention.\n"
            },
            {
                "ref_title_clean": "bleu a method for automatic evaluation of machine translation",
                "ref_title_full": "Bleu: a method for automatic evaluation of machine translation",
                "year": "none",
                "full_block": "Bleu: a method for automatic evaluation of machine translation.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "none",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "show attend and tell neural image caption generation with visual attention",
                "ref_title_full": "Show, attend and tell: Neural image caption generation with visual attention",
                "year": "none",
                "full_block": "Show, attend and tell: Neural image caption generation with visual\n  attention.\n"
            },
            {
                "ref_title_clean": "recurrent neural network regularization",
                "ref_title_full": "Recurrent neural network regularization",
                "year": "none",
                "full_block": "Recurrent neural network regularization.\n"
            }
        ],
        "refs_source": "unpacked_sources/1508.04025/emnlp15.bbl"
    },
    "knowledge and reasoning in program synthesis": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Knowledge and reasoning in program synthesis",
        "link": "none",
        "n_parents": 1,
        "year": "1975"
    },
    "a deductive approach to program synthesis": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "A deductive approach to program synthesis",
        "link": "none",
        "n_parents": 1,
        "year": "1980"
    },
    "computation of normalized edit distance and applications": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Computation of normalized edit distance and applications",
        "link": "none",
        "n_parents": 1,
        "year": "1993"
    },
    "a machine learning framework for programming by example": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "A machine learning framework for programming by example",
        "link": "none",
        "n_parents": 3,
        "year": "2013"
    },
    "neural programmer inducing latent programs with gradient descent": {
        "id": "1511.04834",
        "depth": 1,
        "children_titles": [
            "learning to compose neural networks for question answering",
            "neural machine translation by jointly learning to align and translate",
            "end to end attention based large vocabulary speech recognition",
            "question answering with subgraph embeddings",
            "functional imaging of numerical processing in adults and 4 y old children",
            "listen attend and spell",
            "driving semantic parsing from the worlds response",
            "learning context free grammars capabilities and limitations of a recurrent neural network with an external stack memory",
            "using prior knowledge in an nnpda to learn context free languages",
            "numerical processing in the human parietal cortex during experimental and natural conditions",
            "reading to learn constructing features from semantic abstracts",
            "processing of abstract ordinal knowledge in the horizontal segment of the intraparietal sulcus",
            "lstm recurrent networks learn simple context free and context sensitive languages",
            "generating sequences with recurrent neural networks",
            "towards end to end speech recognition with recurrent neural networks",
            "neural turing machines",
            "deep speech scaling up end to end speech recognition",
            "teaching machines to read and comprehend",
            "deep neural networks for acoustic modeling in speech recognition",
            "long short term memory",
            "robust estimation of a location parameter",
            "a neural network for factoid question answering over paragraphs",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "adam a method for stochastic optimization",
            "imagenet classification with deep convolutional neural networks",
            "impaired neural networks for approximate calculation in dyscalculic children a functional mri study",
            "ask me anything dynamic memory networks for natural language processing",
            "learning programs a hierarchical bayesian approach",
            "learning dependency based compositional semantics",
            "modeling relation paths for representation learning of knowledge bases",
            "addressing the rare word problem in neural machine translation",
            "compositional vector space models for knowledge base completion",
            "adding gradient noise improves learning for very deep networks",
            "compositional semantic parsing on semi structured tables",
            "towards neural network based reasoning",
            "a bayesian model of the acquisition of compositional semantics",
            "tuning curves for approximate numerosity in the human intraparietal sulcus",
            "grounded unsupervised semantic parsing",
            "neural programmer interpreters",
            "a self referentialweight matrix",
            "neural responding machine for short text conversation",
            "a recurrent network that performs a context sensitive prediction task",
            "end to end memory networks",
            "sequence to sequence learning with neural networks",
            "a neural conversational model",
            "show and tell a neural image caption generator",
            "first draft of a report on the edvac",
            "building a semantic parser overnight",
            "bayesian learning via stochastic gradient langevin dynamics",
            "backpropagation through time what does it do and how to do it",
            "memory networks",
            "show attend and tell neural image caption generation with visual attention",
            "neural enquirer learning to query tables with natural language",
            "learning to parse database queries using inductive logic programming",
            "discrete recurrent neural networks for grammatical inference",
            "learning to map sentences to logical form structured classification with probabilistic categorial grammars"
        ],
        "status": "expanded",
        "title_full": "Neural programmer: Inducing latent programs with gradient descent",
        "link": "https://arxiv.org/abs/1511.04834",
        "n_parents": 6,
        "year": "2016",
        "children_full_dicts": [
            {
                "ref_title_clean": "learning to compose neural networks for question answering",
                "ref_title_full": "Learning to compose neural networks for question answering",
                "year": "2016",
                "full_block": "Learning to compose neural networks for question answering.\n"
            },
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "2014",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "end to end attention based large vocabulary speech recognition",
                "ref_title_full": "End to end attention based large vocabulary speech recognition",
                "year": "2015",
                "full_block": "End-to-end attention-based large vocabulary speech recognition.\n"
            },
            {
                "ref_title_clean": "question answering with subgraph embeddings",
                "ref_title_full": "Question answering with subgraph embeddings",
                "year": "2014",
                "full_block": "Question answering with subgraph embeddings.\n"
            },
            {
                "ref_title_clean": "functional imaging of numerical processing in adults and 4 y old children",
                "ref_title_full": "Functional imaging of numerical processing in adults and 4 y old children",
                "year": "2006",
                "full_block": "Functional imaging of numerical processing in adults and 4-y-old\n  children.\n"
            },
            {
                "ref_title_clean": "listen attend and spell",
                "ref_title_full": "Listen, attend and spell",
                "year": "2015",
                "full_block": "Listen, attend and spell.\n"
            },
            {
                "ref_title_clean": "driving semantic parsing from the worlds response",
                "ref_title_full": "Driving semantic parsing from the world's response",
                "year": "2010",
                "full_block": "Driving semantic parsing from the world's response.\n"
            },
            {
                "ref_title_clean": "learning context free grammars capabilities and limitations of a recurrent neural network with an external stack memory",
                "ref_title_full": "Learning context free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory",
                "year": "none",
                "full_block": "Learning context-free grammars: Capabilities and limitations of a\n  recurrent neural network with an external stack memory.\n"
            },
            {
                "ref_title_clean": "using prior knowledge in an nnpda to learn context free languages",
                "ref_title_full": "Using prior knowledge in an NNPDA to learn context free languages",
                "year": "none",
                "full_block": "Using prior knowledge in an {NNPDA} to learn context-free languages.\n"
            },
            {
                "ref_title_clean": "numerical processing in the human parietal cortex during experimental and natural conditions",
                "ref_title_full": "Numerical processing in the human parietal cortex during experimental and natural conditions",
                "year": "2013",
                "full_block": "Numerical processing in the human parietal cortex during experimental\n  and natural conditions.\n"
            },
            {
                "ref_title_clean": "reading to learn constructing features from semantic abstracts",
                "ref_title_full": "Reading to learn: Constructing features from semantic abstracts",
                "year": "2009",
                "full_block": "Reading to learn: Constructing features from semantic abstracts.\n"
            },
            {
                "ref_title_clean": "processing of abstract ordinal knowledge in the horizontal segment of the intraparietal sulcus",
                "ref_title_full": "Processing of abstract ordinal knowledge in the horizontal segment of the intraparietal sulcus",
                "year": "2007",
                "full_block": "Processing of abstract ordinal knowledge in the horizontal segment of\n  the intraparietal sulcus.\n"
            },
            {
                "ref_title_clean": "lstm recurrent networks learn simple context free and context sensitive languages",
                "ref_title_full": "LSTM recurrent networks learn simple context free and context sensitive languages",
                "year": "2001",
                "full_block": "{LSTM} recurrent networks learn simple context free and context\n  sensitive languages.\n"
            },
            {
                "ref_title_clean": "generating sequences with recurrent neural networks",
                "ref_title_full": "Generating sequences with recurrent neural networks",
                "year": "2013",
                "full_block": "Generating sequences with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "towards end to end speech recognition with recurrent neural networks",
                "ref_title_full": "Towards end to end speech recognition with recurrent neural networks",
                "year": "2014",
                "full_block": "Towards end-to-end speech recognition with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural Turing Machines",
                "year": "2014",
                "full_block": "Neural {T}uring {M}achines.\n"
            },
            {
                "ref_title_clean": "deep speech scaling up end to end speech recognition",
                "ref_title_full": "Deep Speech: Scaling up end to end speech recognition",
                "year": "2014",
                "full_block": "Deep {S}peech: Scaling up end-to-end speech recognition.\n"
            },
            {
                "ref_title_clean": "teaching machines to read and comprehend",
                "ref_title_full": "Teaching machines to read and comprehend",
                "year": "2015",
                "full_block": "Teaching machines to read and comprehend.\n"
            },
            {
                "ref_title_clean": "deep neural networks for acoustic modeling in speech recognition",
                "ref_title_full": "Deep neural networks for acoustic modeling in speech recognition",
                "year": "2012",
                "full_block": "Deep neural networks for acoustic modeling in speech recognition.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "robust estimation of a location parameter",
                "ref_title_full": "Robust estimation of a location parameter",
                "year": "1964",
                "full_block": "Robust estimation of a location parameter.\n"
            },
            {
                "ref_title_clean": "a neural network for factoid question answering over paragraphs",
                "ref_title_full": "A neural network for factoid question answering over paragraphs",
                "year": "2014",
                "full_block": "A neural network for factoid question answering over paragraphs.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "adam a method for stochastic optimization",
                "ref_title_full": "Adam: A method for stochastic optimization",
                "year": "2014",
                "full_block": "Adam: A method for stochastic optimization.\n"
            },
            {
                "ref_title_clean": "imagenet classification with deep convolutional neural networks",
                "ref_title_full": "Imagenet classification with deep convolutional neural networks",
                "year": "2012",
                "full_block": "Imagenet classification with deep convolutional neural networks.\n"
            },
            {
                "ref_title_clean": "impaired neural networks for approximate calculation in dyscalculic children a functional mri study",
                "ref_title_full": "Impaired neural networks for approximate calculation in dyscalculic children: a functional mri study",
                "year": "2006",
                "full_block": "Impaired neural networks for approximate calculation in dyscalculic\n  children: a functional mri study.\n"
            },
            {
                "ref_title_clean": "ask me anything dynamic memory networks for natural language processing",
                "ref_title_full": "Ask me anything: Dynamic memory networks for natural language processing",
                "year": "2015",
                "full_block": "Ask me anything: Dynamic memory networks for natural language\n  processing.\n"
            },
            {
                "ref_title_clean": "learning programs a hierarchical bayesian approach",
                "ref_title_full": "Learning programs: A hierarchical Bayesian approach",
                "year": "2010",
                "full_block": "Learning programs: A hierarchical {B}ayesian approach.\n"
            },
            {
                "ref_title_clean": "learning dependency based compositional semantics",
                "ref_title_full": "Learning dependency based compositional semantics",
                "year": "2011",
                "full_block": "Learning dependency-based compositional semantics.\n"
            },
            {
                "ref_title_clean": "modeling relation paths for representation learning of knowledge bases",
                "ref_title_full": "Modeling relation paths for representation learning of knowledge bases",
                "year": "2015",
                "full_block": "Modeling relation paths for representation learning of knowledge\n  bases.\n"
            },
            {
                "ref_title_clean": "addressing the rare word problem in neural machine translation",
                "ref_title_full": "Addressing the rare word problem in neural machine translation",
                "year": "2014",
                "full_block": "Addressing the rare word problem in neural machine translation.\n"
            },
            {
                "ref_title_clean": "compositional vector space models for knowledge base completion",
                "ref_title_full": "Compositional vector space models for knowledge base completion",
                "year": "2015",
                "full_block": "Compositional vector space models for knowledge base completion.\n"
            },
            {
                "ref_title_clean": "adding gradient noise improves learning for very deep networks",
                "ref_title_full": "Adding gradient noise improves learning for very deep networks",
                "year": "2016",
                "full_block": "Adding gradient noise improves learning for very deep networks.\n"
            },
            {
                "ref_title_clean": "compositional semantic parsing on semi structured tables",
                "ref_title_full": "Compositional semantic parsing on semi structured tables",
                "year": "2015",
                "full_block": "Compositional semantic parsing on semi-structured tables.\n"
            },
            {
                "ref_title_clean": "towards neural network based reasoning",
                "ref_title_full": "Towards neural network based reasoning",
                "year": "2015",
                "full_block": "Towards neural network-based reasoning.\n"
            },
            {
                "ref_title_clean": "a bayesian model of the acquisition of compositional semantics",
                "ref_title_full": "A Bayesian model of the acquisition of compositional semantics",
                "year": "2008",
                "full_block": "A {B}ayesian model of the acquisition of compositional semantics.\n"
            },
            {
                "ref_title_clean": "tuning curves for approximate numerosity in the human intraparietal sulcus",
                "ref_title_full": "Tuning curves for approximate numerosity in the human intraparietal sulcus",
                "year": "2004",
                "full_block": "Tuning curves for approximate numerosity in the human intraparietal\n  sulcus.\n"
            },
            {
                "ref_title_clean": "grounded unsupervised semantic parsing",
                "ref_title_full": "Grounded unsupervised semantic parsing",
                "year": "2013",
                "full_block": "Grounded unsupervised semantic parsing.\n"
            },
            {
                "ref_title_clean": "neural programmer interpreters",
                "ref_title_full": "Neural programmer interpreters",
                "year": "2016",
                "full_block": "Neural programmer-interpreters.\n"
            },
            {
                "ref_title_clean": "a self referentialweight matrix",
                "ref_title_full": "A self referentialweight matrix",
                "year": "1993",
                "full_block": "A self-referentialweight matrix.\n"
            },
            {
                "ref_title_clean": "neural responding machine for short text conversation",
                "ref_title_full": "Neural responding machine for short text conversation",
                "year": "2015",
                "full_block": "Neural responding machine for short-text conversation.\n"
            },
            {
                "ref_title_clean": "a recurrent network that performs a context sensitive prediction task",
                "ref_title_full": "A recurrent network that performs a context sensitive prediction task",
                "year": "1996",
                "full_block": "A recurrent network that performs a context-sensitive prediction\n  task.\n"
            },
            {
                "ref_title_clean": "end to end memory networks",
                "ref_title_full": "End to end memory networks",
                "year": "2015",
                "full_block": "End-to-end memory networks.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "2014",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "a neural conversational model",
                "ref_title_full": "A neural conversational model",
                "year": "2015",
                "full_block": "A neural conversational model.\n"
            },
            {
                "ref_title_clean": "show and tell a neural image caption generator",
                "ref_title_full": "Show and tell: A neural image caption generator",
                "year": "2015",
                "full_block": "Show and tell: A neural image caption generator.\n"
            },
            {
                "ref_title_clean": "first draft of a report on the edvac",
                "ref_title_full": "First draft of a report on the EDVAC",
                "year": "1945",
                "full_block": "First draft of a report on the {EDVAC}.\n"
            },
            {
                "ref_title_clean": "building a semantic parser overnight",
                "ref_title_full": "Building a semantic parser overnight",
                "year": "2015",
                "full_block": "Building a semantic parser overnight.\n"
            },
            {
                "ref_title_clean": "bayesian learning via stochastic gradient langevin dynamics",
                "ref_title_full": "Bayesian learning via stochastic gradient Langevin dynamics",
                "year": "2011",
                "full_block": "Bayesian learning via stochastic gradient {L}angevin dynamics.\n"
            },
            {
                "ref_title_clean": "backpropagation through time what does it do and how to do it",
                "ref_title_full": "Backpropagation through time: what does it do and how to do it",
                "year": "1990",
                "full_block": "Backpropagation through time: what does it do and how to do it.\n"
            },
            {
                "ref_title_clean": "memory networks",
                "ref_title_full": "Memory Networks",
                "year": "none",
                "full_block": "Memory {N}etworks.\n"
            },
            {
                "ref_title_clean": "show attend and tell neural image caption generation with visual attention",
                "ref_title_full": "Show, attend and tell: Neural image caption generation with visual attention",
                "year": "2015",
                "full_block": "Show, attend and tell: Neural image caption generation with visual\n  attention.\n"
            },
            {
                "ref_title_clean": "neural enquirer learning to query tables with natural language",
                "ref_title_full": "Neural enquirer: Learning to query tables with natural language",
                "year": "2015",
                "full_block": "Neural enquirer: Learning to query tables with natural language.\n"
            },
            {
                "ref_title_clean": "learning to parse database queries using inductive logic programming",
                "ref_title_full": "Learning to parse database queries using inductive logic programming",
                "year": "1996",
                "full_block": "Learning to parse database queries using inductive logic programming.\n"
            },
            {
                "ref_title_clean": "discrete recurrent neural networks for grammatical inference",
                "ref_title_full": "Discrete recurrent neural networks for grammatical inference",
                "year": "1994",
                "full_block": "Discrete recurrent neural networks for grammatical inference.\n"
            },
            {
                "ref_title_clean": "learning to map sentences to logical form structured classification with probabilistic categorial grammars",
                "ref_title_full": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
                "year": "2005",
                "full_block": "Learning to map sentences to logical form: Structured classification\n  with probabilistic categorial grammars.\n"
            }
        ],
        "refs_source": "unpacked_sources/1511.04834/neural_programmer.bbl"
    },
    "neuro symbolic program synthesis": {
        "id": "1611.01855",
        "depth": 1,
        "children_titles": [
            "syntax guided synthesis",
            "phog probabilistic model for code",
            "the inference of regular lisp programs from examples",
            "adaptive neural compilation",
            "terpret a probabilistic programming language for program induction",
            "neural turing machines",
            "automating string processing in spreadsheets using input output examples",
            "synthesis of loop free programs",
            "spreadsheet data manipulation using examples",
            "on the naturalness of software",
            "bidirectional recursive neural networks for token level labeling with structure",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "neural random access machines",
            "the inside outside recursive neural network model for dependency parsing",
            "learning programs a hierarchical bayesian approach",
            "structured generative models of natural source code",
            "a machine learning framework for programming by example",
            "neural programmer inducing latent programs with gradient descent",
            "global belief recursive neural networks",
            "predicting program properties from big code",
            "neural programmer interpreters",
            "programming with a differentiable forth interpreter",
            "stochastic superoptimization",
            "synthesizing data structure manipulations from storyboards",
            "automated feedback generation for introductory programming assignments",
            "program synthesis by sketching",
            "programming by sketching for bit streaming programs",
            "a methodology for lisp program construction from examples",
            "transit specifying protocols with concolic snippets",
            "grammar as a foreign language"
        ],
        "status": "expanded",
        "title_full": "Neuro symbolic program synthesis",
        "link": "https://arxiv.org/abs/1611.01855",
        "n_parents": 1,
        "year": "2017",
        "children_full_dicts": [
            {
                "ref_title_clean": "syntax guided synthesis",
                "ref_title_full": "Syntax guided synthesis",
                "year": "2015",
                "full_block": "Syntax-guided synthesis.\n"
            },
            {
                "ref_title_clean": "phog probabilistic model for code",
                "ref_title_full": "PHOG: probabilistic model for code",
                "year": "2016",
                "full_block": "{PHOG:} probabilistic model for code.\n"
            },
            {
                "ref_title_clean": "the inference of regular lisp programs from examples",
                "ref_title_full": "The inference of regular lisp programs from examples",
                "year": "1978",
                "full_block": "The inference of regular lisp programs from examples.\n"
            },
            {
                "ref_title_clean": "adaptive neural compilation",
                "ref_title_full": "Adaptive neural compilation",
                "year": "2016",
                "full_block": "Adaptive neural compilation.\n"
            },
            {
                "ref_title_clean": "terpret a probabilistic programming language for program induction",
                "ref_title_full": "Terpret: A probabilistic programming language for program induction",
                "year": "2016",
                "full_block": "Terpret: A probabilistic programming language for program induction.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural turing machines",
                "year": "2014",
                "full_block": "Neural turing machines.\n"
            },
            {
                "ref_title_clean": "automating string processing in spreadsheets using input output examples",
                "ref_title_full": "Automating string processing in spreadsheets using input output examples",
                "year": "2011",
                "full_block": "Automating string processing in spreadsheets using input-output\n  examples.\n"
            },
            {
                "ref_title_clean": "synthesis of loop free programs",
                "ref_title_full": "Synthesis of loop free programs",
                "year": "2011",
                "full_block": "Synthesis of loop-free programs.\n"
            },
            {
                "ref_title_clean": "spreadsheet data manipulation using examples",
                "ref_title_full": "Spreadsheet data manipulation using examples",
                "year": "2012",
                "full_block": "Spreadsheet data manipulation using examples.\n"
            },
            {
                "ref_title_clean": "on the naturalness of software",
                "ref_title_full": "On the naturalness of software",
                "year": "2016",
                "full_block": "On the naturalness of software.\n"
            },
            {
                "ref_title_clean": "bidirectional recursive neural networks for token level labeling with structure",
                "ref_title_full": "Bidirectional recursive neural networks for token level labeling with structure",
                "year": "2013",
                "full_block": "Bidirectional recursive neural networks for token-level labeling with\n  structure.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "neural random access machines",
                "ref_title_full": "Neural random access machines",
                "year": "2015",
                "full_block": "Neural random-access machines.\n"
            },
            {
                "ref_title_clean": "the inside outside recursive neural network model for dependency parsing",
                "ref_title_full": "The inside outside recursive neural network model for dependency parsing",
                "year": "2014",
                "full_block": "The inside-outside recursive neural network model for dependency\n  parsing.\n"
            },
            {
                "ref_title_clean": "learning programs a hierarchical bayesian approach",
                "ref_title_full": "Learning programs: A hierarchical Bayesian approach",
                "year": "2010",
                "full_block": "Learning programs: A hierarchical {Bayesian} approach.\n"
            },
            {
                "ref_title_clean": "structured generative models of natural source code",
                "ref_title_full": "Structured generative models of natural source code",
                "year": "2014",
                "full_block": "Structured generative models of natural source code.\n"
            },
            {
                "ref_title_clean": "a machine learning framework for programming by example",
                "ref_title_full": "A machine learning framework for programming by example",
                "year": "2013",
                "full_block": "A machine learning framework for programming by example.\n"
            },
            {
                "ref_title_clean": "neural programmer inducing latent programs with gradient descent",
                "ref_title_full": "Neural programmer: Inducing latent programs with gradient descent",
                "year": "2015",
                "full_block": "Neural programmer: Inducing latent programs with gradient descent.\n"
            },
            {
                "ref_title_clean": "global belief recursive neural networks",
                "ref_title_full": "Global belief recursive neural networks",
                "year": "2014",
                "full_block": "Global belief recursive neural networks.\n"
            },
            {
                "ref_title_clean": "predicting program properties from big code",
                "ref_title_full": "Predicting program properties from \"big code\"",
                "year": "2015",
                "full_block": "Predicting program properties from \"big code\".\n"
            },
            {
                "ref_title_clean": "neural programmer interpreters",
                "ref_title_full": "Neural programmer interpreters",
                "year": "2015",
                "full_block": "Neural programmer-interpreters.\n"
            },
            {
                "ref_title_clean": "programming with a differentiable forth interpreter",
                "ref_title_full": "Programming with a differentiable forth interpreter",
                "year": "2016",
                "full_block": "Programming with a differentiable forth interpreter.\n"
            },
            {
                "ref_title_clean": "stochastic superoptimization",
                "ref_title_full": "Stochastic superoptimization",
                "year": "2013",
                "full_block": "Stochastic superoptimization.\n"
            },
            {
                "ref_title_clean": "synthesizing data structure manipulations from storyboards",
                "ref_title_full": "Synthesizing data structure manipulations from storyboards",
                "year": "2011",
                "full_block": "Synthesizing data structure manipulations from storyboards.\n"
            },
            {
                "ref_title_clean": "automated feedback generation for introductory programming assignments",
                "ref_title_full": "Automated feedback generation for introductory programming assignments",
                "year": "2013",
                "full_block": "Automated feedback generation for introductory programming\n  assignments.\n"
            },
            {
                "ref_title_clean": "program synthesis by sketching",
                "ref_title_full": "\\Program Synthesis By Sketching",
                "year": "2008",
                "full_block": "\\emph{Program Synthesis By Sketching}.\n"
            },
            {
                "ref_title_clean": "programming by sketching for bit streaming programs",
                "ref_title_full": "Programming by sketching for bit streaming programs",
                "year": "2005",
                "full_block": "Programming by sketching for bit-streaming programs.\n"
            },
            {
                "ref_title_clean": "a methodology for lisp program construction from examples",
                "ref_title_full": "A methodology for lisp program construction from examples",
                "year": "1977",
                "full_block": "A methodology for lisp program construction from examples.\n"
            },
            {
                "ref_title_clean": "transit specifying protocols with concolic snippets",
                "ref_title_full": "TRANSIT: specifying protocols with concolic snippets",
                "year": "2013",
                "full_block": "{TRANSIT:} specifying protocols with concolic snippets.\n"
            },
            {
                "ref_title_clean": "grammar as a foreign language",
                "ref_title_full": "Grammar as a foreign language",
                "year": "2015",
                "full_block": "Grammar as a foreign language.\n"
            }
        ],
        "refs_source": "unpacked_sources/1611.01855/iclr2017_conference.bbl"
    },
    "flashmeta a framework for inductive program synthesis": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Flashmeta: a framework for inductive program synthesis",
        "link": "none",
        "n_parents": 3,
        "year": "2015"
    },
    "neural programmer interpreters": {
        "id": "1511.06279",
        "depth": 1,
        "children_titles": [
            "neural reuse a fundamental organizational principle of the brain",
            "programmable reinforcement learning agents",
            "genetic programming an introduction volume1",
            "hierarchical reinforcement learning with the maxq value function decomposition",
            "programming in the brain a neural network theoretical framework",
            "a programmer\u2013interpreter neural network architecture for prefrontal cognitive control",
            "3d object detection and viewpoint estimation with a deformable 3d cuboid model",
            "neural turing machines",
            "long short term memory",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "neural gpus learn algorithms",
            "adam a method for stochastic optimization",
            "hierarchical apprenticeship learning with application to quadruped locomotion",
            "neural random access machines",
            "catastrophic interference in connectionist networks the sequential learning problem",
            "building program vector representations for deep learning",
            "neural programmer inducing latent programs with gradient descent",
            "complementary learning systems",
            "modular inverse reinforcement learning for visuomotor behavior",
            "parallel distributed processing explorations in the microstructure of cognition vol 1",
            "universal value function approximators",
            "learning to control fast weight memories an alternative to dynamic recurrent networks",
            "controlled and automatic processing behavior theory and biological mechanisms",
            "learning options through human interaction",
            "using matrices to model symbolic relationship",
            "sequence to sequence learning with neural networks",
            "between mdps and semi mdps a framework for temporal abstraction in reinforcement learning",
            "pointer networks",
            "learning to execute",
            "reinforcement learning neural turing machines",
            "learning simple algorithms from examples"
        ],
        "status": "expanded",
        "title_full": "Neural programmer interpreters",
        "link": "https://arxiv.org/abs/1511.06279",
        "n_parents": 6,
        "year": "2016",
        "children_full_dicts": [
            {
                "ref_title_clean": "neural reuse a fundamental organizational principle of the brain",
                "ref_title_full": "Neural reuse: A fundamental organizational principle of the brain",
                "year": "2010",
                "full_block": "Neural reuse: A fundamental organizational principle of the brain.\n"
            },
            {
                "ref_title_clean": "programmable reinforcement learning agents",
                "ref_title_full": "Programmable reinforcement learning agents",
                "year": "2001",
                "full_block": "Programmable reinforcement learning agents.\n"
            },
            {
                "ref_title_clean": "genetic programming an introduction volume1",
                "ref_title_full": "\\Genetic programming: An introduction, volume~1",
                "year": "1998",
                "full_block": "\\emph{Genetic programming: An introduction}, volume~1.\n"
            },
            {
                "ref_title_clean": "hierarchical reinforcement learning with the maxq value function decomposition",
                "ref_title_full": "Hierarchical reinforcement learning with the MAXQ value function decomposition",
                "year": "2000",
                "full_block": "Hierarchical reinforcement learning with the {MAXQ} value function\n  decomposition.\n"
            },
            {
                "ref_title_clean": "programming in the brain a neural network theoretical framework",
                "ref_title_full": "Programming in the brain: A neural network theoretical framework",
                "year": "2012",
                "full_block": "Programming in the brain: A neural network theoretical framework.\n"
            },
            {
                "ref_title_clean": "a programmer\u2013interpreter neural network architecture for prefrontal cognitive control",
                "ref_title_full": "A programmer\u2013interpreter neural network architecture for prefrontal cognitive control",
                "year": "2015",
                "full_block": "A programmer\u2013interpreter neural network architecture for prefrontal\n  cognitive control.\n"
            },
            {
                "ref_title_clean": "3d object detection and viewpoint estimation with a deformable 3d cuboid model",
                "ref_title_full": "3D object detection and viewpoint estimation with a deformable 3D cuboid model",
                "year": "2012",
                "full_block": "{3D} object detection and viewpoint estimation with a deformable {3D}\n  cuboid model.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural Turing machines",
                "year": "2014",
                "full_block": "Neural {Turing} machines.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring algorithmic patterns with stack augmented recurrent nets",
                "year": "2015",
                "full_block": "Inferring algorithmic patterns with stack-augmented recurrent nets.\n"
            },
            {
                "ref_title_clean": "neural gpus learn algorithms",
                "ref_title_full": "Neural gpus learn algorithms",
                "year": "2015",
                "full_block": "Neural gpus learn algorithms.\n"
            },
            {
                "ref_title_clean": "adam a method for stochastic optimization",
                "ref_title_full": "Adam: A method for stochastic optimization",
                "year": "none",
                "full_block": "Adam: A method for stochastic optimization.\n"
            },
            {
                "ref_title_clean": "hierarchical apprenticeship learning with application to quadruped locomotion",
                "ref_title_full": "Hierarchical apprenticeship learning with application to quadruped locomotion",
                "year": "2008",
                "full_block": "Hierarchical apprenticeship learning with application to quadruped\n  locomotion.\n"
            },
            {
                "ref_title_clean": "neural random access machines",
                "ref_title_full": "Neural random access machines",
                "year": "2015",
                "full_block": "Neural random-access machines.\n"
            },
            {
                "ref_title_clean": "catastrophic interference in connectionist networks the sequential learning problem",
                "ref_title_full": "Catastrophic interference in connectionist networks: The sequential learning problem",
                "year": "1989",
                "full_block": "Catastrophic interference in connectionist networks: {The} sequential\n  learning problem.\n"
            },
            {
                "ref_title_clean": "building program vector representations for deep learning",
                "ref_title_full": "Building program vector representations for deep learning",
                "year": "2014",
                "full_block": "Building program vector representations for deep learning.\n"
            },
            {
                "ref_title_clean": "neural programmer inducing latent programs with gradient descent",
                "ref_title_full": "Neural programmer: Inducing latent programs with gradient descent",
                "year": "2015",
                "full_block": "Neural programmer: Inducing latent programs with gradient descent.\n"
            },
            {
                "ref_title_clean": "complementary learning systems",
                "ref_title_full": "Complementary learning systems",
                "year": "2014",
                "full_block": "Complementary learning systems.\n"
            },
            {
                "ref_title_clean": "modular inverse reinforcement learning for visuomotor behavior",
                "ref_title_full": "Modular inverse reinforcement learning for visuomotor behavior",
                "year": "2013",
                "full_block": "Modular inverse reinforcement learning for visuomotor behavior.\n"
            },
            {
                "ref_title_clean": "parallel distributed processing explorations in the microstructure of cognition vol 1",
                "ref_title_full": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1",
                "year": "1986",
                "full_block": "Parallel distributed processing: Explorations in the microstructure\n  of cognition, vol. 1.\n"
            },
            {
                "ref_title_clean": "universal value function approximators",
                "ref_title_full": "Universal value function approximators",
                "year": "2015",
                "full_block": "Universal value function approximators.\n"
            },
            {
                "ref_title_clean": "learning to control fast weight memories an alternative to dynamic recurrent networks",
                "ref_title_full": "Learning to control fast weight memories: An alternative to dynamic recurrent networks",
                "year": "1992",
                "full_block": "Learning to control fast-weight memories: An alternative to dynamic\n  recurrent networks.\n"
            },
            {
                "ref_title_clean": "controlled and automatic processing behavior theory and biological mechanisms",
                "ref_title_full": "Controlled and automatic processing: behavior, theory, and biological mechanisms",
                "year": "2003",
                "full_block": "Controlled and automatic processing: behavior, theory, and biological\n  mechanisms.\n"
            },
            {
                "ref_title_clean": "learning options through human interaction",
                "ref_title_full": "Learning options through human interaction",
                "year": "2011",
                "full_block": "Learning options through human interaction.\n"
            },
            {
                "ref_title_clean": "using matrices to model symbolic relationship",
                "ref_title_full": "Using matrices to model symbolic relationship",
                "year": "2009",
                "full_block": "Using matrices to model symbolic relationship.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "2014",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "between mdps and semi mdps a framework for temporal abstraction in reinforcement learning",
                "ref_title_full": "Between MDPs and semi MDPs: A framework for temporal abstraction in reinforcement learning",
                "year": "1999",
                "full_block": "Between {MDPs} and {semi-MDPs}: A framework for temporal abstraction\n  in reinforcement learning.\n"
            },
            {
                "ref_title_clean": "pointer networks",
                "ref_title_full": "Pointer networks",
                "year": "2015",
                "full_block": "Pointer networks.\n"
            },
            {
                "ref_title_clean": "learning to execute",
                "ref_title_full": "Learning to execute",
                "year": "2014",
                "full_block": "Learning to execute.\n"
            },
            {
                "ref_title_clean": "reinforcement learning neural turing machines",
                "ref_title_full": "Reinforcement learning neural turing machines",
                "year": "2015",
                "full_block": "Reinforcement learning neural turing machines.\n"
            },
            {
                "ref_title_clean": "learning simple algorithms from examples",
                "ref_title_full": "Learning simple algorithms from examples",
                "year": "2015",
                "full_block": "Learning simple algorithms from examples.\n"
            }
        ],
        "refs_source": "unpacked_sources/1511.06279/iclr2016_full.bbl"
    },
    "programming with a differentiable forth interpreter": {
        "id": "1605.06640",
        "depth": 1,
        "children_titles": [
            "url urlhttptensorfloworg",
            "recursive program synthesis",
            "neural module networks",
            "learning to generate textual data",
            "starting forth",
            "adaptive neural compilation",
            "terpret a probabilistic programming language for program induction",
            "church a language for generative models",
            "neural turing machines",
            "hybrid computing using a neural network with dynamic external memory",
            "learning to transduce with unbounded memory",
            "a neural compiler",
            "long short term memory",
            "inferring algorithmic patterns with stack augmented recurrent nets",
            "neural gpus learn algorithms",
            "symbolic execution and program testing",
            "adam a method for stochastic optimization",
            "inductive programming a survey of program synthesis techniques",
            "parsing algebraic word problems into equations",
            "genetic programming on the programming of computers by means of natural selection volume1",
            "neural random access machines",
            "learning to automatically solve algebra word problems",
            "learning repetitive text editing procedures with smartedit",
            "gradient based hyperparameter optimization through reversible learning",
            "toward automatic program synthesis",
            "neural programmer inducing latent programs with gradient descent",
            "adding gradient noise improves learning for very deep networks",
            "evolutionary program induction of binary machine code and its applications",
            "neural programmer interpreters",
            "solving general arithmetic word problems",
            "reasoning about quantities in natural language",
            "neural programming language",
            "programming by sketching for bit streaming programs",
            "combinatorial sketching for finite programs",
            "sequence to sequence learning with neural networks"
        ],
        "status": "expanded",
        "title_full": "Programming with a differentiable forth interpreter",
        "link": "https://arxiv.org/abs/1605.06640",
        "n_parents": 4,
        "year": "2016",
        "children_full_dicts": [
            {
                "ref_title_clean": "url urlhttptensorfloworg",
                "ref_title_full": "URL \\urlhttp://tensorflow.org/",
                "year": "2015",
                "full_block": "URL \\url{http://tensorflow.org/}.\n"
            },
            {
                "ref_title_clean": "recursive program synthesis",
                "ref_title_full": "Recursive program synthesis",
                "year": "2013",
                "full_block": "Recursive program synthesis.\n"
            },
            {
                "ref_title_clean": "neural module networks",
                "ref_title_full": "Neural module networks",
                "year": "2016",
                "full_block": "Neural module networks.\n"
            },
            {
                "ref_title_clean": "learning to generate textual data",
                "ref_title_full": "Learning to generate textual data",
                "year": "2016",
                "full_block": "Learning to generate textual data.\n"
            },
            {
                "ref_title_clean": "starting forth",
                "ref_title_full": "\\Starting Forth",
                "year": "1980",
                "full_block": "\\emph{{Starting Forth}}.\n"
            },
            {
                "ref_title_clean": "adaptive neural compilation",
                "ref_title_full": "Adaptive neural compilation",
                "year": "2016",
                "full_block": "Adaptive neural compilation.\n"
            },
            {
                "ref_title_clean": "terpret a probabilistic programming language for program induction",
                "ref_title_full": "TerpreT: A Probabilistic Programming Language for Program Induction",
                "year": "2016",
                "full_block": "{TerpreT: A Probabilistic Programming Language for Program\n  Induction}.\n"
            },
            {
                "ref_title_clean": "church a language for generative models",
                "ref_title_full": "Church: a language for generative models",
                "year": "2008",
                "full_block": "Church: a language for generative models.\n"
            },
            {
                "ref_title_clean": "neural turing machines",
                "ref_title_full": "Neural Turing Machines",
                "year": "2014",
                "full_block": "{Neural Turing Machines}.\n"
            },
            {
                "ref_title_clean": "hybrid computing using a neural network with dynamic external memory",
                "ref_title_full": "Hybrid computing using a neural network with dynamic external memory",
                "year": "2016",
                "full_block": "Hybrid computing using a neural network with dynamic external memory.\n"
            },
            {
                "ref_title_clean": "learning to transduce with unbounded memory",
                "ref_title_full": "Learning to Transduce with Unbounded Memory",
                "year": "2015",
                "full_block": "{Learning to Transduce with Unbounded Memory}.\n"
            },
            {
                "ref_title_clean": "a neural compiler",
                "ref_title_full": "A Neural compiler",
                "year": "1995",
                "full_block": "{A Neural compiler}.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "inferring algorithmic patterns with stack augmented recurrent nets",
                "ref_title_full": "Inferring Algorithmic Patterns with Stack Augmented Recurrent Nets",
                "year": "2015",
                "full_block": "{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}.\n"
            },
            {
                "ref_title_clean": "neural gpus learn algorithms",
                "ref_title_full": "Neural GPUs learn algorithms",
                "year": "2015",
                "full_block": "{Neural GPUs learn algorithms}.\n"
            },
            {
                "ref_title_clean": "symbolic execution and program testing",
                "ref_title_full": "Symbolic Execution and Program Testing",
                "year": "1976",
                "full_block": "{Symbolic Execution and Program Testing}.\n"
            },
            {
                "ref_title_clean": "adam a method for stochastic optimization",
                "ref_title_full": "Adam: A Method for Stochastic Optimization",
                "year": "2015",
                "full_block": "{Adam: A Method for Stochastic Optimization}.\n"
            },
            {
                "ref_title_clean": "inductive programming a survey of program synthesis techniques",
                "ref_title_full": "Inductive Programming: A Survey of Program Synthesis Techniques",
                "year": "2009",
                "full_block": "{Inductive Programming: A Survey of Program Synthesis Techniques}.\n"
            },
            {
                "ref_title_clean": "parsing algebraic word problems into equations",
                "ref_title_full": "Parsing Algebraic Word Problems into Equations",
                "year": "2015",
                "full_block": "{Parsing Algebraic Word Problems into Equations}.\n"
            },
            {
                "ref_title_clean": "genetic programming on the programming of computers by means of natural selection volume1",
                "ref_title_full": "\\Genetic Programming: On the Programming of Computers by Means of Natural Selection, volume~1",
                "year": "1992",
                "full_block": "\\emph{{Genetic Programming: On the Programming of Computers by Means\n  of Natural Selection}}, volume~1.\n"
            },
            {
                "ref_title_clean": "neural random access machines",
                "ref_title_full": "Neural Random Access Machines",
                "year": "2016",
                "full_block": "{Neural Random-Access Machines}.\n"
            },
            {
                "ref_title_clean": "learning to automatically solve algebra word problems",
                "ref_title_full": "Learning to Automatically Solve Algebra Word Problems",
                "year": "2014",
                "full_block": "{Learning to Automatically Solve Algebra Word Problems}.\n"
            },
            {
                "ref_title_clean": "learning repetitive text editing procedures with smartedit",
                "ref_title_full": "Learning repetitive text editing procedures with smartedit",
                "year": "2001",
                "full_block": "Learning repetitive text-editing procedures with smartedit.\n"
            },
            {
                "ref_title_clean": "gradient based hyperparameter optimization through reversible learning",
                "ref_title_full": "Gradient based Hyperparameter Optimization through Reversible Learning",
                "year": "2015",
                "full_block": "{Gradient-based Hyperparameter Optimization through Reversible\n  Learning}.\n"
            },
            {
                "ref_title_clean": "toward automatic program synthesis",
                "ref_title_full": "Toward automatic program synthesis",
                "year": "1971",
                "full_block": "Toward automatic program synthesis.\n"
            },
            {
                "ref_title_clean": "neural programmer inducing latent programs with gradient descent",
                "ref_title_full": "Neural Programmer: Inducing latent programs with gradient descent",
                "year": "none",
                "full_block": "{Neural Programmer: Inducing latent programs with gradient descent}.\n"
            },
            {
                "ref_title_clean": "adding gradient noise improves learning for very deep networks",
                "ref_title_full": "Adding Gradient Noise Improves Learning for Very Deep Networks",
                "year": "none",
                "full_block": "{Adding Gradient Noise Improves Learning for Very Deep Networks}.\n"
            },
            {
                "ref_title_clean": "evolutionary program induction of binary machine code and its applications",
                "ref_title_full": "\\Evolutionary Program Induction of Binary Machine Code and its Applications",
                "year": "1997",
                "full_block": "\\emph{{Evolutionary Program Induction of Binary Machine Code and its\n  Applications}}.\n"
            },
            {
                "ref_title_clean": "neural programmer interpreters",
                "ref_title_full": "Neural programmer interpreters",
                "year": "2015",
                "full_block": "Neural programmer-interpreters.\n"
            },
            {
                "ref_title_clean": "solving general arithmetic word problems",
                "ref_title_full": "Solving General Arithmetic Word Problems",
                "year": "2015",
                "full_block": "{Solving General Arithmetic Word Problems}.\n"
            },
            {
                "ref_title_clean": "reasoning about quantities in natural language",
                "ref_title_full": "Reasoning about quantities in natural language",
                "year": "2015",
                "full_block": "Reasoning about quantities in natural language.\n"
            },
            {
                "ref_title_clean": "neural programming language",
                "ref_title_full": "Neural Programming Language",
                "year": "1994",
                "full_block": "{Neural Programming Language}.\n"
            },
            {
                "ref_title_clean": "programming by sketching for bit streaming programs",
                "ref_title_full": "Programming by Sketching for Bit streaming Programs",
                "year": "2005",
                "full_block": "{Programming by Sketching for Bit-streaming Programs}.\n"
            },
            {
                "ref_title_clean": "combinatorial sketching for finite programs",
                "ref_title_full": "Combinatorial Sketching for Finite Programs",
                "year": "2006",
                "full_block": "{Combinatorial Sketching for Finite Programs}.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to Sequence Learning with Neural Networks",
                "year": "2014",
                "full_block": "{Sequence to Sequence Learning with Neural Networks}.\n"
            }
        ],
        "refs_source": "unpacked_sources/1605.06640/main.bbl"
    },
    "sequence to sequence learning with neural networks": {
        "id": "1409.3215",
        "depth": 1,
        "children_titles": [
            "joint language and translation modeling with recurrent neural networks",
            "neural machine translation by jointly learning to align and translate",
            "a neural probabilistic language model",
            "learning long term dependencies with gradient descent is difficult",
            "learning phrase representations using rnn encoder decoder for statistical machine translation",
            "multi column deep neural networks for image classification",
            "context dependent pre trained deep neural networks for large vocabulary speech recognition",
            "fast and robust neural network joint models for statistical machine translation",
            "edinburghs phrase based machine translation systems for wmt 14",
            "generating sequences with recurrent neural networks",
            "connectionist temporal classification labelling unsegmented sequence data with recurrent neural networks",
            "multilingual distributed representations without word alignment",
            "deep neural networks for acoustic modeling in speech recognition",
            "untersuchungen zu dynamischen neuronalen netzen",
            "long short term memory",
            "lstm can solve hard long time lag problems",
            "recurrent continuous translation models",
            "imagenet classification with deep convolutional neural networks",
            "building high level features using large scale unsupervised learning",
            "gradient based learning applied to document recognition",
            "statistical language models based on neural networks",
            "recurrent neural network based language model",
            "bleu a method for automatic evaluation of machine translation",
            "on the difficulty of training recurrent neural networks",
            "overcoming the curse of sentence length for neural machine translation using automatic segmentation",
            "on small depth threshold circuits",
            "learning representations by back propagating errors",
            "university le mans",
            "lstm neural networks for language modeling",
            "backpropagation through time what it does and how to do it"
        ],
        "status": "expanded",
        "title_full": "Sequence to sequence learning with neural networks",
        "link": "https://arxiv.org/abs/1409.3215",
        "n_parents": 12,
        "year": "2014",
        "children_full_dicts": [
            {
                "ref_title_clean": "joint language and translation modeling with recurrent neural networks",
                "ref_title_full": "Joint language and translation modeling with recurrent neural networks",
                "year": "2013",
                "full_block": "Joint language and translation modeling with recurrent neural\n  networks.\n"
            },
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "2014",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "a neural probabilistic language model",
                "ref_title_full": "A neural probabilistic language model",
                "year": "2003",
                "full_block": "A neural probabilistic language model.\n"
            },
            {
                "ref_title_clean": "learning long term dependencies with gradient descent is difficult",
                "ref_title_full": "Learning long term dependencies with gradient descent is difficult",
                "year": "1994",
                "full_block": "Learning long-term dependencies with gradient descent is difficult.\n"
            },
            {
                "ref_title_clean": "learning phrase representations using rnn encoder decoder for statistical machine translation",
                "ref_title_full": "Learning phrase representations using RNN encoder decoder for statistical machine translation",
                "year": "2014",
                "full_block": "Learning phrase representations using {RNN} encoder-decoder for\n  statistical machine translation.\n"
            },
            {
                "ref_title_clean": "multi column deep neural networks for image classification",
                "ref_title_full": "Multi column deep neural networks for image classification",
                "year": "2012",
                "full_block": "Multi-column deep neural networks for image classification.\n"
            },
            {
                "ref_title_clean": "context dependent pre trained deep neural networks for large vocabulary speech recognition",
                "ref_title_full": "Context dependent pre trained deep neural networks for large vocabulary speech recognition",
                "year": "2012",
                "full_block": "Context-dependent pre-trained deep neural networks for large\n  vocabulary speech recognition.\n"
            },
            {
                "ref_title_clean": "fast and robust neural network joint models for statistical machine translation",
                "ref_title_full": "Fast and robust neural network joint models for statistical machine translation",
                "year": "2014",
                "full_block": "Fast and robust neural network joint models for statistical machine\n  translation.\n"
            },
            {
                "ref_title_clean": "edinburghs phrase based machine translation systems for wmt 14",
                "ref_title_full": "Edinburgh's phrase based machine translation systems for wmt 14",
                "year": "2014",
                "full_block": "Edinburgh's phrase-based machine translation systems for wmt-14.\n"
            },
            {
                "ref_title_clean": "generating sequences with recurrent neural networks",
                "ref_title_full": "Generating sequences with recurrent neural networks",
                "year": "2013",
                "full_block": "Generating sequences with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "connectionist temporal classification labelling unsegmented sequence data with recurrent neural networks",
                "ref_title_full": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
                "year": "2006",
                "full_block": "Connectionist temporal classification: labelling unsegmented sequence\n  data with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "multilingual distributed representations without word alignment",
                "ref_title_full": "Multilingual distributed representations without word alignment",
                "year": "2014",
                "full_block": "Multilingual distributed representations without word alignment.\n"
            },
            {
                "ref_title_clean": "deep neural networks for acoustic modeling in speech recognition",
                "ref_title_full": "Deep neural networks for acoustic modeling in speech recognition",
                "year": "2012",
                "full_block": "Deep neural networks for acoustic modeling in speech recognition.\n"
            },
            {
                "ref_title_clean": "untersuchungen zu dynamischen neuronalen netzen",
                "ref_title_full": "Untersuchungen zu dynamischen neuronalen netzen",
                "year": "1991",
                "full_block": "Untersuchungen zu dynamischen neuronalen netzen.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "lstm can solve hard long time lag problems",
                "ref_title_full": "LSTM can solve hard long time lag problems",
                "year": "none",
                "full_block": "{LSTM} can solve hard long time lag problems.\n"
            },
            {
                "ref_title_clean": "recurrent continuous translation models",
                "ref_title_full": "Recurrent continuous translation models",
                "year": "2013",
                "full_block": "Recurrent continuous translation models.\n"
            },
            {
                "ref_title_clean": "imagenet classification with deep convolutional neural networks",
                "ref_title_full": "ImageNet classification with deep convolutional neural networks",
                "year": "2012",
                "full_block": "{ImageNet} classification with deep convolutional neural networks.\n"
            },
            {
                "ref_title_clean": "building high level features using large scale unsupervised learning",
                "ref_title_full": "Building high level features using large scale unsupervised learning",
                "year": "2012",
                "full_block": "Building high-level features using large scale unsupervised learning.\n"
            },
            {
                "ref_title_clean": "gradient based learning applied to document recognition",
                "ref_title_full": "Gradient based learning applied to document recognition",
                "year": "1998",
                "full_block": "Gradient-based learning applied to document recognition.\n"
            },
            {
                "ref_title_clean": "statistical language models based on neural networks",
                "ref_title_full": "Statistical Language Models based on Neural Networks",
                "year": "2012",
                "full_block": "{\\em Statistical Language Models based on Neural Networks}.\n"
            },
            {
                "ref_title_clean": "recurrent neural network based language model",
                "ref_title_full": "Recurrent neural network based language model",
                "year": "2010",
                "full_block": "Recurrent neural network based language model.\n"
            },
            {
                "ref_title_clean": "bleu a method for automatic evaluation of machine translation",
                "ref_title_full": "BLEU: a method for automatic evaluation of machine translation",
                "year": "2002",
                "full_block": "{BLEU}: a method for automatic evaluation of machine translation.\n"
            },
            {
                "ref_title_clean": "on the difficulty of training recurrent neural networks",
                "ref_title_full": "On the difficulty of training recurrent neural networks",
                "year": "2012",
                "full_block": "On the difficulty of training recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "overcoming the curse of sentence length for neural machine translation using automatic segmentation",
                "ref_title_full": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation",
                "year": "2014",
                "full_block": "Overcoming the curse of sentence length for neural machine\n  translation using automatic segmentation.\n"
            },
            {
                "ref_title_clean": "on small depth threshold circuits",
                "ref_title_full": "On small depth threshold circuits",
                "year": "1992",
                "full_block": "On small depth threshold circuits.\n"
            },
            {
                "ref_title_clean": "learning representations by back propagating errors",
                "ref_title_full": "Learning representations by back propagating errors",
                "year": "1986",
                "full_block": "Learning representations by back-propagating errors.\n"
            },
            {
                "ref_title_clean": "university le mans",
                "ref_title_full": "University le mans",
                "year": "2014",
                "full_block": "University le mans.\n"
            },
            {
                "ref_title_clean": "lstm neural networks for language modeling",
                "ref_title_full": "LSTM neural networks for language modeling",
                "year": "2010",
                "full_block": "{LSTM} neural networks for language modeling.\n"
            },
            {
                "ref_title_clean": "backpropagation through time what it does and how to do it",
                "ref_title_full": "Backpropagation through time: what it does and how to do it",
                "year": "1990",
                "full_block": "Backpropagation through time: what it does and how to do it.\n"
            }
        ],
        "refs_source": "unpacked_sources/1409.3215/translate.bbl"
    },
    "improved semantic representations from tree structured long short term memory networks": {
        "id": "1503.00075",
        "depth": 1,
        "children_titles": [
            "neural machine translation by jointly learning to align and translate",
            "learning long term dependencies with gradient descent is difficult",
            "the meaning factory formal semantics for recognizing textual entailment and determining semantic similarity",
            "a convolutional neural network for modelling sentences",
            "a fast and accurate dependency parser using neural networks",
            "natural language processing almost from scratch",
            "adaptive subgradient methods for online learning and stochastic optimization",
            "finding structure in time",
            "the measurement of textual coherence with latent semantic analysis",
            "ppdb the paraphrase database",
            "learning task dependent distributed representations by backpropagation through structure",
            "hybrid speech recognition with deep bidirectional lstm",
            "multi step regression learning for compositional distributional semantics",
            "improving neural networks by preventing co adaptation of feature detectors",
            "the vanishing gradient problem during learning recurrent neural nets and problem solutions",
            "long short term memory",
            "improving word representations via global context and multiple word prototypes",
            "deep recursive neural networks for compositionality in language",
            "unal nlp combining soft cardinality features for semantic textual similarity relatedness and entailment",
            "convolutional neural networks for sentence classification",
            "accurate unlexicalized parsing",
            "illinois lh a denotational and distributional approach to semantics",
            "a solution to platos problem the latent semantic analysis theory of acquisition induction and representation of knowledge",
            "distributed representations of sentences and documents",
            "statistical language models based on neural networks",
            "distributed representations of words and phrases and their compositionality",
            "composition in distributional models of semantics",
            "glove global vectors for word representation",
            "learning representations by back propagating errors",
            "semantic compositionality through recursive matrix vector spaces",
            "grounded compositional semantics for finding and describing images with sentences",
            "parsing natural scenes and natural language with recursive neural networks",
            "recursive deep models for semantic compositionality over a sentiment treebank",
            "modeling documents with deep boltzmann machines",
            "sequence to sequence learning with neural networks",
            "word representations a simple and general method for semi supervised learning",
            "show and tell a neural image caption generator",
            "compositional matrix space models for sentiment analysis",
            "learning to execute",
            "ecnu one stone two birds ensemble of heterogenous measures for semantic relatedness and textual entailment"
        ],
        "status": "expanded",
        "title_full": "Improved semantic representations from tree structured long short term memory networks",
        "link": "https://arxiv.org/abs/1503.00075",
        "n_parents": 1,
        "year": "2015",
        "children_full_dicts": [
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "none",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "learning long term dependencies with gradient descent is difficult",
                "ref_title_full": "Learning long term dependencies with gradient descent is difficult",
                "year": "none",
                "full_block": "Learning long-term dependencies with gradient descent is difficult.\n"
            },
            {
                "ref_title_clean": "the meaning factory formal semantics for recognizing textual entailment and determining semantic similarity",
                "ref_title_full": "The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity",
                "year": "none",
                "full_block": "{The Meaning Factory}: Formal semantics for recognizing textual\n  entailment and determining semantic similarity.\n"
            },
            {
                "ref_title_clean": "a convolutional neural network for modelling sentences",
                "ref_title_full": "A convolutional neural network for modelling sentences",
                "year": "none",
                "full_block": "A convolutional neural network for modelling sentences.\n"
            },
            {
                "ref_title_clean": "a fast and accurate dependency parser using neural networks",
                "ref_title_full": "A fast and accurate dependency parser using neural networks",
                "year": "none",
                "full_block": "A fast and accurate dependency parser using neural networks.\n"
            },
            {
                "ref_title_clean": "natural language processing almost from scratch",
                "ref_title_full": "Natural language processing (almost) from scratch",
                "year": "none",
                "full_block": "Natural language processing (almost) from scratch.\n"
            },
            {
                "ref_title_clean": "adaptive subgradient methods for online learning and stochastic optimization",
                "ref_title_full": "Adaptive subgradient methods for online learning and stochastic optimization",
                "year": "none",
                "full_block": "Adaptive subgradient methods for online learning and stochastic\n  optimization.\n"
            },
            {
                "ref_title_clean": "finding structure in time",
                "ref_title_full": "Finding structure in time",
                "year": "none",
                "full_block": "Finding structure in time.\n"
            },
            {
                "ref_title_clean": "the measurement of textual coherence with latent semantic analysis",
                "ref_title_full": "The measurement of textual coherence with latent semantic analysis",
                "year": "none",
                "full_block": "The measurement of textual coherence with latent semantic analysis.\n"
            },
            {
                "ref_title_clean": "ppdb the paraphrase database",
                "ref_title_full": "PPDB: The Paraphrase Database",
                "year": "none",
                "full_block": "{PPDB}: {The Paraphrase Database}.\n"
            },
            {
                "ref_title_clean": "learning task dependent distributed representations by backpropagation through structure",
                "ref_title_full": "Learning task dependent distributed representations by backpropagation through structure",
                "year": "none",
                "full_block": "Learning task-dependent distributed representations by\n  backpropagation through structure.\n"
            },
            {
                "ref_title_clean": "hybrid speech recognition with deep bidirectional lstm",
                "ref_title_full": "Hybrid speech recognition with deep bidirectional LSTM",
                "year": "none",
                "full_block": "Hybrid speech recognition with deep bidirectional {LSTM}.\n"
            },
            {
                "ref_title_clean": "multi step regression learning for compositional distributional semantics",
                "ref_title_full": "Multi step regression learning for compositional distributional semantics",
                "year": "none",
                "full_block": "Multi-step regression learning for compositional distributional\n  semantics.\n"
            },
            {
                "ref_title_clean": "improving neural networks by preventing co adaptation of feature detectors",
                "ref_title_full": "Improving neural networks by preventing co adaptation of feature detectors",
                "year": "none",
                "full_block": "Improving neural networks by preventing co-adaptation of feature\n  detectors.\n"
            },
            {
                "ref_title_clean": "the vanishing gradient problem during learning recurrent neural nets and problem solutions",
                "ref_title_full": "The vanishing gradient problem during learning recurrent neural nets and problem solutions",
                "year": "none",
                "full_block": "The vanishing gradient problem during learning recurrent neural nets\n  and problem solutions.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long Short Term Memory",
                "year": "none",
                "full_block": "{Long Short-Term Memory}.\n"
            },
            {
                "ref_title_clean": "improving word representations via global context and multiple word prototypes",
                "ref_title_full": "Improving word representations via global context and multiple word prototypes",
                "year": "none",
                "full_block": "Improving word representations via global context and multiple word\n  prototypes.\n"
            },
            {
                "ref_title_clean": "deep recursive neural networks for compositionality in language",
                "ref_title_full": "Deep recursive neural networks for compositionality in language",
                "year": "none",
                "full_block": "Deep recursive neural networks for compositionality in language.\n"
            },
            {
                "ref_title_clean": "unal nlp combining soft cardinality features for semantic textual similarity relatedness and entailment",
                "ref_title_full": "UNAL NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment",
                "year": "none",
                "full_block": "{UNAL-NLP}: Combining soft cardinality features for semantic textual\n  similarity, relatedness and entailment.\n"
            },
            {
                "ref_title_clean": "convolutional neural networks for sentence classification",
                "ref_title_full": "Convolutional neural networks for sentence classification",
                "year": "none",
                "full_block": "Convolutional neural networks for sentence classification.\n"
            },
            {
                "ref_title_clean": "accurate unlexicalized parsing",
                "ref_title_full": "Accurate unlexicalized parsing",
                "year": "none",
                "full_block": "Accurate unlexicalized parsing.\n"
            },
            {
                "ref_title_clean": "illinois lh a denotational and distributional approach to semantics",
                "ref_title_full": "Illinois lh: A denotational and distributional approach to semantics",
                "year": "none",
                "full_block": "Illinois-lh: A denotational and distributional approach to semantics.\n"
            },
            {
                "ref_title_clean": "a solution to platos problem the latent semantic analysis theory of acquisition induction and representation of knowledge",
                "ref_title_full": "A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
                "year": "none",
                "full_block": "A solution to plato's problem: The latent semantic analysis theory of\n  acquisition, induction, and representation of knowledge.\n"
            },
            {
                "ref_title_clean": "distributed representations of sentences and documents",
                "ref_title_full": "Distributed representations of sentences and documents",
                "year": "none",
                "full_block": "Distributed representations of sentences and documents.\n"
            },
            {
                "ref_title_clean": "statistical language models based on neural networks",
                "ref_title_full": "Statistical Language Models Based on Neural Networks\\/",
                "year": "none",
                "full_block": "{\\em Statistical Language Models Based on Neural Networks\\/}.\n"
            },
            {
                "ref_title_clean": "distributed representations of words and phrases and their compositionality",
                "ref_title_full": "Distributed representations of words and phrases and their compositionality",
                "year": "none",
                "full_block": "Distributed representations of words and phrases and their\n  compositionality.\n"
            },
            {
                "ref_title_clean": "composition in distributional models of semantics",
                "ref_title_full": "Composition in distributional models of semantics",
                "year": "none",
                "full_block": "Composition in distributional models of semantics.\n"
            },
            {
                "ref_title_clean": "glove global vectors for word representation",
                "ref_title_full": "Glove: Global vectors for word representation",
                "year": "none",
                "full_block": "Glove: Global vectors for word representation.\n"
            },
            {
                "ref_title_clean": "learning representations by back propagating errors",
                "ref_title_full": "Learning representations by back propagating errors",
                "year": "none",
                "full_block": "Learning representations by back-propagating errors.\n"
            },
            {
                "ref_title_clean": "semantic compositionality through recursive matrix vector spaces",
                "ref_title_full": "Semantic compositionality through recursive matrix vector spaces",
                "year": "none",
                "full_block": "Semantic compositionality through recursive matrix-vector spaces.\n"
            },
            {
                "ref_title_clean": "grounded compositional semantics for finding and describing images with sentences",
                "ref_title_full": "Grounded compositional semantics for finding and describing images with sentences",
                "year": "none",
                "full_block": "Grounded compositional semantics for finding and describing images\n  with sentences.\n"
            },
            {
                "ref_title_clean": "parsing natural scenes and natural language with recursive neural networks",
                "ref_title_full": "Parsing natural scenes and natural language with recursive neural networks",
                "year": "none",
                "full_block": "Parsing natural scenes and natural language with recursive neural\n  networks.\n"
            },
            {
                "ref_title_clean": "recursive deep models for semantic compositionality over a sentiment treebank",
                "ref_title_full": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "year": "none",
                "full_block": "Recursive deep models for semantic compositionality over a sentiment\n  treebank.\n"
            },
            {
                "ref_title_clean": "modeling documents with deep boltzmann machines",
                "ref_title_full": "Modeling documents with deep boltzmann machines",
                "year": "none",
                "full_block": "Modeling documents with deep boltzmann machines.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "none",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "word representations a simple and general method for semi supervised learning",
                "ref_title_full": "Word representations: a simple and general method for semi supervised learning",
                "year": "none",
                "full_block": "Word representations: a simple and general method for semi-supervised\n  learning.\n"
            },
            {
                "ref_title_clean": "show and tell a neural image caption generator",
                "ref_title_full": "Show and tell: A neural image caption generator",
                "year": "none",
                "full_block": "Show and tell: A neural image caption generator.\n"
            },
            {
                "ref_title_clean": "compositional matrix space models for sentiment analysis",
                "ref_title_full": "Compositional matrix space models for sentiment analysis",
                "year": "none",
                "full_block": "Compositional matrix-space models for sentiment analysis.\n"
            },
            {
                "ref_title_clean": "learning to execute",
                "ref_title_full": "Learning to execute",
                "year": "none",
                "full_block": "Learning to execute.\n"
            },
            {
                "ref_title_clean": "ecnu one stone two birds ensemble of heterogenous measures for semantic relatedness and textual entailment",
                "ref_title_full": "ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment",
                "year": "none",
                "full_block": "{ECNU}: One stone two birds: Ensemble of heterogenous measures for\n  semantic relatedness and textual entailment.\n"
            }
        ],
        "refs_source": "unpacked_sources/1503.00075/treelstm.bbl"
    },
    "grammar as a foreign language": {
        "id": "1412.7449",
        "depth": 1,
        "children_titles": [
            "sequence to sequence learning with neural networks",
            "neural machine translation by jointly learning to align and translate",
            "addressing the rare word problem in neural machine translation",
            "on using very large target vocabulary for neural machine translation",
            "long short term memory",
            "efficient estimation of word representations in vector space",
            "ontonotes the 90 solution",
            "questionbank creating a corpus of parse annotated questions",
            "building a large annotated corpus of english the penn treebank",
            "ambiguity aware ensemble training for semi supervised dependency parsing",
            "learning accurate compact and interpretable tree annotation",
            "fast and accurate shift reduce constituent parsing",
            "products of random latent variable grammars",
            "self training pcfg grammars with latent annotations across languages",
            "effective self training for parsing",
            "self training with products of latent variable grammars",
            "sparser better faster gpu parsing",
            "three generative lexicalised models for statistical parsing",
            "accurate unlexicalized parsing",
            "inducing history representations for broad coverage statistical parsing",
            "discriminative training of a neural network statistical parser",
            "constituent parsing with incremental sigmoid belief networks",
            "deep learning for efficient discriminative parsing",
            "parsing natural scenes and natural language with recursive neural networks",
            "a linear observed time statistical parser based on maximum entropy models",
            "incremental parsing with the perceptron algorithm",
            "generating sequences with recurrent neural networks",
            "end to end continuous speech recognition using attention based recurrent nn first results",
            "recurrent continuous translation models",
            "show and tell a neural image caption generator",
            "a neural network for learning how to parse tree adjoining grammar"
        ],
        "status": "expanded",
        "title_full": "Grammar as a foreign language",
        "link": "https://arxiv.org/abs/1412.7449",
        "n_parents": 3,
        "year": "2015",
        "children_full_dicts": [
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "2014",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "2014",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "addressing the rare word problem in neural machine translation",
                "ref_title_full": "Addressing the rare word problem in neural machine translation",
                "year": "2014",
                "full_block": "Addressing the rare word problem in neural machine translation.\n"
            },
            {
                "ref_title_clean": "on using very large target vocabulary for neural machine translation",
                "ref_title_full": "On using very large target vocabulary for neural machine translation",
                "year": "2014",
                "full_block": "On using very large target vocabulary for neural machine translation.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "efficient estimation of word representations in vector space",
                "ref_title_full": "Efficient estimation of word representations in vector space",
                "year": "2013",
                "full_block": "Efficient estimation of word representations in vector space.\n"
            },
            {
                "ref_title_clean": "ontonotes the 90 solution",
                "ref_title_full": "Ontonotes: The 90\\% solution",
                "year": "2006",
                "full_block": "Ontonotes: The 90\\% solution.\n"
            },
            {
                "ref_title_clean": "questionbank creating a corpus of parse annotated questions",
                "ref_title_full": "Questionbank: Creating a corpus of parse annotated questions",
                "year": "2006",
                "full_block": "Questionbank: Creating a corpus of parse-annotated questions.\n"
            },
            {
                "ref_title_clean": "building a large annotated corpus of english the penn treebank",
                "ref_title_full": "Building a large annotated corpus of english: The penn treebank",
                "year": "1993",
                "full_block": "Building a large annotated corpus of english: The penn treebank.\n"
            },
            {
                "ref_title_clean": "ambiguity aware ensemble training for semi supervised dependency parsing",
                "ref_title_full": "Ambiguity aware ensemble training for semi supervised dependency parsing",
                "year": "2014",
                "full_block": "Ambiguity-aware ensemble training for semi-supervised dependency\n  parsing.\n"
            },
            {
                "ref_title_clean": "learning accurate compact and interpretable tree annotation",
                "ref_title_full": "Learning accurate, compact, and interpretable tree annotation",
                "year": "2006",
                "full_block": "Learning accurate, compact, and interpretable tree annotation.\n"
            },
            {
                "ref_title_clean": "fast and accurate shift reduce constituent parsing",
                "ref_title_full": "Fast and accurate shift reduce constituent parsing",
                "year": "2013",
                "full_block": "Fast and accurate shift-reduce constituent parsing.\n"
            },
            {
                "ref_title_clean": "products of random latent variable grammars",
                "ref_title_full": "Products of random latent variable grammars",
                "year": "2010",
                "full_block": "Products of random latent variable grammars.\n"
            },
            {
                "ref_title_clean": "self training pcfg grammars with latent annotations across languages",
                "ref_title_full": "Self training PCFG grammars with latent annotations across languages",
                "year": "2009",
                "full_block": "Self-training {PCFG} grammars with latent annotations across\n  languages.\n"
            },
            {
                "ref_title_clean": "effective self training for parsing",
                "ref_title_full": "Effective self training for parsing",
                "year": "2006",
                "full_block": "Effective self-training for parsing.\n"
            },
            {
                "ref_title_clean": "self training with products of latent variable grammars",
                "ref_title_full": "Self training with products of latent variable grammars",
                "year": "2010",
                "full_block": "Self-training with products of latent variable grammars.\n"
            },
            {
                "ref_title_clean": "sparser better faster gpu parsing",
                "ref_title_full": "Sparser, better, faster gpu parsing",
                "year": "2014",
                "full_block": "Sparser, better, faster gpu parsing.\n"
            },
            {
                "ref_title_clean": "three generative lexicalised models for statistical parsing",
                "ref_title_full": "Three generative, lexicalised models for statistical parsing",
                "year": "1997",
                "full_block": "Three generative, lexicalised models for statistical parsing.\n"
            },
            {
                "ref_title_clean": "accurate unlexicalized parsing",
                "ref_title_full": "Accurate unlexicalized parsing",
                "year": "2003",
                "full_block": "Accurate unlexicalized parsing.\n"
            },
            {
                "ref_title_clean": "inducing history representations for broad coverage statistical parsing",
                "ref_title_full": "Inducing history representations for broad coverage statistical parsing",
                "year": "2003",
                "full_block": "Inducing history representations for broad coverage statistical\n  parsing.\n"
            },
            {
                "ref_title_clean": "discriminative training of a neural network statistical parser",
                "ref_title_full": "Discriminative training of a neural network statistical parser",
                "year": "2004",
                "full_block": "Discriminative training of a neural network statistical parser.\n"
            },
            {
                "ref_title_clean": "constituent parsing with incremental sigmoid belief networks",
                "ref_title_full": "Constituent parsing with incremental sigmoid belief networks",
                "year": "2007",
                "full_block": "Constituent parsing with incremental sigmoid belief networks.\n"
            },
            {
                "ref_title_clean": "deep learning for efficient discriminative parsing",
                "ref_title_full": "Deep learning for efficient discriminative parsing",
                "year": "2011",
                "full_block": "Deep learning for efficient discriminative parsing.\n"
            },
            {
                "ref_title_clean": "parsing natural scenes and natural language with recursive neural networks",
                "ref_title_full": "Parsing natural scenes and natural language with recursive neural networks",
                "year": "2011",
                "full_block": "Parsing natural scenes and natural language with recursive neural\n  networks.\n"
            },
            {
                "ref_title_clean": "a linear observed time statistical parser based on maximum entropy models",
                "ref_title_full": "A linear observed time statistical parser based on maximum entropy models",
                "year": "1997",
                "full_block": "A linear observed time statistical parser based on maximum entropy\n  models.\n"
            },
            {
                "ref_title_clean": "incremental parsing with the perceptron algorithm",
                "ref_title_full": "Incremental parsing with the perceptron algorithm",
                "year": "2004",
                "full_block": "Incremental parsing with the perceptron algorithm.\n"
            },
            {
                "ref_title_clean": "generating sequences with recurrent neural networks",
                "ref_title_full": "Generating sequences with recurrent neural networks",
                "year": "2013",
                "full_block": "Generating sequences with recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "end to end continuous speech recognition using attention based recurrent nn first results",
                "ref_title_full": "End to end continuous speech recognition using attention based recurrent nn: First results",
                "year": "2014",
                "full_block": "End-to-end continuous speech recognition using attention-based\n  recurrent nn: First results.\n"
            },
            {
                "ref_title_clean": "recurrent continuous translation models",
                "ref_title_full": "Recurrent continuous translation models",
                "year": "2013",
                "full_block": "Recurrent continuous translation models.\n"
            },
            {
                "ref_title_clean": "show and tell a neural image caption generator",
                "ref_title_full": "Show and tell: A neural image caption generator",
                "year": "2014",
                "full_block": "Show and tell: A neural image caption generator.\n"
            },
            {
                "ref_title_clean": "a neural network for learning how to parse tree adjoining grammar",
                "ref_title_full": "A neural network for learning how to parse tree adjoining grammar",
                "year": "1990",
                "full_block": "A neural network for learning how to parse tree adjoining grammar.\n"
            }
        ],
        "refs_source": "unpacked_sources/1412.7449/nips2015.bbl"
    },
    "prow a step toward automatic program writing": {
        "id": null,
        "depth": 1,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Prow: A step toward automatic program writing",
        "link": "none",
        "n_parents": 1,
        "year": "1969"
    },
    "show attend and tell neural image caption generation with visual attention": {
        "id": "1502.03044",
        "depth": 1,
        "children_titles": [
            "multiple object recognition with visual attention",
            "neural machine translation by jointly learning to align and translate",
            "the dropout learning algorithm",
            "theano new features and speed improvements",
            "theano a cpu and gpu math expression compiler",
            "learning a recurrent visual representation for image caption generation",
            "learning phrase representations using rnn encoder decoder for statistical machine translation",
            "control of goal directed and stimulus driven attention in the brain",
            "learning where to attend with deep architectures for image tracking",
            "meteor universal language specific translation evaluation for any target language",
            "long term recurrent convolutional networks for visual recognition and description",
            "image description using visual dependency representations",
            "from captions to visual concepts and back",
            "long short term memory",
            "framing image description as a ranking task data models and evaluation metrics",
            "deep visual semantic alignments for generating image descriptions",
            "adam a method for stochastic optimization",
            "multimodal neural language models",
            "unifying visual semantic embeddings with multimodal neural language models",
            "imagenet classification with deep convolutional neural networks",
            "babytalk understanding and generating simple image descriptions",
            "collective generation of natural image descriptions",
            "treetalk composition and compression of trees for image descriptions",
            "learning to combine foveal glimpses with a third order boltzmann machine",
            "composing simple image descriptions using web scale n grams",
            "microsoft coco common objects in context",
            "deep captioning with multimodal recurrent neural networks m rnn",
            "midge generating image descriptions from computer vision detections",
            "recurrent models of visual attention",
            "how to construct deep recurrent neural networks",
            "the dynamic representation of scenes",
            "very deep convolutional networks for large scale image recognition",
            "practical bayesian optimization of machine learning algorithms",
            "input warping for bayesian optimization of non stationary functions",
            "dropout a simple way to prevent neural networks from overfitting",
            "sequence to sequence learning with neural networks",
            "going deeper with convolutions",
            "learning generative models with visual attention",
            "lecture 65 rmsprop",
            "show and tell a neural image caption generator",
            "the optimal reward baseline for gradient based reinforcement learning",
            "simple statistical gradient following algorithms for connectionist reinforcement learning",
            "corpus guided sentence generation of natural images",
            "from image descriptions to visual denotations new similarity metrics for semantic inference over event descriptions",
            "recurrent neural network regularization"
        ],
        "status": "expanded",
        "title_full": "Show, attend and tell: Neural image caption generation with visual attention",
        "link": "https://arxiv.org/abs/1502.03044",
        "n_parents": 3,
        "year": "2015",
        "children_full_dicts": [
            {
                "ref_title_clean": "multiple object recognition with visual attention",
                "ref_title_full": "Multiple object recognition with visual attention",
                "year": "none",
                "full_block": "Multiple object recognition with visual attention.\n"
            },
            {
                "ref_title_clean": "neural machine translation by jointly learning to align and translate",
                "ref_title_full": "Neural machine translation by jointly learning to align and translate",
                "year": "none",
                "full_block": "Neural machine translation by jointly learning to align and\n  translate.\n"
            },
            {
                "ref_title_clean": "the dropout learning algorithm",
                "ref_title_full": "The dropout learning algorithm",
                "year": "2014",
                "full_block": "The dropout learning algorithm.\n"
            },
            {
                "ref_title_clean": "theano new features and speed improvements",
                "ref_title_full": "Theano: new features and speed improvements",
                "year": "2012",
                "full_block": "{T}heano: new features and speed improvements.\n"
            },
            {
                "ref_title_clean": "theano a cpu and gpu math expression compiler",
                "ref_title_full": "Theano: a CPU and GPU math expression compiler",
                "year": "2010",
                "full_block": "Theano: a {CPU} and {GPU} math expression compiler.\n"
            },
            {
                "ref_title_clean": "learning a recurrent visual representation for image caption generation",
                "ref_title_full": "Learning a recurrent visual representation for image caption generation",
                "year": "2014",
                "full_block": "Learning a recurrent visual representation for image caption\n  generation.\n"
            },
            {
                "ref_title_clean": "learning phrase representations using rnn encoder decoder for statistical machine translation",
                "ref_title_full": "Learning phrase representations using RNN encoder decoder for statistical machine translation",
                "year": "2014",
                "full_block": "Learning phrase representations using {RNN} encoder-decoder for\n  statistical machine translation.\n"
            },
            {
                "ref_title_clean": "control of goal directed and stimulus driven attention in the brain",
                "ref_title_full": "Control of goal directed and stimulus driven attention in the brain",
                "year": "2002",
                "full_block": "Control of goal-directed and stimulus-driven attention in the brain.\n"
            },
            {
                "ref_title_clean": "learning where to attend with deep architectures for image tracking",
                "ref_title_full": "Learning where to attend with deep architectures for image tracking",
                "year": "2012",
                "full_block": "Learning where to attend with deep architectures for image tracking.\n"
            },
            {
                "ref_title_clean": "meteor universal language specific translation evaluation for any target language",
                "ref_title_full": "Meteor universal: Language specific translation evaluation for any target language",
                "year": "2014",
                "full_block": "Meteor universal: Language specific translation evaluation for any\n  target language.\n"
            },
            {
                "ref_title_clean": "long term recurrent convolutional networks for visual recognition and description",
                "ref_title_full": "Long term recurrent convolutional networks for visual recognition and description",
                "year": "none",
                "full_block": "Long-term recurrent convolutional networks for visual recognition and\n  description.\n"
            },
            {
                "ref_title_clean": "image description using visual dependency representations",
                "ref_title_full": "Image description using visual dependency representations",
                "year": "2013",
                "full_block": "Image description using visual dependency representations.\n"
            },
            {
                "ref_title_clean": "from captions to visual concepts and back",
                "ref_title_full": "From captions to visual concepts and back",
                "year": "none",
                "full_block": "From captions to visual concepts and back.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "framing image description as a ranking task data models and evaluation metrics",
                "ref_title_full": "Framing image description as a ranking task: Data, models and evaluation metrics",
                "year": "2013",
                "full_block": "Framing image description as a ranking task: Data, models and\n  evaluation metrics.\n"
            },
            {
                "ref_title_clean": "deep visual semantic alignments for generating image descriptions",
                "ref_title_full": "Deep visual semantic alignments for generating image descriptions",
                "year": "none",
                "full_block": "Deep visual-semantic alignments for generating image descriptions.\n"
            },
            {
                "ref_title_clean": "adam a method for stochastic optimization",
                "ref_title_full": "Adam: A Method for Stochastic Optimization",
                "year": "none",
                "full_block": "Adam: {A}{ Method for Stochastic Optimization}.\n"
            },
            {
                "ref_title_clean": "multimodal neural language models",
                "ref_title_full": "Multimodal neural language models",
                "year": "none",
                "full_block": "Multimodal neural language models.\n"
            },
            {
                "ref_title_clean": "unifying visual semantic embeddings with multimodal neural language models",
                "ref_title_full": "Unifying visual semantic embeddings with multimodal neural language models",
                "year": "1411",
                "full_block": "Unifying visual-semantic embeddings with multimodal neural language\n  models.\n"
            },
            {
                "ref_title_clean": "imagenet classification with deep convolutional neural networks",
                "ref_title_full": "ImageNet classification with deep convolutional neural networks",
                "year": "2012",
                "full_block": "{ImageNet} classification with deep convolutional neural networks.\n"
            },
            {
                "ref_title_clean": "babytalk understanding and generating simple image descriptions",
                "ref_title_full": "Babytalk: Understanding and generating simple image descriptions",
                "year": "2013",
                "full_block": "Babytalk: Understanding and generating simple image descriptions.\n"
            },
            {
                "ref_title_clean": "collective generation of natural image descriptions",
                "ref_title_full": "Collective generation of natural image descriptions",
                "year": "2012",
                "full_block": "Collective generation of natural image descriptions.\n"
            },
            {
                "ref_title_clean": "treetalk composition and compression of trees for image descriptions",
                "ref_title_full": "Treetalk: Composition and compression of trees for image descriptions",
                "year": "2014",
                "full_block": "Treetalk: Composition and compression of trees for image\n  descriptions.\n"
            },
            {
                "ref_title_clean": "learning to combine foveal glimpses with a third order boltzmann machine",
                "ref_title_full": "Learning to combine foveal glimpses with a third order boltzmann machine",
                "year": "2010",
                "full_block": "Learning to combine foveal glimpses with a third-order boltzmann\n  machine.\n"
            },
            {
                "ref_title_clean": "composing simple image descriptions using web scale n grams",
                "ref_title_full": "Composing simple image descriptions using web scale n grams",
                "year": "2011",
                "full_block": "Composing simple image descriptions using web-scale n-grams.\n"
            },
            {
                "ref_title_clean": "microsoft coco common objects in context",
                "ref_title_full": "Microsoft coco: Common objects in context",
                "year": "2014",
                "full_block": "Microsoft coco: Common objects in context.\n"
            },
            {
                "ref_title_clean": "deep captioning with multimodal recurrent neural networks m rnn",
                "ref_title_full": "Deep captioning with multimodal recurrent neural networks (m rnn)",
                "year": "none",
                "full_block": "Deep captioning with multimodal recurrent neural networks (m-rnn).\n"
            },
            {
                "ref_title_clean": "midge generating image descriptions from computer vision detections",
                "ref_title_full": "Midge: Generating image descriptions from computer vision detections",
                "year": "2012",
                "full_block": "Midge: Generating image descriptions from computer vision detections.\n"
            },
            {
                "ref_title_clean": "recurrent models of visual attention",
                "ref_title_full": "Recurrent models of visual attention",
                "year": "2014",
                "full_block": "Recurrent models of visual attention.\n"
            },
            {
                "ref_title_clean": "how to construct deep recurrent neural networks",
                "ref_title_full": "How to construct deep recurrent neural networks",
                "year": "2014",
                "full_block": "How to construct deep recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "the dynamic representation of scenes",
                "ref_title_full": "The dynamic representation of scenes",
                "year": "2000",
                "full_block": "The dynamic representation of scenes.\n"
            },
            {
                "ref_title_clean": "very deep convolutional networks for large scale image recognition",
                "ref_title_full": "Very deep convolutional networks for large scale image recognition",
                "year": "2014",
                "full_block": "Very deep convolutional networks for large-scale image recognition.\n"
            },
            {
                "ref_title_clean": "practical bayesian optimization of machine learning algorithms",
                "ref_title_full": "Practical bayesian optimization of machine learning algorithms",
                "year": "2012",
                "full_block": "Practical bayesian optimization of machine learning algorithms.\n"
            },
            {
                "ref_title_clean": "input warping for bayesian optimization of non stationary functions",
                "ref_title_full": "Input warping for bayesian optimization of non stationary functions",
                "year": "2014",
                "full_block": "Input warping for bayesian optimization of non-stationary functions.\n"
            },
            {
                "ref_title_clean": "dropout a simple way to prevent neural networks from overfitting",
                "ref_title_full": "Dropout: A simple way to prevent neural networks from overfitting",
                "year": "2014",
                "full_block": "Dropout: A simple way to prevent neural networks from overfitting.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "2014",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "going deeper with convolutions",
                "ref_title_full": "Going deeper with convolutions",
                "year": "2014",
                "full_block": "Going deeper with convolutions.\n"
            },
            {
                "ref_title_clean": "learning generative models with visual attention",
                "ref_title_full": "Learning generative models with visual attention",
                "year": "2014",
                "full_block": "Learning generative models with visual attention.\n"
            },
            {
                "ref_title_clean": "lecture 65 rmsprop",
                "ref_title_full": "Lecture 6.5 rmsprop",
                "year": "2012",
                "full_block": "Lecture 6.5 - rmsprop.\n"
            },
            {
                "ref_title_clean": "show and tell a neural image caption generator",
                "ref_title_full": "Show and tell: A neural image caption generator",
                "year": "none",
                "full_block": "Show and tell: A neural image caption generator.\n"
            },
            {
                "ref_title_clean": "the optimal reward baseline for gradient based reinforcement learning",
                "ref_title_full": "The optimal reward baseline for gradient based reinforcement learning",
                "year": "2001",
                "full_block": "The optimal reward baseline for gradient-based reinforcement\n  learning.\n"
            },
            {
                "ref_title_clean": "simple statistical gradient following algorithms for connectionist reinforcement learning",
                "ref_title_full": "Simple statistical gradient following algorithms for connectionist reinforcement learning",
                "year": "1992",
                "full_block": "Simple statistical gradient-following algorithms for connectionist\n  reinforcement learning.\n"
            },
            {
                "ref_title_clean": "corpus guided sentence generation of natural images",
                "ref_title_full": "Corpus guided sentence generation of natural images",
                "year": "2011",
                "full_block": "Corpus-guided sentence generation of natural images.\n"
            },
            {
                "ref_title_clean": "from image descriptions to visual denotations new similarity metrics for semantic inference over event descriptions",
                "ref_title_full": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                "year": "2014",
                "full_block": "From image descriptions to visual denotations: New similarity metrics\n  for semantic inference over event descriptions.\n"
            },
            {
                "ref_title_clean": "recurrent neural network regularization",
                "ref_title_full": "Recurrent neural network regularization",
                "year": "2014",
                "full_block": "Recurrent neural network regularization.\n"
            }
        ],
        "refs_source": "unpacked_sources/1502.03044/capgen.bbl"
    },
    "learning to execute": {
        "id": "1410.4615",
        "depth": 1,
        "children_titles": [
            "curriculum learning",
            "advances in optimizing recurrent networks",
            "can recursive neural tensor networks learn logical reasoning",
            "recursive neural networks for learning logical semantics",
            "learning phrase representations using rnn encoder decoder for statistical machine translation",
            "speech recognition with deep recurrent neural networks",
            "a statistical derivation of the significant digit law",
            "long short term memory",
            "optimization and applications of echo state networks with leaky integrator neurons",
            "a clockwork rnn",
            "self paced learning for latent variable models",
            "learning the easy things first self paced visual category discovery",
            "structured generative models of natural source code",
            "deep learning via hessian free optimization",
            "statistical language models based on neural networks",
            "recurrent neural network based language model",
            "building program vector representations for deep learning",
            "how to construct deep recurrent neural networks",
            "dropout improves recurrent neural networks for handwriting recognition",
            "the use of recurrent neural networks in continuous speech recognition",
            "training recurrent neural networks",
            "sequence to sequence learning with neural networks",
            "learning to discover efficient mathematical identities",
            "recurrent neural network regularization"
        ],
        "status": "expanded",
        "title_full": "Learning to execute",
        "link": "https://arxiv.org/abs/1410.4615",
        "n_parents": 5,
        "year": "2014",
        "children_full_dicts": [
            {
                "ref_title_clean": "curriculum learning",
                "ref_title_full": "Curriculum learning",
                "year": "2009",
                "full_block": "Curriculum learning.\n"
            },
            {
                "ref_title_clean": "advances in optimizing recurrent networks",
                "ref_title_full": "Advances in optimizing recurrent networks",
                "year": "2013",
                "full_block": "Advances in optimizing recurrent networks.\n"
            },
            {
                "ref_title_clean": "can recursive neural tensor networks learn logical reasoning",
                "ref_title_full": "Can recursive neural tensor networks learn logical reasoning?",
                "year": "2013",
                "full_block": "Can recursive neural tensor networks learn logical reasoning?\n"
            },
            {
                "ref_title_clean": "recursive neural networks for learning logical semantics",
                "ref_title_full": "Recursive neural networks for learning logical semantics",
                "year": "2014",
                "full_block": "Recursive neural networks for learning logical semantics.\n"
            },
            {
                "ref_title_clean": "learning phrase representations using rnn encoder decoder for statistical machine translation",
                "ref_title_full": "Learning phrase representations using rnn encoder decoder for statistical machine translation",
                "year": "2014",
                "full_block": "Learning phrase representations using rnn encoder-decoder for\n  statistical machine translation.\n"
            },
            {
                "ref_title_clean": "speech recognition with deep recurrent neural networks",
                "ref_title_full": "Speech recognition with deep recurrent neural networks",
                "year": "2013",
                "full_block": "Speech recognition with deep recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "a statistical derivation of the significant digit law",
                "ref_title_full": "A statistical derivation of the significant digit law",
                "year": "1995",
                "full_block": "A statistical derivation of the significant-digit law.\n"
            },
            {
                "ref_title_clean": "long short term memory",
                "ref_title_full": "Long short term memory",
                "year": "1997",
                "full_block": "Long short-term memory.\n"
            },
            {
                "ref_title_clean": "optimization and applications of echo state networks with leaky integrator neurons",
                "ref_title_full": "Optimization and applications of echo state networks with leaky integrator neurons",
                "year": "2007",
                "full_block": "Optimization and applications of echo state networks with\n  leaky-integrator neurons.\n"
            },
            {
                "ref_title_clean": "a clockwork rnn",
                "ref_title_full": "A clockwork rnn",
                "year": "2014",
                "full_block": "A clockwork rnn.\n"
            },
            {
                "ref_title_clean": "self paced learning for latent variable models",
                "ref_title_full": "Self paced learning for latent variable models",
                "year": "2010",
                "full_block": "Self-paced learning for latent variable models.\n"
            },
            {
                "ref_title_clean": "learning the easy things first self paced visual category discovery",
                "ref_title_full": "Learning the easy things first: Self paced visual category discovery",
                "year": "2011",
                "full_block": "Learning the easy things first: Self-paced visual category discovery.\n"
            },
            {
                "ref_title_clean": "structured generative models of natural source code",
                "ref_title_full": "Structured generative models of natural source code",
                "year": "2014",
                "full_block": "Structured generative models of natural source code.\n"
            },
            {
                "ref_title_clean": "deep learning via hessian free optimization",
                "ref_title_full": "Deep learning via hessian free optimization",
                "year": "2010",
                "full_block": "Deep learning via hessian-free optimization.\n"
            },
            {
                "ref_title_clean": "statistical language models based on neural networks",
                "ref_title_full": "\\Statistical language models based on neural networks",
                "year": "2012",
                "full_block": "\\emph{Statistical language models based on neural networks}.\n"
            },
            {
                "ref_title_clean": "recurrent neural network based language model",
                "ref_title_full": "Recurrent neural network based language model",
                "year": "2010",
                "full_block": "Recurrent neural network based language model.\n"
            },
            {
                "ref_title_clean": "building program vector representations for deep learning",
                "ref_title_full": "Building program vector representations for deep learning",
                "year": "2014",
                "full_block": "Building program vector representations for deep learning.\n"
            },
            {
                "ref_title_clean": "how to construct deep recurrent neural networks",
                "ref_title_full": "How to construct deep recurrent neural networks",
                "year": "2013",
                "full_block": "How to construct deep recurrent neural networks.\n"
            },
            {
                "ref_title_clean": "dropout improves recurrent neural networks for handwriting recognition",
                "ref_title_full": "Dropout improves recurrent neural networks for handwriting recognition",
                "year": "2013",
                "full_block": "Dropout improves recurrent neural networks for handwriting\n  recognition.\n"
            },
            {
                "ref_title_clean": "the use of recurrent neural networks in continuous speech recognition",
                "ref_title_full": "The use of recurrent neural networks in continuous speech recognition",
                "year": "1996",
                "full_block": "The use of recurrent neural networks in continuous speech\n  recognition.\n"
            },
            {
                "ref_title_clean": "training recurrent neural networks",
                "ref_title_full": "\\Training Recurrent Neural Networks",
                "year": "2013",
                "full_block": "\\emph{Training Recurrent Neural Networks}.\n"
            },
            {
                "ref_title_clean": "sequence to sequence learning with neural networks",
                "ref_title_full": "Sequence to sequence learning with neural networks",
                "year": "2014",
                "full_block": "Sequence to sequence learning with neural networks.\n"
            },
            {
                "ref_title_clean": "learning to discover efficient mathematical identities",
                "ref_title_full": "Learning to discover efficient mathematical identities",
                "year": "none",
                "full_block": "Learning to discover efficient mathematical identities.\n"
            },
            {
                "ref_title_clean": "recurrent neural network regularization",
                "ref_title_full": "Recurrent neural network regularization",
                "year": "none",
                "full_block": "Recurrent neural network regularization.\n"
            }
        ],
        "refs_source": "unpacked_sources/1410.4615/algebra.bbl"
    },
    "domain adaptation via pseudo in domain data selection": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Domain adaptation via pseudo in domain data selection",
        "link": "none",
        "n_parents": 1,
        "year": "none"
    },
    "theano new features and speed improvements": {
        "id": "1211.5590",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Theano: new features and speed improvements",
        "link": "https://arxiv.org/abs/1211.5590",
        "n_parents": 2,
        "year": "none"
    },
    "learning long term dependencies with gradient descent is difficult": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Learning long term dependencies with gradient descent is difficult",
        "link": "none",
        "n_parents": 5,
        "year": "none"
    },
    "a neural probabilistic language model": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "A neural probabilistic language model",
        "link": "none",
        "n_parents": 2,
        "year": "none"
    },
    "theano a cpu and gpu math expression compiler": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Theano: a CPU and GPU math expression compiler",
        "link": "none",
        "n_parents": 2,
        "year": "none"
    },
    "audio chord recognition with recurrent neural networks": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Audio chord recognition with recurrent neural networks",
        "link": "none",
        "n_parents": 1,
        "year": "none"
    },
    "learning phrase representations using rnn encoder decoder for statistical machine translation": {
        "id": "1406.1078",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Learning phrase representations using RNN encoder decoder for statistical machine translation",
        "link": "https://arxiv.org/abs/1406.1078",
        "n_parents": 6,
        "year": "none"
    },
    "on the properties of neural machine translation encoder decoder approaches": {
        "id": "1409.1259",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "On the properties of neural machine translation: Encoder Decoder approaches",
        "link": "https://arxiv.org/abs/1409.1259",
        "n_parents": 2,
        "year": "none"
    },
    "fast and robust neural network joint models for statistical machine translation": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Fast and robust neural network joint models for statistical machine translation",
        "link": "none",
        "n_parents": 2,
        "year": "none"
    },
    "recursive hetero associative memories for translation": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Recursive hetero associative memories for translation",
        "link": "none",
        "n_parents": 1,
        "year": "none"
    },
    "maxout networks": {
        "id": "1302.4389",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Maxout networks",
        "link": "https://arxiv.org/abs/1302.4389",
        "n_parents": 1,
        "year": "none"
    },
    "sequence transduction with recurrent neural networks": {
        "id": "1211.3711",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Sequence transduction with recurrent neural networks",
        "link": "https://arxiv.org/abs/1211.3711",
        "n_parents": 1,
        "year": "none"
    },
    "generating sequences with recurrent neural networks": {
        "id": "1308.0850",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Generating sequences with recurrent neural networks",
        "link": "https://arxiv.org/abs/1308.0850",
        "n_parents": 6,
        "year": "1308"
    },
    "hybrid speech recognition with deep bidirectional lstm": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Hybrid speech recognition with deep bidirectional LSTM",
        "link": "none",
        "n_parents": 2,
        "year": "none"
    },
    "multilingual distributed representations without word alignment": {
        "id": "1312.6173",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Multilingual distributed representations without word alignment",
        "link": "https://arxiv.org/abs/1312.6173",
        "n_parents": 2,
        "year": "none"
    },
    "long short term memory": {
        "id": "1507.01526",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Long short term memory",
        "link": "https://arxiv.org/abs/1507.01526",
        "n_parents": 14,
        "year": "none"
    },
    "recurrent continuous translation models": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Recurrent continuous translation models",
        "link": "none",
        "n_parents": 4,
        "year": "none"
    },
    "statistical machine translation": {
        "id": "1809.01272",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Statistical Machine Translation\\/",
        "link": "https://arxiv.org/abs/1809.01272",
        "n_parents": 1,
        "year": "none"
    },
    "statistical phrase based translation": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Statistical phrase based translation",
        "link": "none",
        "n_parents": 2,
        "year": "none"
    },
    "on the difficulty of training recurrent neural networks": {
        "id": "1211.5063",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "On the difficulty of training recurrent neural networks",
        "link": "https://arxiv.org/abs/1211.5063",
        "n_parents": 3,
        "year": "none"
    },
    "how to construct deep recurrent neural networks": {
        "id": "1312.6026",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "How to construct deep recurrent neural networks",
        "link": "https://arxiv.org/abs/1312.6026",
        "n_parents": 3,
        "year": "none"
    },
    "overcoming the curse of sentence length for neural machine translation using automatic segmentation": {
        "id": "1409.1257",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation",
        "link": "https://arxiv.org/abs/1409.1257",
        "n_parents": 2,
        "year": "none"
    },
    "bidirectional recurrent neural networks": {
        "id": "1604.03390",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Bidirectional recurrent neural networks",
        "link": "https://arxiv.org/abs/1604.03390",
        "n_parents": 1,
        "year": "none"
    },
    "continuous space translation models for phrase based statistical machine translation": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Continuous space translation models for phrase based statistical machine translation",
        "link": "none",
        "n_parents": 1,
        "year": "none"
    },
    "continuous space language models for statistical machine translation": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Continuous space language models for statistical machine translation",
        "link": "none",
        "n_parents": 1,
        "year": "none"
    },
    "adadelta an adaptive learning rate method": {
        "id": "1212.5701",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "ADADELTA: An adaptive learning rate method",
        "link": "https://arxiv.org/abs/1212.5701",
        "n_parents": 1,
        "year": "1212"
    },
    "deepmath deep sequence models for premise selection": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "DeepMath deep sequence models for premise selection",
        "link": "none",
        "n_parents": 1,
        "year": "2016"
    },
    "adaptive neural compilation": {
        "id": "1605.07969",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Adaptive neural compilation",
        "link": "https://arxiv.org/abs/1605.07969",
        "n_parents": 4,
        "year": "2016"
    },
    "the helmholtz machine": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "The Helmholtz machine",
        "link": "none",
        "n_parents": 1,
        "year": "1995"
    },
    "on label dependence and loss minimization in multi label classification": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "On label dependence and loss minimization in multi label classification",
        "link": "none",
        "n_parents": 1,
        "year": "2012"
    },
    "bayes optimal multilabel classification via probabilistic classifier chains": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Bayes optimal multilabel classification via probabilistic classifier chains",
        "link": "none",
        "n_parents": 1,
        "year": "2010"
    },
    "synthesizing data structure transformations from input output examples": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Synthesizing data structure transformations from input output examples",
        "link": "none",
        "n_parents": 1,
        "year": "2015"
    },
    "learning to transduce with unbounded memory": {
        "id": "1506.02516",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Learning to transduce with unbounded memory",
        "link": "https://arxiv.org/abs/1506.02516",
        "n_parents": 4,
        "year": "2015"
    },
    "programming by examples applications algorithms and ambiguity resolution": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Programming by examples: Applications, algorithms, and ambiguity resolution",
        "link": "none",
        "n_parents": 1,
        "year": "2016"
    },
    "synthesis of loop free programs": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Synthesis of loop free programs",
        "link": "none",
        "n_parents": 2,
        "year": "2011"
    },
    "learning to pass expectation propagation messages": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Learning to pass expectation propagation messages",
        "link": "none",
        "n_parents": 1,
        "year": "2013"
    },
    "the informed sampler a discriminative approach to bayesian inference in generative computer vision models": {
        "id": "1402.0859",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "The informed sampler: A discriminative approach to Bayesian inference in generative computer vision models",
        "link": "https://arxiv.org/abs/1402.0859",
        "n_parents": 1,
        "year": "2015"
    },
    "stochastic gradient vb and the variational auto encoder": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Stochastic gradient VB and the variational auto encoder",
        "link": "none",
        "n_parents": 1,
        "year": "2014"
    },
    "gated graph sequence neural networks": {
        "id": "1511.05493",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Gated graph sequence neural networks",
        "link": "https://arxiv.org/abs/1511.05493",
        "n_parents": 1,
        "year": "2016"
    },
    "latent predictor networks for code generation": {
        "id": "1603.06744",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Latent predictor networks for code generation",
        "link": "https://arxiv.org/abs/1603.06744",
        "n_parents": 1,
        "year": "2016"
    },
    "deep network guided proof search": {
        "id": "1701.06972",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Deep network guided proof search",
        "link": "https://arxiv.org/abs/1701.06972",
        "n_parents": 1,
        "year": "2017"
    },
    "learning program embeddings to propagate feedback on student code": {
        "id": "1505.05969",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Learning program embeddings to propagate feedback on student code",
        "link": "https://arxiv.org/abs/1505.05969",
        "n_parents": 1,
        "year": "2015"
    },
    "stochastic program optimization": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Stochastic program optimization",
        "link": "none",
        "n_parents": 1,
        "year": "2016"
    },
    "real time human pose recognition in parts from single depth images": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Real time human pose recognition in parts from single depth images",
        "link": "none",
        "n_parents": 1,
        "year": "2013"
    },
    "predicting a correct program in programming by example": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Predicting a correct program in programming by example",
        "link": "none",
        "n_parents": 1,
        "year": "2015"
    },
    "program synthesis by sketching": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "\\Program Synthesis By Sketching",
        "link": "none",
        "n_parents": 3,
        "year": "2008"
    },
    "learning stochastic inverses": {
        "id": null,
        "depth": 2,
        "children_titles": [],
        "status": "no_id",
        "title_full": "Learning stochastic inverses",
        "link": "none",
        "n_parents": 1,
        "year": "2013"
    },
    "end to end memory networks": {
        "id": "1503.08895",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "End to end memory networks",
        "link": "https://arxiv.org/abs/1503.08895",
        "n_parents": 4,
        "year": "2015"
    },
    "memory networks": {
        "id": "1410.3916",
        "depth": 2,
        "children_titles": [],
        "status": "max_depth",
        "title_full": "Memory networks",
        "link": "https://arxiv.org/abs/1410.3916",
        "n_parents": 5,
        "year": "2015"
    },
    "learning simple algorithms from examples": {
        